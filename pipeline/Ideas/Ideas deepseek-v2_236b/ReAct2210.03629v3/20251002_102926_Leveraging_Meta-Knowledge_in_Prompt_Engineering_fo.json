{
    "query": "Leveraging Meta-Knowledge in Prompt Engineering for Enhanced Language Model Performance",
    "result": {
        "1": "Improving large language models for clinical named entity recognition via prompt engineering. Yan Hu, Iqra Ameer, X. Zuo, Xueqing Peng, Yujia Zhou, Zehan Li, Yiming Li, Jianfu Li, Xiaoqian Jiang, Hua Xu. J. Am. Medical Informatics Assoc., 2024.\nNumber of citations: 235\nAbstract: Abstract Importance The study highlights the potential of large language models, specifically GPT-3.5 and GPT-4, in processing complex clinical data and extracting meaningful information with minimal training data. By developing and refining prompt-based strategies, we can significantly enhance the models\u2019 performance, making them viable tools for clinical NER tasks and possibly reducing the reliance on extensive annotated datasets. Objectives This study quantifies the capabilities of GPT-3.5 and GPT-4 for clinical named entity recognition (NER) tasks and proposes task-specific prompts to improve their performance. Materials and Methods We evaluated these models on 2 clinical NER tasks: (1) to extract medical problems, treatments, and tests from clinical notes in the MTSamples corpus, following the 2010 i2b2 concept extraction shared task, and (2) to identify nervous system disorder-related adverse events from safety reports in the vaccine adverse event reporting system (VAERS). To improve the GPT models' performance, we developed a clinical task-specific prompt framework that includes (1) baseline prompts with task description and format specification, (2) annotation guideline-based prompts, (3) error analysis-based instructions, and (4) annotated samples for few-shot learning. We assessed each prompt's effectiveness and compared the models to BioClinicalBERT. Results Using baseline prompts, GPT-3.5 and GPT-4 achieved relaxed F1 scores of 0.634, 0.804 for MTSamples and 0.301, 0.593 for VAERS. Additional prompt components consistently improved model performance. When all 4 components were used, GPT-3.5 and GPT-4 achieved relaxed F1 socres of 0.794, 0.861 for MTSamples and 0.676, 0.736 for VAERS, demonstrating the effectiveness of our prompt framework. Although these results trail BioClinicalBERT (F1 of 0.901 for the MTSamples dataset and 0.802 for the VAERS), it is very promising considering few training samples are needed. Discussion The study\u2019s findings suggest a promising direction in leveraging LLMs for clinical NER tasks. However, while the performance of GPT models improved with task-specific prompts, there's a need for further development and refinement. LLMs like GPT-4 show potential in achieving close performance to state-of-the-art models like BioClinicalBERT, but they still require careful prompt engineering and understanding of task-specific knowledge. The study also underscores the importance of evaluation schemas that accurately reflect the capabilities and performance of LLMs in clinical settings. Conclusion While direct application of GPT models to clinical NER tasks falls short of optimal performance, our task-specific prompt framework, incorporating medical knowledge and training samples, significantly enhances GPT models' feasibility for potential clinical applications.",
        "2": "EyeGPT for Patient Inquiries and Medical Education: Development and Validation of an Ophthalmology Large Language Model. Xiaolan Chen, Ziwei Zhao, Weiyi Zhang, Pusheng Xu, Yue Wu, Mingpu Xu, Le Gao, Yinwen Li, Xianwen Shang, Danli Shi, M. He. Journal of Medical Internet Research, 2024.\nNumber of citations: 16\nAbstract: Background Large language models (LLMs) have the potential to enhance clinical flow and improve medical education, but they encounter challenges related to specialized knowledge in ophthalmology. Objective This study aims to enhance ophthalmic knowledge by refining a general LLM into an ophthalmology-specialized assistant for patient inquiries and medical education. Methods We transformed Llama2 into an ophthalmology-specialized LLM, termed EyeGPT, through the following 3 strategies: prompt engineering for role-playing, fine-tuning with publicly available data sets filtered for eye-specific terminology (83,919 samples), and retrieval-augmented generation leveraging a medical database and 14 ophthalmology textbooks. The efficacy of various EyeGPT variants was evaluated by 4 board-certified ophthalmologists through comprehensive use of 120 diverse category questions in both simple and complex question-answering scenarios. The performance of the best EyeGPT model was then compared with that of the unassisted human physician group and the EyeGPT+human group. We proposed 4 metrics for assessment: accuracy, understandability, trustworthiness, and empathy. The proportion of hallucinations was also reported. Results The best fine-tuned model significantly outperformed the original Llama2 model at providing informed advice (mean 9.30, SD 4.42 vs mean 13.79, SD 5.70; P<.001) and mitigating hallucinations (97/120, 80.8% vs 53/120, 44.2%, P<.001). Incorporating information retrieval from reliable sources, particularly ophthalmology textbooks, further improved the model's response compared with solely the best fine-tuned model (mean 13.08, SD 5.43 vs mean 15.14, SD 4.64; P=.001) and reduced hallucinations (71/120, 59.2% vs 57/120, 47.4%, P=.02). Subgroup analysis revealed that EyeGPT showed robustness across common diseases, with consistent performance across different users and domains. Among the variants, the model integrating fine-tuning and book retrieval ranked highest, closely followed by the combination of fine-tuning and the manual database, standalone fine-tuning, and pure role-playing methods. EyeGPT demonstrated competitive capabilities in understandability and empathy when compared with human ophthalmologists. With the assistance of EyeGPT, the performance of the ophthalmologist was notably enhanced. Conclusions We pioneered and introduced EyeGPT by refining a general domain LLM and conducted a comprehensive comparison and evaluation of different strategies to develop an ophthalmology-specific assistant. Our results highlight EyeGPT\u2019s potential to assist ophthalmologists and patients in medical settings.",
        "3": "Testing prompt engineering methods for knowledge extraction from text. Fina Polat, Ilaria Tiddi, Paul Groth. Semantic Web, 2024.\nNumber of citations: 14\nAbstract: The capabilities of Large Language Models (LLMs,) such as Mistral 7B, Llama 3, GPT-4, present a significant opportunity for knowledge extraction (KE) from text. However, LLMs\u2019 context-sensitivity can hinder obtaining precise and task-aligned outcomes, thereby requiring prompt engineering. This study explores the efficacy of five prompt methods with different task demonstration strategies across 17 different prompt templates, utilizing a relation extraction dataset (RED-FM) with the aforementioned LLMs. To facilitate evaluation, we introduce a novel framework grounded in Wikidata\u2019s ontology. The findings demonstrate that LLMs are capable of extracting a diverse array of facts from text. Notably, incorporating a simple instruction accompanied by a task demonstration \u2013 comprising three examples selected via a retrieval mechanism \u2013 significantly enhances performance across Mistral 7B, Llama 3, and GPT-4. The effectiveness of reasoning-oriented prompting methods such as Chain-of-Thought, Reasoning and Acting, while improved with task demonstrations, does not surpass alternative methods. This suggests that framing extraction as a reasoning task may not be necessary for KE. Notably, task demonstrations leveraging examples selected via retrieval mechanisms facilitate effective knowledge extraction across all tested prompting strategies and LLMs.",
        "4": "Improving Training Dataset Balance with ChatGPT Prompt Engineering. Mateusz Kochanek, Igor Cichecki, Oliwier Kaszyca, Dominika Szyd\u0142o, Micha\u0142 Madej, Dawid J\u0119drzejewski, Przemyslaw Kazienko, Jan Koco\u0144. Electronics, 2024.\nNumber of citations: 13\nAbstract: The rapid evolution of large language models, in particular OpenAI\u2019s GPT-3.5-turbo and GPT-4, indicates a growing interest in advanced computational methodologies. This paper proposes a novel approach to synthetic data generation and knowledge distillation through prompt engineering. The potential of large language models (LLMs) is used to address the problem of unbalanced training datasets for other machine learning models. This is not only a common issue but also a crucial determinant of the final model quality and performance. Three prompting strategies have been considered: basic, composite, and similarity prompts. Although the initial results do not match the performance of comprehensive datasets, the similarity prompts method exhibits considerable promise, thus outperforming other methods. The investigation of our rebalancing methods opens pathways for future research on leveraging continuously developed LLMs for the enhanced generation of high-quality synthetic data. This could have an impact on many large-scale engineering applications.",
        "5": "Leveraging Large Language Models with Chain-of-Thought and Prompt Engineering for Traffic Crash Severity Analysis and Inference. Hao Zhen, Yucheng Shi, Yongcan Huang, Jidong J. Yang, Ninghao Liu. De Computis, 2024.\nNumber of citations: 7\nAbstract: Harnessing the power of Large Language Models (LLMs), this study explores the use of three state-of-the-art LLMs, specifically GPT-3.5-turbo, LLaMA3-8B, and LLaMA3-70B, for crash severity inference, framing it as a classification task. We generate textual narratives from original traffic crash tabular data using a pre-built template infused with domain knowledge. Additionally, we incorporated Chain-of-Thought (CoT) reasoning to guide the LLMs in analyzing the crash causes and then inferring the severity. This study also examine the impact of prompt engineering specifically designed for crash severity inference. The LLMs were tasked with crash severity inference to: (1) evaluate the models' capabilities in crash severity analysis, (2) assess the effectiveness of CoT and domain-informed prompt engineering, and (3) examine the reasoning abilities with the CoT framework. Our results showed that LLaMA3-70B consistently outperformed the other models, particularly in zero-shot settings. The CoT and Prompt Engineering techniques significantly enhanced performance, improving logical reasoning and addressing alignment issues. Notably, the CoT offers valuable insights into LLMs' reasoning processes, unleashing their capacity to consider diverse factors such as environmental conditions, driver behavior, and vehicle characteristics in severity analysis and inference.",
        "6": "FinFlier: Automating Graphical Overlays for Financial Visualizations With Knowledge-Grounding Large Language Model. Jianing Hao, Manling Yang, Qing Shi, Yuzhe Jiang, Guang Zhang, Wei Zeng. IEEE Transactions on Visualization and Computer Graphics, 2024.\nNumber of citations: 4\nAbstract: Graphical overlays that layer visual elements onto charts, are effective to convey insights and context in financial narrative visualizations. However, automating graphical overlays is challenging due to complex narrative structures and limited understanding of effective overlays. To address the challenge, we first summarize the commonly used graphical overlays and narrative structures, and the proper correspondence between them in financial narrative visualizations, elected by a survey of 1752 layered charts with corresponding narratives. We then design FinFlier, a two-stage innovative system leveraging a knowledge-grounding large language model to automate graphical overlays for financial visualizations. The text-data binding module enhances the connection between financial vocabulary and tabular data through advanced prompt engineering, and the graphics overlaying module generates effective overlays with narrative sequencing. We demonstrate the feasibility and expressiveness of FinFlier through a gallery of graphical overlays covering diverse financial narrative visualizations. Performance evaluations and user studies further confirm system\u2019s effectiveness and the quality of generated layered charts.",
        "7": "AI Tutor Enhanced with Prompt Engineering and Deep Knowledge Tracing. Radhika Makharia, Yeoun Chan Kim, Su Bin Jo, Min Ah Kim, Aagam Jain, P. Agarwal, Anish Srivastava, Anant Vikram Agarwal, Pankaj Agarwal. 2024 IEEE International Conference on Interdisciplinary Approaches in Technology and Management for Social Innovation (IATMSI), 2024.\nNumber of citations: 2\nAbstract: The evolving educational landscape necessitates creative solutions to address the demand for immediate and personalized academic support. This study explores the integration of prompt engineering of the OpenAI\u2019s Generative Pre-trained Transformer (GPT) and Deep Knowledge Tracing (DKT) to develop an AI tutor capable of shaping responses to students\u2019 knowledge levels, promoting a dynamic and adaptive learning experience. By leveraging Large Language Models (LLMs) like GPT-3.5 and integrating DKT, our AI tutor addresses the need for real-time, tailored academic assistance. LLMs serve as virtual instructors, explaining concepts and providing detailed solutions, while DKT ensures responses align with the student\u2019s knowledge level, optimizing challenge and engagement. Our research introduces an AI tutor that revolutionizes personalized learning experiences. Students can interact with the AI tutor by shaking their device during quizzes, initiating customized assistance and encouraging a deeper understanding of concepts, ultimately enhancing academic performance through individualized learning experiences.",
        "8": "Prompt Engineering for Automatic Short Answer Grading in Brazilian Portuguese. R. Mello, Luiz Rodrigues, Luciano Cabral, Filipe Dwan Pereira, C. P. Pereira J\u00fanior, D. Ga\u0161evi\u0107, Geber Ramalho. Anais do XXXV Simp\u00f3sio Brasileiro de Inform\u00e1tica na Educa\u00e7\u00e3o (SBIE 2024), 2024.\nNumber of citations: 2\nAbstract: Automatic Short Answer Grading (ASAG) is a prominent area of Artificial Intelligence in Education (AIED). Despite much research, developing ASAG systems is challenging, even when focused on a single subject, mostly due to the variability in length and content of students' answers. While recent research has explored Large Language Models (LLMs) to enhance the efficiency of ASAG, the LLM performance is highly dependent on the prompt design. In that context, prompt engineering plays a crucial role. However, to the best of our knowledge, no research has systematically investigated prompt engineering in ASAG. Thus, this study compares over 128 prompt combinations for a Portuguese dataset based on GPT-3.5-Turbo and GPT-4-Turbo. Our findings indicate the crucial role of specific prompt components in improving GPT results and shows that GPT-4 consistently outperformed GPT-3.5 in this domain. These insights guide prompt design for ASAG in the context of Brazilian Portuguese. Therefore, we recommend students, educators, and developers leverage these findings to optimize prompt design and benefit from the advancements offered by state-of-the-art LLMs whenever possible.",
        "9": "Data Redundancy Elimination and Noise Processing via Large Language Model Prompt Engineering. Jin Guang, Jing Chao, Tianqi Zong, Chen Siya, Chaoyuan Cui, Fan Jun. 2024 10th International Conference on Big Data and Information Analytics (BigDIA), 2024.\nNumber of citations: 0\nAbstract: With the rapid advancement of artificial intelligence (AI), intelligent systems have been widely applied across various practical domains. During their experimental processes, data collection often occurs in complex environments, leading to challenges in ensuring data quality and integrity. Two significant issues are data redundancy and data noise, which can degrade model performance and impact the generalization ability and prediction accuracy of AI models. Traditional methods for handling these issues are either rule-based, relying heavily on domain knowledge, or machine learning-based, which require large volumes of high-quality training data and significant computational resources. In this paper, we propose a novel approach leveraging Large Language Models (LLMs) combined with the Sequential Chain optimization algorithm to address data redundancy elimination and noise processing. By designing automated prompt templates tailored to experimental data characteristics and utilizing LLMs' extensive knowledge and reasoning capabilities, our approach improves the effectiveness of preprocessing tasks. Additionally, the Sequential Chain technique enhances LLM processing per-formance, reducing the hallucination phenomenon and ensuring higher accuracy in data preprocessing. Our experimental results demonstrate that LLM-based methods can achieve results comparable to traditional techniques while offering greater scalability and adaptability. Future work could focus on developing more efficient LLMs with lower computational requirements and refining prompt engineering techniques to reduce the time investment needed, making these advanced methods more accessible and practical for a broader range of applications.",
        "10": "A Large Language Model Driven Knowledge Graph Construction Scheme for Semantic Communication. Chang Guo, Jiaqi Liu, Wei Gao, Zhenhai Lu, Yao Li, Chengyuan Wang, Jungang Yang. Applied Sciences, 2025.\nNumber of citations: 0\nAbstract: This study presents a knowledge graph construction scheme leveraging large language models (LLMs) for task-oriented semantic communication systems. The proposed methodology systematically addresses four critical stages: corpus collection, entity extraction and relationship analysis, knowledge base generation, and dynamic updating mechanisms. It is worth noting that prompt engineering is combined with few-shot learning to enhance reliability and accuracy in this methodology. Experimental demonstration showed that this methodology had superior entity extraction performance, achieving 89.7% precision and 92.3% recall rate. This scheme overcomes the demand for domain knowledge and the labor cost of traditional knowledge base construction schemes. It greatly improves the construction efficiency of knowledge graphs. This paper provides an efficient and reliable task knowledge base construction scheme for task-oriented semantic communication, which is expected to promote its wider application."
    }
}