{
    "query": "neural-symbolic integration for explainable decision-making",
    "result": {
        "1": "Multimodal classification of Alzheimer's disease and mild cognitive impairment using custom MKSCDDL kernel over CNN with transparent decision-making for explainable diagnosis. V. Adarsh, G. R. Gangadharan, Ugo Fiore, P. Zanetti. Scientific Reports, 2024.\nNumber of citations: 41\nAbstract: The study presents an innovative diagnostic framework that synergises Convolutional Neural Networks (CNNs) with a Multi-feature Kernel Supervised within-class-similar Discriminative Dictionary Learning (MKSCDDL). This integrative methodology is designed to facilitate the precise classification of individuals into categories of Alzheimer's Disease, Mild Cognitive Impairment (MCI), and Cognitively Normal (CN) statuses while also discerning the nuanced phases within the MCI spectrum. Our approach is distinguished by its robustness and interpretability, offering clinicians an exceptionally transparent tool for diagnosis and therapeutic strategy formulation. We use scandent decision trees to deal with the unpredictability and complexity of neuroimaging data. Considering that different people's brain scans are different, this enables the model to make more detailed individualised assessments and explains how the algorithm illuminates the specific neuroanatomical regions that are indicative of cognitive impairment. This explanation is beneficial for clinicians because it gives them concrete ideas for early intervention and targeted care. The empirical review of our model shows that it makes diagnoses with a level of accuracy that is unmatched, with a classification efficacy of 98.27%. This shows that the model is good at finding important parts of the brain that may be damaged by cognitive diseases.",
        "2": "Guaranteeing Correctness in Black-Box Machine Learning: A Fusion of Explainable AI and Formal Methods for Healthcare Decision-Making. Nadia Khan, Muhammad Nauman, Ahmad S. Almadhor, Nadeem Akhtar, Abdullah Alghuried, Adi Alhudhaif. IEEE Access, 2024.\nNumber of citations: 15\nAbstract: In recent years, Explainable Artificial Intelligence (XAI) has attracted considerable attention from the research community, primarily focusing on elucidating the opaque decision-making processes inherent in complex black-box machine learning systems such as deep neural networks. This spike in interest originates from the widespread adoption of black-box models, particularly in critical domains like healthcare and fraud detection, highlighting the pressing need to understand and validate their decision-making mechanisms rigorously. In addition, prominent XAI techniques, including LIME (Local Interpretable Model-Agnostic Explanations) and SHAP (Shapley Additive exPlanations), rely on heuristics and cannot guarantee the correctness of the explanations provided. This article systematically addresses this critical issue associated with machine learning and deep learning models, underscoring XAI\u2019s pivotal role in promoting model transparency to enhance decision-making quality. Furthermore, this study advocates integrating Formal Methods to provide correctness guarantees for black-box internal decision-making. The proposed methodology unfolds in three pivotal stages: firstly, training black-box models using neural networks to generate synthetic datasets; secondly, employing LIME and SHAP techniques to interpret the models and visualize their internal decision-making processes; and finally, training decision trees on the synthetic datasets to implement Formal Methods for ensuring the correctness of the black-box model\u2019s decision-making. To validate this proposed approach, experimentation was conducted on four widely recognized medical datasets, including the Wisconsin Breast Cancer and Thyroid Cancer (TC) datasets, which are available in the UCI Machine Learning Repository. Specifically, this research represents a significant contribution by pioneering a novel approach that seamlessly integrates XAI and Formal Methods, thereby furnishing correctness guarantees for internal decision-making processes within the healthcare domain.",
        "3": "Enhancing Breast Cancer Diagnosis in Mammography: Evaluation and Integration of Convolutional Neural Networks and Explainable AI. Maryam Ahmed, Tooba Bibi, Rizwan Ahmed Khan, Sidra Nasir. International Multi-Topic Conference, 2024.\nNumber of citations: 8\nAbstract: The Deep learning (DL) models for diagnosing breast cancer from mammographic images often operate as \"black boxes,\" making it difficult for healthcare professionals to trust and understand their decision-making processes. The study presents an integrated framework combining Convolutional Neural Networks (CNNs) and Explainable Artificial Intelligence (XAI) for the enhanced diagnosis of breast cancer using the CBIS-DDSM dataset. The methodology encompasses an elaborate data preprocessing pipeline and advanced data augmentation techniques to counteract dataset limitations and transfer learning using pre-trained networks such as VGG-16, Inception-V3 and ResNet was employed. A focal point of our study is the evaluation of XAI\u2019s effectiveness in interpreting model predictions, highlighted by utilising the Hausdorff measure to assess the alignment between AI-generated explanations and expert annotations quantitatively. This approach is critical for XAI in promoting trustworthiness and ethical fairness in AI-assisted diagnostics.",
        "4": "Neural-Symbolic Integration for Interactive Learning and Conceptual Grounding. Benedikt Wagner, A. Garcez. arXiv.org, 2021.\nNumber of citations: 6\nAbstract: We propose neural-symbolic integration for abstract concept explanation and interactive learning. Neural-symbolic integration and explanation allow users and domain-experts to learn about the data-driven decision making process of large neural models. The models are queried using a symbolic logic language. Interaction with the user then confirms or rejects a revision of the neural model using logic-based constraints that can be distilled into the model architecture. The approach is illustrated using the Logic Tensor Network framework alongside Concept Activation Vectors and applied to a Convolutional Neural Network.",
        "5": "ReDBN: An Interpretable Deep Belief Network for Fan Fault Diagnosis in Iron and Steel Production Lines. Xiaoqiang Liao, Dong Wang, Siqi Qiu, Xinguo Ming. IEEE/ASME transactions on mechatronics, 2025.\nNumber of citations: 5\nAbstract: Fan fault diagnosis in steelmaking production lines is critical for safety production and environmental protection. Deep neural networks (DNNs) achieve slight success for fan fault diagnosis. These models are unable to provide explanations for fan diagnostic decisions due to DNN's opaque structure. From the perspective of neural-symbolic integration, researchers gradually pay attention to how to extract relational knowledge from DNNs to provide an explainable representation of DNN's features learning and reasoning. This study introduces a neural\u2013symbolic model, reverse deep belief network (ReDBN), where interpretable logic representations (CR-rules) are derived based on the integration of confidence and rough rules so as to tackle fan uncertain diagnosis decision-making. To extract confidence rules, a <italic>k</italic>-logic restricted Boltzmann machine (<inline-formula><tex-math notation=\"LaTeX\">${\\bm{k}}$</tex-math></inline-formula>-LRBM) is deployed and evaluated. In <inline-formula><tex-math notation=\"LaTeX\">${\\bm{k}}$</tex-math></inline-formula>-LRBMs, confidence rules can be extracted by considering the effect of <italic>k</italic> different literal clusters on neuron's activation. Besides, this article introduces a symbolic language, termed rough rules, which can solve uncertain reasoning during fan fault diagnosis. Rough rules, assigning a belief value for attribute variables, can represent the probability of how likely the sample belongs to specific fault labels. Verified on two fan datasets from a fan testbed and a real production site in Shanghai, ReDBN can achieve better performance than other typical models.",
        "6": "Explainable and Interpretable Diabetic Retinopathy Classification Based on Neural-Symbolic Learning. Se-In Jang, M. Girard, Alexandre H. Thi\u00e9ry. arXiv.org, 2022.\nNumber of citations: 5\nAbstract: In this paper, we propose an explainable and interpretable diabetic retinopathy (ExplainDR) classification model based on neural-symbolic learning. To gain explainability, a highlevel symbolic representation should be considered in decision making. Specifically, we introduce a human-readable symbolic representation, which follows a taxonomy style of diabetic retinopathy characteristics related to eye health conditions to achieve explainability. We then include humanreadable features obtained from the symbolic representation in the disease prediction. Experimental results on a diabetic retinopathy classification dataset show that our proposed ExplainDR method exhibits promising performance when compared to that from state-of-the-art methods applied to the IDRiD dataset, while also providing interpretability and explainability.",
        "7": "Neurosymbolic Learning in the XAI Framework for Enhanced Cyberattack Detection with Expert Knowledge Integration. Chathuranga Sampath Kalutharage, Xiaodong Liu, Christos Chrysoulas, O. Bamgboye. IFIP International Information Security Conference, 2024.\nNumber of citations: 2\nAbstract: None",
        "8": "Towards explainable decision support using hybrid neural models for logistic terminal automation. Riccardo DElia, Alberto Termine, Francesco Flammini. , 2025.\nNumber of citations: 0\nAbstract: The integration of Deep Learning (DL) in System Dynamics (SD) modeling for transportation logistics offers significant advantages in scalability and predictive accuracy. However, these gains are often offset by the loss of explainability and causal reliability $-$ key requirements in critical decision-making systems. This paper presents a novel framework for interpretable-by-design neural system dynamics modeling that synergizes DL with techniques from Concept-Based Interpretability, Mechanistic Interpretability, and Causal Machine Learning. The proposed hybrid approach enables the construction of neural network models that operate on semantically meaningful and actionable variables, while retaining the causal grounding and transparency typical of traditional SD models. The framework is conceived to be applied to real-world case-studies from the EU-funded project AutoMoTIF, focusing on data-driven decision support, automation, and optimization of multimodal logistic terminals. We aim at showing how neuro-symbolic methods can bridge the gap between black-box predictive models and the need for critical decision support in complex dynamical environments within cyber-physical systems enabled by the industrial Internet-of-Things.",
        "9": "Neuro-Symbolic Generative AI for Explainable Reasoning. Awolesi Abolanle Ogunboyo. International Journal of Science and Research Archive, 2025.\nNumber of citations: 0\nAbstract: The integration of neural and symbolic systems termed neuro-symbolic AI presents a compelling path toward explainable reasoning in Artificial Intelligence (AI). While deep learning models excel at pattern recognition and generative capabilities, their opaque decision-making process has raised concerns about transparency, interpretability, and trustworthiness. This research investigates the convergence of generative AI and neuro-symbolic architectures to enhance explainable reasoning. Employing a mixed-methods methodology grounded in empirical evaluation, knowledge representation, and symbolic rule induction, the study presents a hybrid framework where large language models (LLMs) are augmented with symbolic reasoning layers, allowing for natural language generation with traceable logic paths. Experimental results on benchmark datasets such as CLEVR, e-SNLI, and RuleTakers demonstrate substantial improvements in logical coherence, reasoning accuracy, and explanation fidelity over purely neural baselines. The study further explores implications for regulated domains, including healthcare, law, and cybersecurity. This work provides a foundation for future AI systems that are powerful in generation and transparent in justification, offering an interpretable-by-design approach to responsible AI.",
        "10": "A Review on AI-Driven Approaches for Autonomous Vehicles: Progress and Challenges. Rui Zhang. Science and Technology of Engineering, Chemistry and Environmental Protection, 2025.\nNumber of citations: 0\nAbstract: The rapid advancement of artificial intelligence has significantly propelled the development of autonomous vehicles, transforming both technological frameworks and practical applications. This paper systematically examines AI-driven approaches in autonomous vehicle systems, focusing on recent breakthroughs and persistent challenges. In perception systems, multi-sensor fusion and few-shot learning techniques have markedly enhanced object detection accuracy, while hierarchical reinforcement learning and socially compliant models have improved decision-making capabilities. Innovations in control systems, particularly the integration of model predictive control with neural-symbolic methods, demonstrate promising results in real-world scenarios. However, critical challenges remain, including performance degradation in extreme weather conditions, unresolved ethical and regulatory dilemmas regarding liability, and public skepticism toward human-machine interaction. The analysis highlights the necessity for explainable AI frameworks and real-time causal reasoning to address these issues. Future research directions emphasize the importance of cross-domain collaboration involving vehicle-road-cloud systems to achieve robust and trustworthy autonomous driving solutions. This review provides a comprehensive perspective on the current state of AI in autonomous vehicles, offering insights for researchers and practitioners navigating this evolving field."
    }
}