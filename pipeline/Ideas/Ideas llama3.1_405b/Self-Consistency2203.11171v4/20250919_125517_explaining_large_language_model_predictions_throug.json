{
    "query": "explaining large language model predictions through concept-based analysis",
    "result": {
        "1": "Benchmarking and Explaining Large Language Model-based Code Generation: A Causality-Centric Approach. Zhenlan Ji, Pingchuan Ma, Zongjie Li, Shuai Wang. arXiv.org, 2023.\nNumber of citations: 23\nAbstract: While code generation has been widely used in various software development scenarios, the quality of the generated code is not guaranteed. This has been a particular concern in the era of large language models (LLMs)- based code generation, where LLMs, deemed a complex and powerful black-box model, is instructed by a high-level natural language specification, namely a prompt, to generate code. Nevertheless, effectively evaluating and explaining the code generation capability of LLMs is inherently challenging, given the complexity of LLMs and the lack of transparency. Inspired by the recent progress in causality analysis and its application in software engineering, this paper launches a causality analysis-based approach to systematically analyze the causal relations between the LLM input prompts and the generated code. To handle various technical challenges in this study, we first propose a novel causal graph-based representation of the prompt and the generated code, which is established over the fine-grained, human-understandable concepts in the input prompts. The formed causal graph is then used to identify the causal relations between the prompt and the derived code. We illustrate the insights that our framework can provide by studying over 3 popular LLMs with over 12 prompt adjustment strategies. The results of these studies illustrate the potential of our technique to provide insights into LLM effectiveness, and aid end-users in understanding predictions. Additionally, we demonstrate that our approach provides actionable insights to improve the quality of the LLM-generated code by properly calibrating the prompt.",
        "2": "Evaluating and Explaining Large Language Models for Code Using Syntactic Structures. David N. Palacio, Alejandro Velasco, Daniel Rodr\u00edguez-C\u00e1rdenas, Kevin Moran, D. Poshyvanyk. arXiv.org, 2023.\nNumber of citations: 9\nAbstract: Large Language Models (LLMs) for code are a family of high-parameter, transformer-based neural networks pre-trained on massive datasets of both natural and programming languages. These models are rapidly being employed in commercial AI-based developer tools, such as GitHub CoPilot. However, measuring and explaining their effectiveness on programming tasks is a challenging proposition, given their size and complexity. The methods for evaluating and explaining LLMs for code are inextricably linked. That is, in order to explain a model's predictions, they must be reliably mapped to fine-grained, understandable concepts. Once this mapping is achieved, new methods for detailed model evaluations are possible. However, most current explainability techniques and evaluation benchmarks focus on model robustness or individual task performance, as opposed to interpreting model predictions. To this end, this paper introduces ASTxplainer, an explainability method specific to LLMs for code that enables both new methods for LLM evaluation and visualizations of LLM predictions that aid end-users in understanding model predictions. At its core, ASTxplainer provides an automated method for aligning token predictions with AST nodes, by extracting and aggregating normalized model logits within AST structures. To demonstrate the practical benefit of ASTxplainer, we illustrate the insights that our framework can provide by performing an empirical evaluation on 12 popular LLMs for code using a curated dataset of the most popular GitHub projects. Additionally, we perform a user study examining the usefulness of an ASTxplainer-derived visualization of model predictions aimed at enabling model users to explain predictions. The results of these studies illustrate the potential for ASTxplainer to provide insights into LLM effectiveness, and aid end-users in understanding predictions.",
        "3": "Explaining Large Language Models Decisions Using Shapley Values. Behnam Mohammadi. , 2024.\nNumber of citations: 6\nAbstract: The emergence of large language models (LLMs) has opened up exciting possibilities for simulating human behavior and cognitive processes, with potential applications in various domains, including marketing research and consumer behavior analysis. However, the validity of utilizing LLMs as stand-ins for human subjects remains uncertain due to glaring divergences that suggest fundamentally different underlying processes at play and the sensitivity of LLM responses to prompt variations. This paper presents a novel approach based on Shapley values from cooperative game theory to interpret LLM behavior and quantify the relative contribution of each prompt component to the model's output. Through two applications - a discrete choice experiment and an investigation of cognitive biases - we demonstrate how the Shapley value method can uncover what we term\"token noise\"effects, a phenomenon where LLM decisions are disproportionately influenced by tokens providing minimal informative content. This phenomenon raises concerns about the robustness and generalizability of insights obtained from LLMs in the context of human behavior simulation. Our model-agnostic approach extends its utility to proprietary LLMs, providing a valuable tool for practitioners and researchers to strategically optimize prompts and mitigate apparent cognitive biases. Our findings underscore the need for a more nuanced understanding of the factors driving LLM responses before relying on them as substitutes for human subjects in survey settings. We emphasize the importance of researchers reporting results conditioned on specific prompt templates and exercising caution when drawing parallels between human behavior and LLMs.",
        "4": "Can Embeddings Analysis Explain Large Language Model Ranking?. C. Lucchese, G. Minello, F. M. Nardini, Salvatore Orlando, Raffaele Perego, Alberto Veneri. International Conference on Information and Knowledge Management, 2023.\nNumber of citations: 5\nAbstract: Understanding the behavior of deep neural networks for Information Retrieval (IR) is crucial to improve trust in these effective models. Current popular approaches to diagnose the predictions made by deep neural networks are mainly based on: i) the adherence of the retrieval model to some axiomatic property of the IR system, ii) the generation of free-text explanations, or iii) feature importance attributions. In this work, we propose a novel approach that analyzes the changes of document and query embeddings in the latent space and that might explain the inner workings of IR large pre-trained language models. In particular, we focus on predicting query/document relevance, and we characterize the predictions by analyzing the topological arrangement of the embeddings in their latent space and their evolution while passing through the layers of the network. We show that there exists a link between the embedding adjustment and the predicted score, based on how tokens cluster in the embedding space. This novel approach, grounded in the query and document tokens interplay over the latent space, provides a new perspective on neural ranker explanation and a promising strategy for improving the efficiency of the models and Query Performance Prediction (QPP).",
        "5": "Explaining Language Model Predictions with High-Impact Concepts. Ruochen Zhao, Tan Wang, Yongjie Wang, Shafiq R. Joty. Findings, 2024.\nNumber of citations: 3\nAbstract: None",
        "6": "Concept-based Analysis of Neural Networks via Vision-Language Models. Ravi Mangal, Nina Narodytska, Divya Gopinath, Boyue Caroline Hu, Anirban Roy, Susmit Jha, Corina S. P\u0103s\u0103reanu. SAIV, 2024.\nNumber of citations: 3\nAbstract: The analysis of vision-based deep neural networks (DNNs) is highly desirable but it is very challenging due to the difficulty of expressing formal specifications for vision tasks and the lack of efficient verification procedures. In this paper, we propose to leverage emerging multimodal, vision-language, foundation models (VLMs) as a lens through which we can reason about vision models. VLMs have been trained on a large body of images accompanied by their textual description, and are thus implicitly aware of high-level, human-understandable concepts describing the images. We describe a logical specification language $\\texttt{Con}_{\\texttt{spec}}$ designed to facilitate writing specifications in terms of these concepts. To define and formally check $\\texttt{Con}_{\\texttt{spec}}$ specifications, we build a map between the internal representations of a given vision model and a VLM, leading to an efficient verification procedure of natural-language properties for vision models. We demonstrate our techniques on a ResNet-based classifier trained on the RIVAL-10 dataset using CLIP as the multimodal model.",
        "7": "Causality-Aided Evaluation and Explanation of Large Language Model-Based Code Generation. Zhenlan Ji, Pingchuan Ma, Zongjie Li, Zhaoyu Wang, Shuai Wang. Proc. ACM Softw. Eng., 2025.\nNumber of citations: 1\nAbstract: While code generation has been widely used in various software development scenarios, the quality of the generated code is not guaranteed. This has been a particular concern in the era of large language models (LLM)-based code generation, where LLMs, deemed a complex and powerful black-box model, are instructed by a high-level natural language specification, namely a prompt, to generate code. Nevertheless, effectively evaluating and explaining the code generation capability of LLMs is inherently challenging, given the complexity of LLMs and the lack of transparency.\n Inspired by recent progress in causality analysis and its software engineering applications, this paper proposes a causality-driven approach to systematically analyze prompt-code causal relationships. However, this endeavor faces three key technical challenges: (1) representing textual prompts and code in a canonical form, (2) establishing causal relations between high-level concepts and code features, and (3) systematically analyzing diverse prompt variations. To address these challenges, we first propose a novel causal graph-based representation of the prompt and the generated code, which is established over the fine-grained, human-understandable concepts in the input prompts. The formed causal graph is then used to identify the causal relations between the prompt and the derived code. We illustrate the insights that our framework can provide by studying over four popular LLMs with over 12 prompt adjustment strategies. The results of these studies illustrate the potential of our technique to provide insights into LLM effectiveness and aid end-users in understanding predictions. Additionally, we demonstrate that our approach provides actionable insights to improve the quality of the LLM-generated code by properly calibrating the prompt.",
        "8": "From Embeddings to Explainability: A Tutorial on Large-Language-Model-Based Text Analysis for Behavioral Scientists. Rudolf Debelak, Timo K. Koch, Matthias A\u00dfenmacher, Clemens Stachl. Advances in Methods and Practices in Psychological Science, 2025.\nNumber of citations: 0\nAbstract: Large language models (LLMs) are transforming research in psychology and the behavioral sciences by enabling advanced text analysis at scale. Their applications range from the analysis of social media posts to infer psychological traits to the automated scoring of open-ended survey responses. However, despite their potential, many behavioral scientists struggle to integrate LLMs into their research because of the complexity of text modeling. In this tutorial, we aim to provide an accessible introduction to LLM-based text analysis, focusing on the Transformer architecture. We guide researchers through the process of preparing text data, using pretrained Transformer models to generate text embeddings, fine-tuning models for specific tasks such as text classification, and applying interpretability methods, such as Shapley additive explanations and local interpretable model-agnostic explanations, to explain model predictions. By making these powerful techniques more approachable, we hope to empower behavioral scientists to leverage LLMs in their research, unlocking new opportunities for analyzing and interpreting textual data.",
        "9": "On Explaining (Large) Language Models For Code Using Global Code-Based Explanations. David N. Palacio, Dipin Khati, Daniel Rodr\u00edguez-C\u00e1rdenas, Alejandro Velasco, Denys Poshyvanyk. arXiv.org, 2025.\nNumber of citations: 0\nAbstract: In recent years, Language Models for Code (LLM4Code) have significantly changed the landscape of software engineering (SE) on downstream tasks, such as code generation, by making software development more efficient. Therefore, a growing interest has emerged in further evaluating these Language Models to homogenize the quality assessment of generated code. As the current evaluation process can significantly overreact on accuracy-based metrics, practitioners often seek methods to interpret LLM4Code outputs beyond canonical benchmarks. While the majority of research reports on code generation effectiveness in terms of expected ground truth, scant attention has been paid to LLMs' explanations. In essence, the decision-making process to generate code is hard to interpret. To bridge this evaluation gap, we introduce code rationales (Code$Q$), a technique with rigorous mathematical underpinning, to identify subsets of tokens that can explain individual code predictions. We conducted a thorough Exploratory Analysis to demonstrate the method's applicability and a User Study to understand the usability of code-based explanations. Our evaluation demonstrates that Code$Q$ is a powerful interpretability method to explain how (less) meaningful input concepts (i.e., natural language particle `at') highly impact output generation. Moreover, participants of this study highlighted Code$Q$'s ability to show a causal relationship between the input and output of the model with readable and informative explanations on code completion and test generation tasks. Additionally, Code$Q$ also helps to uncover model rationale, facilitating comparison with a human rationale to promote a fair level of trust and distrust in the model.",
        "10": "Explaining Tree-Based Regression Model Predictions with Nearest Training Neighbors. Srimoudgalya Sanagavarapu, Rishu Jain, Harikumaran Rani Dwarakanathan. Systems and Information Engineering Design Symposium, 2024.\nNumber of citations: 0\nAbstract: In recent years, there has been a lot of discussion around the concept of interpretability in machine learning (ML). The inability to comprehend the reasoning behind model predictions from complex \u2018black-box models\u2019 has been a chronic obstacle for adoption of ML based solutions. The expectations from an ML solution have advanced beyond obtaining automated predictions to also understanding the context of a prediction. Classification and Regression Tree (CART) algorithms have become the staple ML techniques for a large variety of corporate use-cases (customer attrition, product pricing, auto-approval of requests, etc.). To interpret the predictions from these models, techniques such as LIME and SHAP have concurrently found solid footing by evaluating feature importance values across the feature set. However, the fact remains that the mathematical foundation of these interpretability techniques themselves remains complex. This requires the user to have faith in the methodology in order to trust the feature importance values. Hence, this becomes a catch-22 situation where it is attempted to explain black-box predictions further with black-box interpretability techniques. This paper proposes a methodology that is built on top of feature importance values obtained from SHAP (LIME can be used as well). Utilizing these values, data points which the model considers as most similar to the prediction data point (or nearest training neighbors), can be extracted from the respective training data set. As the training data comprises realized target values, the target value of these nearest training neighbors can be used to justify the prediction from the model. This paper suggests, firstly, how these nearest training neighbors can be obtained and, secondly, 3 metrics that can be used to justify how well the nearest training neighbors extracted from this methodology represent a prediction holistically. Predictions from tree-based regression models are obtained by averaging various groups of training data points (through bagging and boosting). The primary goal behind this methodology is to identify which data points from the training data set were most likely to have a presence in those groups."
    }
}