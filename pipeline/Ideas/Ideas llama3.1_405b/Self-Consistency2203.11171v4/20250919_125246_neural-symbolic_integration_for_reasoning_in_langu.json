{
    "query": "neural-symbolic integration for reasoning in language models",
    "result": {
        "1": "Perspectives of Neural-Symbolic Integration. B. Hammer, P. Hitzler. Studies in Computational Intelligence, 2007.\nNumber of citations: 96\nAbstract: None",
        "2": "Semantic Understanding of Traffic Scenes with Large Vision Language Models. S. Jain, Surendrabikram Thapa, Kuan-Ting Chen, A. L. Abbott, Abhijit Sarkar. 2024 IEEE Intelligent Vehicles Symposium (IV), 2024.\nNumber of citations: 13\nAbstract: This paper investigates the integration of Large Vision Language Models (LVLMs) with multi-sensor information, including visual and localization data from cameras and LiDAR data to a holistic understanding of traffic videos. Traffic scene understanding is a challenging problem. With complex interaction between the road actors, infrastructure, and traffic rules, it is often difficult to answer questions related to road safety, pedestrian safety, safe maneuvering characteristics, and human factors. Typical processes use a single task-oriented neural network model and combine them through semantic and symbolic reasoning. These processes often suffer from reasoning bias and incompleteness. In recent years, LVLMs have opened new avenues to perceive spatiotemporal information. These models can leverage the large knowledge base from the world and summarize spatiotemporal information effectively. The interactive nature of most of these systems allows humans to directly interact in a visual question-answering mode.In this paper, we have extensively tested the capabilities of such LVLMs to answer key transportation research questions from videos captured through front cameras. We have curated an extensive set of multiple-choice questions to evaluate the performance of these LVLMs. Our results show that LVLMs have abilities to understand various transportation-related aspects to a great extent. Furthermore, we have shown that the addition of supplementary modalities to the VQA settings helps improve the performance of LVLMs. With the addition of 3D trajectories of surrounding objects with the 2D video frames, we observed a significant increase in MCQ performance related to vehicle-to-vehicle interaction tasks. The resources for this paper can be found at https://github.com/sandeshrjain/lvlm-scene",
        "3": "Neural-Symbolic Reasoning over Knowledge Graphs: A Survey from a Query Perspective. Lihui Liu, Zihao Wang, Hanghang Tong. SIGKDD Explorations, 2024.\nNumber of citations: 4\nAbstract: Knowledge graph reasoning is pivotal in various domains such as data mining, artificial intelligence, the Web, and social sciences. These knowledge graphs function as comprehensive repositories of human knowledge, facilitating the inference of new information. Traditional symbolic reasoning, despite its strengths, struggles with the challenges posed by incomplete and noisy data within these graphs. In contrast, the rise of Neural Symbolic AI marks a significant advancement, merging the robustness of deep learning with the precision of symbolic reasoning. This integration aims to develop AI systems that are not only highly interpretable and explainable but also versatile, effectively bridging the gap between symbolic and neural methodologies. Additionally, the advent of large language models (LLMs) has opened new frontiers in knowledge graph reasoning, enabling the extraction and synthesis of knowledge in unprecedented ways. This survey offers a thorough review of knowledge graph reasoning, focusing on various query types and the classification of neural symbolic reasoning. Furthermore, it explores the innovative integration of knowledge graph reasoning with large language models, highlighting the potential for groundbreaking advancements. This comprehensive overview is designed to support researchers and practitioners across multiple fields, including data mining, AI, the Web, and social sciences, by providing a detailed understanding of the current landscape and future directions in knowledge graph reasoning.",
        "4": "Modeling Patterns for Neural-Symbolic Reasoning Using Energy-based Models. Charles Dickens, Connor Pryor, Lise Getoor. AAAI Spring Symposia, 2024.\nNumber of citations: 3\nAbstract: Neural-symbolic (NeSy) AI strives to empower machine learning and large language models with fast, reliable predictions that exhibit commonsense and trustworthy reasoning by seamlessly integrating neural and symbolic methods. With such a broad scope, several taxonomies have been proposed to categorize this integration, emphasizing knowledge representation, reasoning algorithms, and applications. We introduce a knowledge representation-agnostic taxonomy focusing on the neural-symbolic interface capturing methods that reason with probability, logic, and arithmetic constraints. Moreover, we derive expressions for gradients of a prominent class of learning losses and a formalization of reasoning and learning. Through a rigorous empirical analysis spanning three tasks, we show NeSy approaches reach up to a 37% improvement over neural baselines in a semi-supervised setting and a 19% improvement over GPT-4 on question-answering.",
        "5": "Neural-Symbolic Relational Reasoning on Graph Models: Effective Link Inference and Computation from Knowledge Bases. Henrique Lemos, Pedro H. C. Avelar, Marcelo O. R. Prates, L. Lamb, A. Garcez. International Conference on Artificial Neural Networks, 2020.\nNumber of citations: 3\nAbstract: The recent developments and growing interest in neural-symbolic models has shown that hybrid approaches can offer richer models for Artificial Intelligence. The integration of effective relational learning and reasoning methods is one of the key challenges in this direction, as neural learning and symbolic reasoning offer complementary characteristics that can benefit the development of AI systems. Relational labelling or link prediction on knowledge graphs has become one of the main problems in deep learning-based natural language processing research. Moreover, other fields which make use of neural-symbolic techniques may also benefit from such research endeavours. There have been several efforts towards the identification of missing facts from existing ones in knowledge graphs. Two lines of research try and predict knowledge relations between two entities by considering all known facts connecting them or several paths of facts connecting them. We propose a neural-symbolic graph neural network which applies learning over all the paths by feeding the model with the embedding of the minimal subset of the knowledge graph containing such paths. By learning to produce representations for entities and facts corresponding to word embeddings, we show how the model can be trained end-to-end to decode these representations and infer relations between entities in a multitask approach. Our contribution is two-fold: a neural-symbolic methodology leverages the resolution of relational inference in large graphs, and we also demonstrate that such neural-symbolic model is shown more effective than path-based approaches",
        "6": "MediSage: An AI Assistant for Healthcare via Composition of Neural-Symbolic Reasoning Operators. Sutanay Choudhury, Khushbu Agarwal, Colby Ham, Suzanne Tamang. The Web Conference, 2023.\nNumber of citations: 2\nAbstract: We introduce MediSage, an AI decision support assistant for medical professionals and caregivers that simplifies the way in which they interact with different modalities of electronic health records (EHRs) through a conversational interface. It provides step-by-step reasoning support to an end-user to summarize patient health, predict patient outcomes and provide comprehensive and personalized healthcare recommendations. MediSage provides these reasoning capabilities by using a knowledge graph that combines general purpose clinical knowledge resources with recent-most information from the EHR data. By combining the structured representation of knowledge with the predictive power of neural models trained over both EHR and knowledge graph data, MediSage brings explainability by construction and represents a stepping stone into the future through further integration with biomedical language models.",
        "7": "A Pipeline of Neural-Symbolic Integration to Enhance Spatial Reasoning in Large Language Models. Rong Wang, Kun Sun, Jonas Kuhn. arXiv.org, 2024.\nNumber of citations: 1\nAbstract: None",
        "8": "Application of Neuro-Symbolic Reasoning in Natural Language Processing. Shivani Aithal, Abishek Rao, C. B, Sanjay Singh. Conference Information and Communication Technology, 2022.\nNumber of citations: 1\nAbstract: Commonsense reasoning is a crucial part of human intelligence. It has been a goal in the study of artificial intelligence to develop machines mimicking commonsense reasoning at par with humans. The deep learning models have advanced in many natural language processing tasks. However, they lack logical reasoning ability. Symbolic artificial intelligence has the potential reasoning ability wherein logic is used to reason efficiently. This led to the rise of neuro-symbolic reasoning, which is an integration of symbolic logic with deep learning. In this paper, we have experimented with the BERT, RoBERTa and GPT-J language models for question answering task to check their reasoning ability and found that they lack the reasoning ability. This paper compares the performance of the BERT, RoBERTa and GPT-J question-answering models with the neuro-symbolic reasoning model using logical neural networks. It is found that the neuro-symbolic reasoning model has the reasoning ability like humans, which infers that the neuro-symbolic reasoning model can effectively handle logical reasoning and is a key to the future of artificial intelligence.",
        "9": "COOL: A Constraint Object-Oriented Logic Programming Language and its Neural-Symbolic Compilation System. Jipeng Han. arXiv.org, 2023.\nNumber of citations: 0\nAbstract: This paper explores the integration of neural networks with logic programming, addressing the longstanding challenges of combining the generalization and learning capabilities of neural networks with the precision of symbolic logic. Traditional attempts at this integration have been hampered by difficulties in initial data acquisition, the reliability of undertrained networks, and the complexity of reusing and augmenting trained models. To overcome these issues, we introduce the COOL (Constraint Object-Oriented Logic) programming language, an innovative approach that seamlessly combines logical reasoning with neural network technologies. COOL is engineered to autonomously handle data collection, mitigating the need for user-supplied initial data. It incorporates user prompts into the coding process to reduce the risks of undertraining and enhances the interaction among models throughout their lifecycle to promote the reuse and augmentation of networks. Furthermore, the foundational principles and algorithms in COOL's design and its compilation system could provide valuable insights for future developments in programming languages and neural network architectures.",
        "10": "Simplifying AI reasoning: unlocking logical capabilities in large language models (LLMs). Peraschi Selvan Subramanian. World Journal of Advanced Research and Reviews, 2025.\nNumber of citations: 0\nAbstract: The integration of logical reasoning capabilities in large language models (LLMs) represents a transformative advancement in artificial intelligence, fundamentally altering the landscape of machine intelligence. This article examines how LLMs have evolved from pattern recognition systems into sophisticated reasoning engines capable of human-like logical deduction and inference across diverse domains. Through strategic architectural innovations, including advanced scaling techniques, synthetic multihop reasoning environments, and hybrid neural-symbolic frameworks, these reasoning capabilities have become increasingly accessible for real-world implementation. The practical impact spans multiple sectors, from revolutionizing legal document processing and accelerating scientific discovery to enhancing autonomous decision-making in dynamic environments. While impressive strides have been made in computational efficiency through specialized hardware and knowledge graph optimizations, significant challenges remain in ensuring ethical transparency and addressing scalability constraints. The continuing evolution of AI reasoning technologies promises to reshape decision-making processes across industries while establishing new paradigms for human-machine collaboration."
    }
}