{
    "query": "meta-learning for large language models with cognitive architectures",
    "result": {
        "1": "Balancing long-term reinforcement and short-term inhibition. B. Best, C. Lebiere. , 2009.\nNumber of citations: 19\nAbstract: None",
        "2": "Intelligent Tutoring Systems: 5th International Conference, ITS 2000, Montreal, Canada, June 19-23, 2000 Proceedings. Gilles Gauthier, C. Frasson, K. VanLehn. , 2000.\nNumber of citations: 11\nAbstract: None",
        "3": "Enabling Cognitive Architectures for UAV Mission Planning. Jon C. Russo, Mohammed Amduka, B. Gelfand, Lockheed Martin. , 2006.\nNumber of citations: 8\nAbstract: None",
        "4": "Evaluation of computational models of infant language development against robust empirical data from meta-analyses: what, why, and how?. Mar\u00eda Andrea Cruz Bland\u00f3n, Alejandrina Cristia, O. R\u00e4s\u00e4nen. , 2021.\nNumber of citations: 4\nAbstract: Computational models of child language development can help us understand the cognitive underpinnings of the language learning process. One advantage of computational modeling is that is has the potential to address multiple aspects of language learning within a single learning architecture. If successful, such integrated models would help to pave the way for a more comprehensive and mechanistic understanding of language development. However, in order to develop more accurate, holistic, and hence impactful models of infant language learning, the research on models also requires model evaluation practices that allow comparison of model behavior to empirical data from infants across a range of language capabilities. Moreover, there is a need for practices that can compare developmental trajectories of infants to those of models as a function of language experience. The present study aims to take the first steps to address these needs. More specifically, we will introduce the concept of comparing models with large-scale cumulative empirical data from infants, as quantified by meta-analyses conducted across a large number of individual behavioral studies. We start by formalizing the connection between measurable model and human behavior, and then present a basic conceptual framework for meta-analytic evaluation of computational models together with basic guidelines intended as a starting point for later work in this direction. We exemplify the meta-analytic model evaluation approach with two modeling experiments on infant-directed speech preference and native/non-native vowel discrimination. We also discuss the advantages, challenges, and potential future directions of meta-analytic evaluation practices.",
        "5": "Effects of Segmented Animation among Students of Different Anxiety Levels : A Cognitive Load Perspective. S. F. Fong, Lee Pei Lin Lily. , 2010.\nNumber of citations: 4\nAbstract: None",
        "6": "Meta-Learning Neural Mechanisms rather than Bayesian Priors. Michael Goodale, Salvador Mascarenhas, Yair Lakretz. Annual Meeting of the Association for Computational Linguistics, 2025.\nNumber of citations: 1\nAbstract: Children acquire language despite being exposed to several orders of magnitude less data than large language models require. Meta-learning has been proposed as a way to integrate human-like learning biases into neural-network architectures, combining both the structured generalizations of symbolic models with the scalability of neural-network models. But what does meta-learning exactly imbue the model with? We investigate the meta-learning of formal languages and find that, contrary to previous claims, meta-trained models are not learning simplicity-based priors when meta-trained on datasets organised around simplicity. Rather, we find evidence that meta-training imprints neural mechanisms (such as counters) into the model, which function like cognitive primitives for the network on downstream tasks. Most surprisingly, we find that meta-training on a single formal language can provide as much improvement to a model as meta-training on 5000 different formal languages, provided that the formal language incentivizes the learning of useful neural mechanisms. Taken together, our findings provide practical implications for efficient meta-learning paradigms and new theoretical insights into linking symbolic theories and neural mechanisms.",
        "7": "Interactionalism: Re-Designing Higher Learning for the Large Language Agent Era. M. Moldoveanu, George Siemens. arXiv.org, 2025.\nNumber of citations: 0\nAbstract: We introduce Interactionalism as a new set of guiding principles and heuristics for the design and architecture of learning now available due to Generative AI (GenAI) platforms. Specifically, we articulate interactional intelligence as a net new skill set that is increasingly important when core cognitive tasks are automatable and augmentable by GenAI functions. We break down these skills into core sets of meta-cognitive and meta-emotional components and show how working with Large Language Model (LLM)-based agents can be proactively used to help develop learners. Interactionalism is not advanced as a theory of learning; but as a blueprint for the practice of learning - in coordination with GenAI.",
        "8": "LLM-Assisted Iterative Evolution with Swarm Intelligence Toward SuperBrain. Weigang Li, P. C. Brom, Lucas Ramson Siefert. , 2025.\nNumber of citations: 0\nAbstract: We propose a novel SuperBrain framework for collective intelligence, grounded in the co-evolution of large language models (LLMs) and human users. Unlike static prompt engineering or isolated agent simulations, our approach emphasizes a dynamic pathway from Subclass Brain to Superclass Brain: (1) A Subclass Brain arises from persistent, personalized interaction between a user and an LLM, forming a cognitive dyad with adaptive learning memory. (2) Through GA-assisted forward-backward evolution, these dyads iteratively refine prompts and task performance. (3) Multiple Subclass Brains coordinate via Swarm Intelligence, optimizing across multi-objective fitness landscapes and exchanging distilled heuristics. (4) Their standardized behaviors and cognitive signatures integrate into a Superclass Brain, an emergent meta-intelligence capable of abstraction, generalization and self-improvement. We outline the theoretical constructs, present initial implementations (e.g., UAV scheduling, KU/KI keyword filtering) and propose a registry for cross-dyad knowledge consolidation. This work provides both a conceptual foundation and an architectural roadmap toward scalable, explainable and ethically aligned collective AI.",
        "9": "DeepThought: An Architecture for Autonomous Self-motivated Systems. Arlindo L. Oliveira, Tiago Domingos, M\u00e1rio A. T. Figueiredo, Pedro U. Lima. arXiv.org, 2023.\nNumber of citations: 0\nAbstract: The ability of large language models (LLMs) to engage in credible dialogues with humans, taking into account the training data and the context of the conversation, has raised discussions about their ability to exhibit intrinsic motivations, agency, or even some degree of consciousness. We argue that the internal architecture of LLMs and their finite and volatile state cannot support any of these properties. By combining insights from complementary learning systems, global neuronal workspace, and attention schema theories, we propose to integrate LLMs and other deep learning systems into an architecture for cognitive language agents able to exhibit properties akin to agency, self-motivation, even some features of meta-cognition.",
        "10": "Highlights of the Issue: Governance, Agents, Evolutionary Search. Kristen W Carlson. Robotics, 2025.\nNumber of citations: 0\nAbstract: \u00a0\nRecursive Self-Improvement (RSI)\nIn the next issue we will publish more articles on RSI, for instance, using reinforcement learning (RL). These articles, along with, in this issue, Darwin G\u00f6del Machine and DarwinLM, show that RSI will come in different flavors, e.g. here using evolutionary search, and have different purposes. So far, no one RSI technique itself triggers dramatic progress toward AGI.\nAgentic AI vs. AI Agents\nThe goal of Agentic AI \u2014 to render AI scalable and adaptable in complex environments \u2014 seems inherently so much more powerful than individual AI agents that the focus on progress toward artificial general intelligence should be on Agentic AI progress against benchmarks.\nClearly, the most advanced Agentic AI should be deployed to advance safety and value alignment, as Kumarage and colleagues show in this issue.\nDevelopment of Agentic AI indicates a need to focus on the type of AGI ecology that will emerge (e.g. multipolar scenarios rather than singletons), and permits using the paradigm of game theory to control safety and value alignment, as I wrote in 2019.[1] I have more to say on this subject.\nTimeline to Artificial General Intelligence 2025 \u2013 2030+\nSenior Editor-at-Large Gil Syswerda has constructed a provocative Timeline to Artificial General Intelligence 2025 \u2013 2030+. He gives key AI advances, economic, social, and geopolitical effects of AI. As early as 2028-2029, AI could replace the majority of human economic activity, potentially disrupting society due to widespread change happening so rapidly. But he is optimistic:\nBy the end of the decade, superintelligence is present on Earth. Human institutions are no longer in control. Everything changes in ways beyond current understanding....Humanity survives the transition\u2014and enters an Age of Abundance. The meaning of citizenship, nationhood, and law undergoes foundational redefinition.\nArticles\nComparing Apples to Oranges: A Taxonomy for Navigating the Global Landscape of AI Regulation\nThe EU has a unified framework for regulating AI, China has its own governance structure, which it has efficiently imposed throughout China due to centralized control. Since the proposed 10-year moratorium on state AI regulation was killed in the \u2018Big Beautiful Bill,\u2019 the US now seems headed toward a uncoordinated patchwork of regulation at the state and federal level. Thus, this work by Alanoca et al. is critically important and urgent. From the abstract:\nClarifying the scope and substance of AI regulation is vital to uphold democratic rights and align international AI efforts. We present a taxonomy to map the global landscape of AI regulation. Our framework targets essential metrics\u2014technology or application-focused rules, horizontal or sectoral regulatory coverage, ex ante or ex post interventions, maturity of the digital legal landscape, enforcement mechanisms, and level of stakeholder participation\u2014to classify the breadth and depth of AI regulation.\nAs our co-founding editor, Steve Omohundro foresaw in 2014, AI itself will play an increasing role in evolving the legacy human legal regime in general and regulations governing AI in particular:\nThe legal codes of many countries have become quite complex. Several AI projects are trying to create formal digital versions of legal codes (CodeX, 2014). These systems will eventually be used to resolve legal issues and perhaps even act as arbitrators or judges. Sophisticated AI systems with knowledge of the legal system will be used to help craft and simplify new legislation. [2]\nAccordingly, humans should initiate the process of AI taking over evolution of law, notably in the area of AI safety, with human oversight. In principle, LLMs can absorb the entire global body of human law and compare the different regimes to provide a cohesive unified over-arching structure, which no human is capable of doing. As AI progresses toward AGI, it can evolve the Alanoca et al. taxonomy to organize the diverse and inchoate international, national, and local regulatory regimes following the examples the authors provide.\nReal-World Gaps in AI Governance Research\nStrauss et al. did a massive survey of 1,178 safety and reliability papers analyzed from 9,439 generative AI papers between Jan 2020 and Mar 2025. They found significant gaps in AI governance research, particularly in post-deployment contexts and high-risk areas. \u200b\n\nCorporate AI research is increasingly influential, focusing on pre-deployment safety while neglecting real-world deployment issues. \u200b\nCorporate AI (Anthropic, Google DeepMind, Meta, Microsoft, OpenAI) has more citations than leading academic institutions (academic institutions (Carnegie Mellon University, Massachusetts Institute of Technology, New York University, Stanford University, University of California Berkeley, and University of Washington) \u2013 see their Table 2.\nGoogle DeepMind has more citations than the top four academic institutions combined. \u200b\nCorporate AI research prioritizes model alignment and testing, with less focus on deployment-stage issues like bias.\nThere is a critical lack of research on the safety and reliability of AI systems in real-world applications.\n\nAI Agents vs. Agentic AI: A Conceptual Taxonomy, Applications and Challenges\nSapkota et al. delineate agent AI vs. agentic AI, briefly giving the history and the motivation behind agentic AI toward its prime goal of scalability and adaptability in complex environments. Thus, agentic AI is a key step in the evolution of AI to AGI and superintelligence. As for safety, trust-centric operations will prioritize safety mechanisms, ensuring verifiable output and ethical compliance.\n\nAI Agents are modular systems driven by LLMs and LIMs for task-specific automation. \u200b\nAgentic AI represents a paradigm shift with multi-agent collaboration, dynamic task decomposition, and coordinated autonomy.\nAI Agents are autonomous software entities designed for goal-directed task execution within bounded environments. \u200b\nKey characteristics include autonomy, task-specificity, and reactivity with adaptation.\nAgentic AI systems manage complex, multi-step tasks requiring coordination among multiple agents..\nGenerative AI systems are stateless and lack the ability to interact with their environment autonomously.\nLLMs exhibit reactive behavior, producing output only when prompted, without autonomous goal pursuit.\nThe evolution from generative models to AI Agents is driven by the integration of large-scale language models (LLMs) as reasoning engines.\nAI Agents utilize LLMs like GPT-3 and LLaMA to perform adaptive planning and real-time decision-making.\nAgents function as cognitive engines that interpret user goals and manage complex workflows.\nAgentic AI systems extend the capabilities of traditional AI Agents by enabling collaboration among multiple intelligent entities.\nThey allow for goal decomposition, where user objectives are parsed into manageable tasks distributed across agents. \u200b\nInter-agent communication is facilitated through asynchronous messaging and shared memory\n\nMeasuring AI Agent Autonomy: Towards a Scalable Approach with Code Inspection\nHow to define \u201cartificial general intelligence\u201d? It\u2019s like trying to define \u201clife\u201d. Most agree autonomy has to be included in defining \u201chuman-level intelligence\u201d. In the next issue we will give our view. Here Cihon et al. present an operational definition of \u201cautonomy\u201d based on the agent architecture (a better description than \u2018code inspection\u2019) and without having to observe agent behavior, per se, based on an eight-component taxonomy of autonomy\u00a0 \u2013 see their Figure 1. For example, human-programmed goals = no autonomy, while agent-programmed goals = autonomy.\nAs the authors note and we stress, \u201cthe level of agent autonomy is crucial for understanding both their potential benefits and risks,\u201d i.e. AGI safety.\nDarwin G\u00f6del Machine: Open-Ended Evolution of Self-Improving Agents: Main Article & Appendix F on Safety\nIn the DGM, improvement in downstream tasks directly reflects an increase in self-improvement ability, enabling the potential for self-accelerating progress.\nHowever, these [various foundation model] approaches have yet to close the self-improvement loop, meaning improvements on downstream tasks do not translate into enhanced capabilities for self-modification or the acceleration of further innovations. We aim to mimic the acceleration of science and technology, where new tools and discoveries catalyze the creation of even more discoveries. Similarly, how can we emulate nature\u2019s arc of evolution, which bends not only toward complexity but also an ever greater capacity to evolve [26, 41, 49]?\nThis article is notable for several reasons. First, recursive self-improvement is a strong signal of progress toward AGI/SI; the Darwin G\u00f6del Machine (DGM) modifies its own code. Second, Schmidh\u00fcber\u2019s provably self-improving G\u00f6del Machine was theoretically bold but stalled on the self-proving piece. Zhang et al. use SOTA benchmarks (SWE-bench, Polyglot) to measure various improvements and therefore prove improvement operationally. They say their \u2018empirical proofs\u2019 weaken the G\u00f6del Machine formal approach \u2013 but I disagree \u2013 the formal methods, for example, cannot prove true statements that their axioms don\u2019t capture. Third, we hypothesize that progress toward AGI requires broadening the ML paradigm. Here Zhang et al. incorporate evolutionary programming to generate a population of coding agents and randomly sample them to find code improvements.\nThe most critical and important application of the most advanced AI is to apply to creating safety and value alignment.\nThus, the authors point out that self-improvement can be focused on the system\u2019s safety (which I call recursive safety improvement) and describe the safety precautions they followed.\nDarwinLM: Evolutionary Structured Pruning of Large Language Models\nThis second article enabling AI recursive self-improvement (RSI) also uses evolutionary search, in this case to"
    }
}