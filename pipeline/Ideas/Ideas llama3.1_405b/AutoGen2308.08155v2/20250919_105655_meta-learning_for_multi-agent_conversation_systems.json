{
    "query": "meta-learning for multi-agent conversation systems",
    "result": {
        "1": "Multi-Agent Meta-Reinforcement Learning for Self-Powered and Sustainable Edge Computing Systems. M. S. Munir, N. H. Tran, W. Saad, C. Hong. IEEE Transactions on Network and Service Management, 2020.\nNumber of citations: 26\nAbstract: The stringent requirements of mobile edge computing (MEC) applications and functions fathom the high capacity and dense deployment of MEC hosts to the upcoming wireless networks. However, operating such high capacity MEC hosts can significantly increase energy consumption. Thus, a base station (BS) unit can act as a self-powered BS. In this article, an effective energy dispatch mechanism for self-powered wireless networks with edge computing capabilities is studied. First, a two-stage linear stochastic programming problem is formulated with the goal of minimizing the total energy consumption cost of the system while fulfilling the energy demand. Second, a semi-distributed data-driven solution is proposed by developing a novel multi-agent meta-reinforcement learning (MAMRL) framework to solve the formulated problem. In particular, each BS plays the role of a local agent that explores a Markovian behavior for both energy consumption and generation while each BS transfers time-varying features to a meta-agent. Sequentially, the meta-agent optimizes (i.e., exploits) the energy dispatch decision by accepting only the observations from each local agent with its own state information. Meanwhile, each BS agent estimates its own energy dispatch policy by applying the learned parameters from meta-agent. Finally, the proposed MAMRL framework is benchmarked by analyzing deterministic, asymmetric, and stochastic environments in terms of non-renewable energy usages, energy cost, and accuracy. Experimental results show that the proposed MAMRL model can reduce up to 11% non-renewable energy usage and by 22.4% the energy cost (with 95.8% prediction accuracy), compared to other baseline methods.",
        "2": "Distributed Reptile Algorithm for Meta-Learning Over Multi-Agent Systems. Xianyang Zhang, Chen Hu, Bing He, Z. Han. IEEE Transactions on Signal Processing, 2022.\nNumber of citations: 16\nAbstract: Learning a good initialization model from distributed data sources over multi-agent systems is highly promising, in which the tasks or data are distributed stored and not accessible to all agents. This paper focuses on the distributed meta-learning problem and proposes a distributed Reptile meta-learning algorithm. In the proposed algorithm, each agent approximates the global model through a bi-level optimization scheme, where the inner step employs a stochastic gradient descent on a specific task, and the outer step utilizes information from neighbors and a gradient-like updating. The suggested algorithm avoids calculating the Hessian-vector products during training, reducing the computational complexity and affording memory miniaturization. We further analyze the convergence properties of the proposed algorithm under the convexity assumption. Finally, we demonstrate the effectiveness of the proposed algorithm on regression and classification tasks. The results show that our algorithm approximates a centralized solution and outperforms the non-cooperative algorithm.",
        "3": "Toward Adaptive and Coordinated Transportation Systems: A Multi-Personality Multi-Agent Meta-Reinforcement Learning Framework. Songjun Huang, Chuanneng Sun, Ruo-Qian Wang, D. Pompili. IEEE transactions on intelligent transportation systems (Print), 2025.\nNumber of citations: 8\nAbstract: Advancements in Intelligent Transportation Systems (ITS) have led to innovative solutions for planning optimization, efficiency enhancement, and resource allocation in transportation networks, which are demonstrated in applications such as smart parking lot management and electric vehicle (EV) charging station allocation, where improved decision-making and system-wide optimization have been achieved. However, as these systems evolve, the demand for better adaptability and coordination continues to grow to maximize their overall effectiveness and efficiency. To achieve this, we propose the Multi-Personality Multi-Agent Meta-Reinforcement Learning (MPMA-MRL) framework. This approach incorporates multiple meta-trained, meta-tested explainable personality policies, which are deployed to each agent. A personality selector is trained and deployed on each agent to optimize the overall performance. MPMA-MRL is superior than traditional methods in terms of the adaptability and coordination in ITS by leveraging improved information from the environment, more practical coordination among agents, faster adaptation speed to intermediate tasks, and more appropriate allocation and planning. The proposed framework is evaluated in the applications of parking lot optimization and EV charging station allocation. Its broader impact on multi-agent smart systems is analyzed to demonstrate its generalizability. The results demonstrate that in parking lot optimization, MPMA-MRL significantly reduces the time required to direct all vehicles to available parking spots. In EV charging station allocation, MPMA-MRL effectively minimizes waiting times at charging stations. Moreover, in both applications, MPMA-MRL exhibits enhanced adaptability to previously unseen scenarios, improving its applicability.",
        "4": "Multi-objective two-stage robust optimization of wind/PV/thermal power system based on meta multi-agent reinforcement learning. Dengao Li, Zhuokai Zhang, Ding Feng, Yu Zhou, Xiaodong Bai, Jumin Zhao. International Journal of Electrical Power &amp; Energy Systems, 2024.\nNumber of citations: 6\nAbstract: None",
        "5": "Hierarchical multi-agent reinforcement learning for cooperative tasks with sparse rewards in continuous domain. Jingyu Cao, Lu Dong, Xin Yuan, Yuanda Wang, Changyin Sun. Neural computing & applications (Print), 2023.\nNumber of citations: 3\nAbstract: None",
        "6": "Distributed unsupervised meta-learning algorithm over multi-agent systems. Zhenzhen Wang, Bing He, Zixin Jiang, Xianyang Zhang, Haidi Dong, Di Ye. Digital Communications and Networks, 2024.\nNumber of citations: 2\nAbstract: None",
        "7": "Multi-Agent Meta-Offline Reinforcement Learning for Timely UAV Path Planning and Data Collection. Eslam Eldeeb, Hirley Alves. arXiv.org, 2025.\nNumber of citations: 1\nAbstract: Multi-agent reinforcement learning (MARL) has been widely adopted in high-performance computing and complex data-driven decision-making in the wireless domain. However, conventional MARL schemes face many obstacles in real-world scenarios. First, most MARL algorithms are online, which might be unsafe and impractical. Second, MARL algorithms are environment-specific, meaning network configuration changes require model retraining. This letter proposes a novel meta-offline MARL algorithm that combines conservative Q-learning (CQL) and model agnostic meta-learning (MAML). CQL enables offline training by leveraging pre-collected datasets, while MAML ensures scalability and adaptability to dynamic network configurations and objectives. We propose two algorithm variants: independent training (M-I-MARL) and centralized training decentralized execution (M-CTDE-MARL). Simulation results show that the proposed algorithm outperforms conventional schemes, especially the CTDE approach that achieves 50 % faster convergence in dynamic scenarios than the benchmarks. The proposed framework enhances scalability, robustness, and adaptability in wireless communication systems by optimizing UAV trajectories and scheduling policies.",
        "8": "KnowPAML:A Knowledge Enhanced Framework for Adaptable Personalized Dialogue Generation Using Meta-Learning. Aditya Shukla, Zishan Ahmad, Asif Ekbal. ICON, 2022.\nNumber of citations: 0\nAbstract: None",
        "9": "Adaptive Multi-Agent Coordination among Different Team Attribute Tasks via Contextual Meta-Reinforcement Learning. Shangjing Huang, Zijie Zhao, Yuanheng Zhu, Dongbin Zhao. 2024 IEEE 13th Data Driven Control and Learning Systems Conference (DDCLS), 2024.\nNumber of citations: 0\nAbstract: In the realm of Multi-Agent Reinforcement Learning (MARL), the challenge of ensuring effective coordination in different multi -agent teams remains a significant hurdle. Existing methods often fall short in generalizing learned policies to novel team compositions, sizes, and capabilities. Addressing this gap, our study focuses on systems with variable and obscure attribute compositions, harnessing a context-based meta-reinforcement learning framework. The approach is twofold: context inference and context-based decision-making. Agents interpret historical data to identify the system's attribute composition, guiding their collective efforts. The accuracy of these inferences is crucial, prompting us to integrate contrastive learning to refine the context inference network via unsupervised training. In the process of decision-making, agents integrate the inferred context with the observation features to select the optimal strategy. Our empirical results underscore the method's efficacy in bolstering decision-making efficiency and precision amidst attribute diversity, marking a significant stride in adaptive teaming for robust multi-robot deployments in dynamic real-world scenarios.",
        "10": "Meta-Reinforcement Learning for Emergent Multi-Agent Languages in Zero-Shot Coordination Tasks. Raj Kashikar. International Conference on Multimodal Interaction, 2025.\nNumber of citations: 0\nAbstract: Recently, emergent communication protocols among agents have been increasingly applied to solve complex multiagent coordination tasks. However, most current approaches lack the ability to adapt quickly and efficiently to novel tasks and adversarial conditions without retraining. This paper introduces a new framework that integrates meta-reinforcement learning (meta-RL) with hierarchical reinforcement learning (HRL) to enable the development of emergent communication protocols by agents, which turn out to be robust, compositional, and adapt in a zero-shot manner to unseen tasks and perturbations. We concretely propose a meta-learning scheme that learns the prior over communication from a diverse set of training scenarios. These learned priors are used by agents at test time to rapidly infer new protocols suitable for unseen tasks or adversarial interference in communication channels. We perform detailed synthetic experiments on a suite of benchmark coordination problems under various adversarial conditions. Results indicate that our approach outperforms baseline methods in terms of zero-shot adaptation performance, resilience to noise, and crossdomain scalability. Our work opens up avenues for scalable, robust, and adaptive communication strategies in multi-agent systems, with wide-ranging implications for autonomous fleets, Internet of Things (IoT) networks, and beyond."
    }
}