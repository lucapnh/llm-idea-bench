{
    "query": "Graph Neural Network + Neurosymbolic Learning",
    "result": {
        "1": "The Graph Neural Network Model. F. Scarselli, M. Gori, A. Tsoi, M. Hagenbuchner, G. Monfardini. IEEE Transactions on Neural Networks, 2009.\nNumber of citations: 7872\nAbstract: None",
        "2": "Reconstructed Graph Neural Network With Knowledge Distillation for Lightweight Anomaly Detection. Xiaokang Zhou, Jiayi Wu, Wei Liang, K. Wang, Zheng Yan, Laurence T. Yang, Qun Jin. IEEE Transactions on Neural Networks and Learning Systems, 2024.\nNumber of citations: 46\nAbstract: The proliferation of Internet-of-Things (IoT) technologies in modern smart society enables massive data exchange for offering intelligent services. It becomes essential to ensure secure communications while exchanging highly sensitive IoT data efficiently, which leads to high demands for lightweight models or algorithms with limited computation capability provided by individual IoT devices. In this study, a graph representation learning model, which seamlessly incorporates graph neural network (GNN) and knowledge distillation (KD) techniques, named reconstructed graph with global\u2013local distillation (RG-GLD), is designed to realize the lightweight anomaly detection across IoT communication networks. In particular, a new graph network reconstruction strategy, which treats data communications as nodes in a directed graph while edges are then connected according to two specifically defined rules, is devised and applied to facilitate the graph representation learning in secure and efficient IoT communications. Both the structural and traffic features are then extracted from the graph data and flow data respectively, based on the graph attention network (GAT) and multilayer perceptron (MLP) techniques. These can benefit the GNN-based KD process in accordance with the more effective feature fusion and representation, considering both structural and data levels across the dynamic IoT networks. Furthermore, a lightweight local subgraph preservation mechanism improved by the graph attention mechanism and downsampling scheme to better utilize the topological information, and a so-called global information alignment defined based on the self-attention mechanism to effectively preserve the global information, are developed and incorporated in a refined graph attention based KD scheme. Compared with four different baseline methods, experiments and evaluations conducted based on two public datasets demonstrate the usefulness and effectiveness of our proposed model in improving the efficiency of knowledge transfer with higher classification accuracy but lower computational load, which can be deployed for lightweight anomaly detection in sustainable IoT computing environments.",
        "3": "3D graph neural network with few-shot learning for predicting drug-drug interactions in scaffold-based cold start scenario. Qiujie Lv, Jun Zhou, Ziduo Yang, Haohuai He, Calvin Yu\u2010Chian Chen. Neural Networks, 2023.\nNumber of citations: 27\nAbstract: None",
        "4": "GA-DRL: Graph Neural Network-Augmented Deep Reinforcement Learning for DAG Task Scheduling Over Dynamic Vehicular Clouds. Zhang Liu, Lianfeng Huang, Zhibin Gao, Manman Luo, Seyyedali Hosseinalipour, H. Dai. IEEE Transactions on Network and Service Management, 2023.\nNumber of citations: 22\nAbstract: Vehicular Clouds (VCs) are modern platforms for processing of computation-intensive tasks over vehicles. Such tasks are often represented as Directed Acyclic Graphs (DAGs) consisting of interdependent vertices/subtasks and directed edges. However, efficient scheduling of DAG tasks over VCs presents significant challenges, mainly due to the dynamic service provisioning of vehicles within VCs and non-Euclidean representation of DAG tasks\u2019 topologies. In this paper, we propose a Graph neural network-Augmented Deep Reinforcement Learning scheme (GA-DRL) for the timely scheduling of DAG tasks over dynamic VCs. In doing so, we first model the VC-assisted DAG task scheduling as a Markov decision process. We then adopt a multi-head Graph ATtention network (GAT) to extract the features of DAG subtasks. Our developed GAT enables a two-way aggregation of the topological information in a DAG task by simultaneously considering predecessors and successors of each subtask. We further introduce non-uniform DAG neighborhood sampling through codifying the scheduling priority of different subtasks, which makes our developed GAT generalizable to completely unseen DAG task topologies. Finally, we augment GAT into a double deep Q-network learning module to conduct subtask-to-vehicle assignment according to the extracted features of subtasks, while considering the dynamics and heterogeneity of the vehicles in VCs. Through simulating various DAG tasks under real-world movement traces of vehicles, we demonstrate that GA-DRL outperforms existing benchmarks in terms of DAG task completion time.",
        "5": "Graph Neural Network-Based Entity Extraction and Relationship Reasoning in Complex Knowledge Graphs. Junliang Du, Guiran Liu, Jia Gao, Xiaoxuan Liao, Jiacheng Hu, Linxiao Wu. 2024 International Conference on Image Processing, Computer Vision and Machine Learning (ICICML), 2024.\nNumber of citations: 16\nAbstract: This study proposed a knowledge graph entity extraction and relationship reasoning algorithm based on a graph neural network, using a graph convolutional network and graph attention network to model the complex structure in the knowledge graph. By building an end-to-end joint model, this paper achieves efficient recognition and reasoning of entities and relationships. In the experiment, this paper compared the model with a variety of deep learning algorithms and verified its superiority through indicators such as AUC, recall rate, precision rate, and F1 value. The experimental results show that the model proposed in this paper performs well in all indicators, especially in complex knowledge graphs, it has stronger generalization ability and stability. This provides strong support for further research on knowledge graphs and also demonstrates the application potential of graph neural networks in entity extraction and relationship reasoning.",
        "6": "MAG-GNN: Reinforcement Learning Boosted Graph Neural Network. Lecheng Kong, Jiarui Feng, Hao Liu, Dacheng Tao, Yixin Chen, Muhan Zhang. Neural Information Processing Systems, 2023.\nNumber of citations: 15\nAbstract: While Graph Neural Networks (GNNs) recently became powerful tools in graph learning tasks, considerable efforts have been spent on improving GNNs' structural encoding ability. A particular line of work proposed subgraph GNNs that use subgraph information to improve GNNs' expressivity and achieved great success. However, such effectivity sacrifices the efficiency of GNNs by enumerating all possible subgraphs. In this paper, we analyze the necessity of complete subgraph enumeration and show that a model can achieve a comparable level of expressivity by considering a small subset of the subgraphs. We then formulate the identification of the optimal subset as a combinatorial optimization problem and propose Magnetic Graph Neural Network (MAG-GNN), a reinforcement learning (RL) boosted GNN, to solve the problem. Starting with a candidate subgraph set, MAG-GNN employs an RL agent to iteratively update the subgraphs to locate the most expressive set for prediction. This reduces the exponential complexity of subgraph enumeration to the constant complexity of a subgraph search algorithm while keeping good expressivity. We conduct extensive experiments on many datasets, showing that MAG-GNN achieves competitive performance to state-of-the-art methods and even outperforms many subgraph GNNs. We also demonstrate that MAG-GNN effectively reduces the running time of subgraph GNNs.",
        "7": "GNNCL: A Graph Neural Network Recommendation Model Based on Contrastive Learning. Jinguang Chen, Jiahe Zhou, Lili Ma. Neural Processing Letters, 2024.\nNumber of citations: 5\nAbstract: In the field of recommendation algorithms, the representation learning for users and items has evolved from using single IDs or historical interactions to utilizing higher-order neighbors. This can be achieved by modeling the user\u2013item interaction graph to capture user preferences for items. Despite the promising results achieved by these algorithms, they still suffer from the issue of data sparsity. In order to mitigate the impact of data sparsity, contrastive learning has been adopted in graph collaborative filtering to enhance performance. However, current recommendation algorithms using contrastive learning yield uneven representations after data augmentation and do not consider the potential relationships among users (or items). To address these challenges, we propose a graph neural network-based recommendation model that integrates contrastive learning (GNNCL). This model combines data augmentation with added noise and the exploration of semantic neighbors for nodes. For the structural neighbors on the interaction graph, we introduce a novel and straightforward contrastive learning approach, abandoning previous graph augmentation methods, and introducing uniform noise into the embedding space to create contrastive views. To unearth potential semantic neighbor relationships in the semantic space, we assume that users with similar representations possess semantic neighbor relationships and merge these semantic neighbors into the prototype contrastive learning. We utilize a clustering algorithm to obtain prototypes for users and items and employ the EM algorithm for prototype contrastive learning. Experimental results validate the effectiveness of our approach. Particularly, on the Yelp2018 and Amazon-book datasets, our method exhibits significant performance improvements compared to basic graph collaborative filtering models.",
        "8": "Towards Probabilistic Inductive Logic Programming with Neurosymbolic Inference and Relaxation. Fieke Hillerstr\u00f6m, G. Burghouts. Theory and Practice of Logic Programming, 2024.\nNumber of citations: 4\nAbstract: Abstract Many inductive logic programming (ILP) methods are incapable of learning programs from probabilistic background knowledge, for example, coming from sensory data or neural networks with probabilities. We propose Propper, which handles flawed and probabilistic background knowledge by extending ILP with a combination of neurosymbolic inference, a continuous criterion for hypothesis selection (binary cross-entropy) and a relaxation of the hypothesis constrainer (NoisyCombo). For relational patterns in noisy images, Propper can learn programs from as few as 8 examples. It outperforms binary ILP and statistical models such as a graph neural network.",
        "9": "Semantic Loss Functions for Neuro-Symbolic Structured Prediction. Kareem Ahmed, Stefano Teso, Paolo Morettin, Luca Di Liello, Pierfrancesco Ardino, Jacopo Gobbi, Yitao Liang, Eric Wang, Kai-Wei Chang, Andrea Passerini, Guy Van den Broeck. Compendium of Neurosymbolic Artificial Intelligence, 2024.\nNumber of citations: 4\nAbstract: Structured output prediction problems are ubiquitous in machine learning. The prominent approach leverages neural networks as powerful feature extractors, otherwise assuming the independence of the outputs. These outputs, however, jointly encode an object, e.g. a path in a graph, and are therefore related through the structure underlying the output space. We discuss the semantic loss, which injects knowledge about such structure, defined symbolically, into training by minimizing the network's violation of such dependencies, steering the network towards predicting distributions satisfying the underlying structure. At the same time, it is agnostic to the arrangement of the symbols, and depends only on the semantics expressed thereby, while also enabling efficient end-to-end training and inference. We also discuss key improvements and applications of the semantic loss. One limitations of the semantic loss is that it does not exploit the association of every data point with certain features certifying its membership in a target class. We should therefore prefer minimum-entropy distributions over valid structures, which we obtain by additionally minimizing the neuro-symbolic entropy. We empirically demonstrate the benefits of this more refined formulation. Moreover, the semantic loss is designed to be modular and can be combined with both discriminative and generative neural models. This is illustrated by integrating it into generative adversarial networks, yielding constrained adversarial networks, a novel class of deep generative models able to efficiently synthesize complex objects obeying the structure of the underlying domain.",
        "10": "Neurosymbolic Reasoning: Building Neural Networks Using Datalog. Matt D. Scaccia, Ilaria Stocchi, Luigi Bellomarini. EDBT/ICDT Workshops, 2022.\nNumber of citations: 0\nAbstract: None"
    }
}