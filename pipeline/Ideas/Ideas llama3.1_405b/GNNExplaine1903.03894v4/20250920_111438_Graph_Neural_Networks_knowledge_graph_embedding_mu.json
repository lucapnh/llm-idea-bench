{
    "query": "Graph Neural Networks + knowledge graph embedding + multimodal learning",
    "result": {
        "1": "Learning Knowledge Graph Embedding With Heterogeneous Relation Attention Networks. Zhifei Li, Hai Liu, Zhaoli Zhang, Tingting Liu, N. Xiong. IEEE Transactions on Neural Networks and Learning Systems, 2021.\nNumber of citations: 244\nAbstract: Knowledge graph (KG) embedding aims to study the embedding representation to retain the inherent structure of KGs. Graph neural networks (GNNs), as an effective graph representation technique, have shown impressive performance in learning graph embedding. However, KGs have an intrinsic property of heterogeneity, which contains various types of entities and relations. How to address complex graph data and aggregate multiple types of semantic information simultaneously is a critical issue. In this article, a novel heterogeneous GNNs framework based on attention mechanism is proposed. Specifically, the neighbor features of an entity are first aggregated under each relation-path. Then the importance of different relation-paths is learned through the relation features. Finally, each relation-path-based features with the learned weight values are aggregated to generate the embedding representation. Thus, the proposed method not only aggregates entity features from different semantic aspects but also allocates appropriate weights to them. This method can capture various types of semantic information and selectively aggregate informative features. The experiment results on three real-world KGs demonstrate superior performance when compared with several state-of-the-art methods.",
        "2": "Knowledge Enhanced Graph Neural Networks for Explainable Recommendation. Ziyu Lyu, Yue Wu, Junjie Lai, Min Yang, Chengming Li, Wei Zhou. IEEE Transactions on Knowledge and Data Engineering, 2023.\nNumber of citations: 62\nAbstract: Recently, explainable recommendation has attracted increasing attentions, which can make the recommender system more transparent and improve user satisfactions by recommending products with useful explanations. However, existing methods trend to trade-off between the recommendation accuracy and the interpretability of recommendation results. In this manuscript, we propose Knowledge Enhanced Graph Neural Networks (KEGNN) for explainable recommendation. Semantic knowledge from the external knowledge base is leveraged into representation learning of three sides, respectively user, items and user-item interactions, and the knowledge enhanced semantic embedding are exploited to initialize the user/item entities and user-item relations of one constructed user behavior graph. We design a graph neural networks based user behavior learning and reasoning model to perform both semantic and relational knowledge propagation and reasoning over the user behavior graph for comprehensive understanding of user behaviors. On the top of comprehensive representations of users/items and user-item interactions, hierarchical neural collaborative filtering layers are developed for precise rating prediction, and one generation-mode and copy-mode combined generator is devised for human-like semantic explanation generation by integrating the copy mechanism into gated recurrent neural networks. Quantitative and qualitative results demonstrate the superiority of KEGNN over the state-of-art methods, and the explainability and interpretability of our method.",
        "3": "Graph Neural Networks for Multimodal Single-Cell Data Integration. Haifang Wen, Jiayuan Ding, Wei Jin, Yuying Xie, Jiliang Tang. Knowledge Discovery and Data Mining, 2022.\nNumber of citations: 59\nAbstract: Recent advances in multimodal single-cell technologies have enabled simultaneous acquisitions of multiple omics data from the same cell, providing deeper insights into cellular states and dynamics. However, it is challenging to learn the joint representations from the multimodal data, model the relationship between modalities, and, more importantly, incorporate the vast amount of single-modality datasets into the downstream analyses. To address these challenges and correspondingly facilitate multimodal single-cell data analyses, three key tasks have been introduced: Modality prediction, Modality matching andJoint embedding. In this work, we present a general Graph Neural Network framework scMoGNN to tackle these three tasks and show that scMoGNN demonstrates superior results in all three tasks compared with the state-of-the-art and conventional approaches. Our method is an official winner in the overall ranking ofModality prediction from NeurIPS 2021 Competition (https://openproblems.bio/neurips_2021/), and all implementations of our methods have been integrated into DANCE package (https://github.com/OmicsML/dance).",
        "4": "Knowledge Graph Embedding Based on Graph Neural Network. Shuang Liang. IEEE International Conference on Data Engineering, 2023.\nNumber of citations: 10\nAbstract: The representation of semantic information pertaining to the real world has been active research for some time now. Among the available methods, knowledge graphs have emerged as a widely accepted approach. Meanwhile, graph neural networks (GNNs) have demonstrated excellent performance in embedding graph-based information. Given the natural graph structure of knowledge graphs, employing GNNs to embed them is expected to yield a more interpretable and trustworthy representation of the learned knowledge. In this paper, we propose three customized GNNs for different scenarios of knowledge graph representation, including traditional, multimodal, and uncertain knowledge graphs. In the traditional knowledge graph scenario, we present a graph self-supervised learning method, named deep relation graph infomax (DRGI), which incorporates both the complete graph structure information and semantic information. In the multimodal knowledge graph scenario, we introduce a novel network, named hyper-node relational graph attention network (HRGAT), which combines different modal information with graph structure information for a more precise representation of multimodal knowledge graphs. In the uncertain knowledge graph scenario, we define a novel message-passing paradigm with box embedding, named box graph neural network (BGNN). BGNN leverages both the graph structure information of uncertain knowledge graphs and the probabilistic semantics of box embedding. To validate the effectiveness of our proposed methods, we conduct a series of experiments and report the results. We also discuss possible future work in GNN-based knowledge graph embedding.",
        "5": "A Knowledge-Graph-Based Multimodal Deep Learning Framework for Identifying Drug\u2013Drug Interactions. Jing Zhang, Meng Chen, Jie Liu, Dongdong Peng, Zong Dai, Xiaoyong Zou, Zhanchao Li. Molecules, 2023.\nNumber of citations: 10\nAbstract: The identification of drug\u2013drug interactions (DDIs) plays a crucial role in various areas of drug development. In this study, a deep learning framework (KGCN_NFM) is presented to recognize DDIs using coupling knowledge graph convolutional networks (KGCNs) with neural factorization machines (NFMs). A KGCN is used to learn the embedding representation containing high-order structural information and semantic information in the knowledge graph (KG). The embedding and the Morgan molecular fingerprint of drugs are then used as input of NFMs to predict DDIs. The performance and effectiveness of the current method have been evaluated and confirmed based on the two real-world datasets with different sizes, and the results demonstrate that KGCN_NFM outperforms the state-of-the-art algorithms. Moreover, the identified interactions between topotecan and dantron by KGCN_NFM were validated through MTT assays, apoptosis experiments, cell cycle analysis, and molecular docking. Our study shows that the combination therapy of the two drugs exerts a synergistic anticancer effect, which provides an effective treatment strategy against lung carcinoma. These results reveal that KGCN_NFM is a valuable tool for integrating heterogeneous information to identify potential DDIs.",
        "6": "Fine-Grained Learning Behavior-Oriented Knowledge Distillation for Graph Neural Networks. Kang Liu, Zhenhua Huang, Changdong Wang, Beibei Gao, Yunwen Chen. IEEE Transactions on Neural Networks and Learning Systems, 2024.\nNumber of citations: 7\nAbstract: Knowledge distillation (KD), as an effective compression technology, is used to reduce the resource consumption of graph neural networks (GNNs) and facilitate their deployment on resource-constrained devices. Numerous studies exist on GNN distillation, and however, the impacts of knowledge complexity and differences in learning behavior between teachers and students on distillation efficiency remain underexplored. We propose a KD method for fine-grained learning behavior (FLB), comprising two main components: feature knowledge decoupling (FKD) and teacher learning behavior guidance (TLBG). Specifically, FKD decouples the intermediate-layer features of the student network into two types: teacher-related features (TRFs) and downstream features (DFs), enhancing knowledge comprehension and learning efficiency by guiding the student to simultaneously focus on these features. TLBG maps the teacher model\u2019s learning behaviors to provide reliable guidance for correcting deviations in student learning. Extensive experiments across eight datasets and 12 baseline frameworks demonstrate that FLB significantly enhances the performance and robustness of student GNNs within the original framework.",
        "7": "Graph Convolutional Multi-Label Hashing for Cross-Modal Retrieval. Xiaobo Shen, Yinfan Chen, Weiwei Liu, Yuhui Zheng, Quansen Sun, Shirui Pan. IEEE Transactions on Neural Networks and Learning Systems, 2024.\nNumber of citations: 3\nAbstract: Cross-modal hashing encodes different modalities of multimodal data into low-dimensional Hamming space for fast cross-modal retrieval. In multi-label cross-modal retrieval, multimodal data are often annotated with multiple labels, and some labels, e.g., \u201cocean\u201d and \u201ccloud,\u201d often co-occur. However, existing cross-modal hashing methods overlook label dependency that is crucial for improving performance. To fulfill this gap, this article proposes graph convolutional multi-label hashing (GCMLH) for effective multi-label cross-modal retrieval. Specifically, GCMLH first generates word embedding of each label and develops label encoder to learn highly correlated label embedding via graph convolutional network (GCN). In addition, GCMLH develops feature encoder for each modality, and feature fusion module to generate highly semantic feature via GCN. GCMLH uses teacher-student learning scheme to transfer knowledge from the teacher modules, i.e., label encoder and feature fusion module, to the student module, i.e., feature encoder, such that learned hash code can well exploit multi-label dependency and multimodal semantic structure. Extensive empirical results on several benchmarks demonstrate the superiority of the proposed method over existing state-of-the-arts.",
        "8": "Multimodal Learning Data Intelligent Personalized Learning Resource Recommendation System for Web-Based Classrooms. Shan Wang. 2024 6th International Conference on Artificial Intelligence and Computer Applications (ICAICA), 2024.\nNumber of citations: 1\nAbstract: Traditional recommender systems are difficult to fully explore the complex correlation between multimodal data and cannot meet learners' personalized needs. To this end, this paper proposes an intelligent personalized learning resource recommendation system (MDRS) based on multimodal data, which achieves accurate recommendation of learning resources by integrating advanced technologies such as natural language processing, computer vision, and deep learning, and at the same time, by introducing a multimodal data processing mechanism and personalized knowledge graph construction. First, we introduce a multimodal feature embedding module for the model, which utilizes deep neural networks for feature extraction and embedding of data in different modalities, such as text, image, audio, and video, in order to capture the multidimensional characteristics of resources. Subsequently, through the cross-modal fusion and attention mechanism module, the model performs weighted fusion of multimodal features to strengthen the expression of features related to user learning needs. By dynamically adjusting the weights of each modality, the system is able to automatically focus on the features that are most important for learning resource recommendation and eliminate redundant information. This mechanism ensures the system's adaptive fusion of different modalities, allowing multimodal information to be deeply related at the feature level. Next, we design a personalized knowledge graph module to associate users' learning behaviors with resource knowledge points to provide background knowledge support for the recommendation process, which updates the knowledge graph in real time by tracking learners' learning paths, preferences, and knowledge mastery and optimizes the recommended content based on learners' dynamic needs. Finally, through the recommendation optimization module, the recommendation strategy is dynamically adjusted in combination with user feedback to enhance the learning effect. The experimental results show that the proposed recommendation system outperforms traditional methods in terms of prediction accuracy, and can significantly enhance the learning experience and learning efficiency.",
        "9": "A Multimodal Knowledge Graph Representation Learning Method Based on Hyperplane Embedding. Weixing Zhang, Tianyu Du, Cong Yang, Xuexiang Li. 2024 7th International Conference on Advanced Algorithms and Control Engineering (ICAACE), 2024.\nNumber of citations: 1\nAbstract: Most existing methods for knowledge graph representation learning primarily focus on structured information and overlook the potential benefits of incorporating multimodal information [14]. Moreover, relying solely on structural triples for learning poses challenges such as insufficient feature semantics, significant gaps between related entities, and low similarity [13]. To address these issues, this paper proposes a multi-modal knowledge graph representation learning method based on hyperplane embedding. Firstly, a graph neural network is used as the structural encoder to learn entity embeddings, while a pre-trained visual model is employed as the image encoder to learn image embeddings. Next, the relationships in each triple are mapped onto hyperplanes, and the entity and visual representations are projected onto the hyperplanes of the relationships to address the multi-relationship data problem. Finally, a cross-translation distance function is utilized to evaluate the probab-ility of the authenticity of each triple and perform link prediction tasks. Experimental results demonstrate the superiority of this approach, with a 0.87% improvement in Hits@10 on the WN18-IMG dataset and a 14.5% improvement on the FB15K-IMG dataset compared to similar models.",
        "10": "Content-based Art Recommendation Using Multimodal Graph Neural Networks. Haimonti Dutta, Anushka Tiwari. 2024 IEEE International Conference on Knowledge Graph (ICKG), 2024.\nNumber of citations: 0\nAbstract: Graph Neural Network (GNN)-based recommendation systems have become very popular in recent years. Their popularity stems from the fact that nodes can access higher-order neighbor information and there are well-designed algorithms for embedding, message passing, and propagation. In this paper, we present the design of a GNN-based recommendation system on a novel data set collected from field research. Designed for an endangered performing art form, the recommendation system uses multimodal text and image data to suggest similar panels (paintings) to the end-user of the system. We show that multimodal data is particularly helpful in graph representation learning, message passing, and propagation and the panels recommended to users using content-based features are similar to those obtained from systems using user-item bipartite graphs such as MMGCN and GRCN."
    }
}