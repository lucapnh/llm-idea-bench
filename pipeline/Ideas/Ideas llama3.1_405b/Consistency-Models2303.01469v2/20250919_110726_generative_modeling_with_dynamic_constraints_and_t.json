{
    "query": "generative modeling with dynamic constraints and temporal consistency",
    "result": {
        "1": "Gaussian Process Latent Variable Models for Human Pose Estimation. C. Ek, P. Torr, Neil D. Lawrence. Machine Learning for Multimodal Interaction, 2007.\nNumber of citations: 175\nAbstract: None",
        "2": "Grounding Creativity in Physics: A Brief Survey of Physical Priors in AIGC. Siwei Meng, Yawei Luo, Ping Liu. arXiv.org, 2025.\nNumber of citations: 8\nAbstract: Recent advancements in AI-generated content have significantly improved the realism of 3D and 4D generation. However, most existing methods prioritize appearance consistency while neglecting underlying physical principles, leading to artifacts such as unrealistic deformations, unstable dynamics, and implausible objects interactions. Incorporating physics priors into generative models has become a crucial research direction to enhance structural integrity and motion realism. This survey provides a review of physics-aware generative methods, systematically analyzing how physical constraints are integrated into 3D and 4D generation. First, we examine recent works in incorporating physical priors into static and dynamic 3D generation, categorizing methods based on representation types, including vision-based, NeRF-based, and Gaussian Splatting-based approaches. Second, we explore emerging techniques in 4D generation, focusing on methods that model temporal dynamics with physical simulations. Finally, we conduct a comparative analysis of major methods, highlighting their strengths, limitations, and suitability for different materials and motion dynamics. By presenting an in-depth analysis of physics-grounded AIGC, this survey aims to bridge the gap between generative models and physical realism, providing insights that inspire future research in physically consistent content generation.",
        "3": "DiST-4D: Disentangled Spatiotemporal Diffusion with Metric Depth for 4D Driving Scene Generation. Jiazhe Guo, Yikang Ding, Xiwu Chen, Shuo Chen, Bohan Li, Yingshuang Zou, Xiaoyang Lyu, Feiyang Tan, Xiaojuan Qi, Zhiheng Li, Hao Zhao. arXiv.org, 2025.\nNumber of citations: 7\nAbstract: Current generative models struggle to synthesize dynamic 4D driving scenes that simultaneously support temporal extrapolation and spatial novel view synthesis (NVS) without per-scene optimization. A key challenge lies in finding an efficient and generalizable geometric representation that seamlessly connects temporal and spatial synthesis. To address this, we propose DiST-4D, the first disentangled spatiotemporal diffusion framework for 4D driving scene generation, which leverages metric depth as the core geometric representation. DiST-4D decomposes the problem into two diffusion processes: DiST-T, which predicts future metric depth and multi-view RGB sequences directly from past observations, and DiST-S, which enables spatial NVS by training only on existing viewpoints while enforcing cycle consistency. This cycle consistency mechanism introduces a forward-backward rendering constraint, reducing the generalization gap between observed and unseen viewpoints. Metric depth is essential for both accurate reliable forecasting and accurate spatial NVS, as it provides a view-consistent geometric representation that generalizes well to unseen perspectives. Experiments demonstrate that DiST-4D achieves state-of-the-art performance in both temporal prediction and NVS tasks, while also delivering competitive performance in planning-related evaluations.",
        "4": "Region-Adaptive Sampling for Diffusion Transformers. Ziming Liu, Yifan Yang, Chengruidong Zhang, Yiqi Zhang, Lili Qiu, Yang You, Yuqing Yang. arXiv.org, 2025.\nNumber of citations: 5\nAbstract: Diffusion models (DMs) have become the leading choice for generative tasks across diverse domains. However, their reliance on multiple sequential forward passes significantly limits real-time performance. Previous acceleration methods have primarily focused on reducing the number of sampling steps or reusing intermediate results, failing to leverage variations across spatial regions within the image due to the constraints of convolutional U-Net structures. By harnessing the flexibility of Diffusion Transformers (DiTs) in handling variable number of tokens, we introduce RAS, a novel, training-free sampling strategy that dynamically assigns different sampling ratios to regions within an image based on the focus of the DiT model. Our key observation is that during each sampling step, the model concentrates on semantically meaningful regions, and these areas of focus exhibit strong continuity across consecutive steps. Leveraging this insight, RAS updates only the regions currently in focus, while other regions are updated using cached noise from the previous step. The model's focus is determined based on the output from the preceding step, capitalizing on the temporal consistency we observed. We evaluate RAS on Stable Diffusion 3 and Lumina-Next-T2I, achieving speedups up to 2.36x and 2.51x, respectively, with minimal degradation in generation quality. Additionally, a user study reveals that RAS delivers comparable qualities under human evaluation while achieving a 1.6x speedup. Our approach makes a significant step towards more efficient diffusion transformers, enhancing their potential for real-time applications.",
        "5": "An Adaptive X\u2010Ray Dynamic Image Estimation Method Based on OMNI Solar Wind Parameters and SXI Simulated Observations. R. C. Wang, Anders M. Jorgensen, Dalin Li, Tianran Sun, Zhen Yang, Xiaodong Peng. Space Weather, 2024.\nNumber of citations: 1\nAbstract: Observations of the overall interactions between solar wind and the Earth's magnetosphere are crucial for space weather monitoring. Upcoming missions like the Solar Wind Magnetosphere Ionosphere Link Explorer (SMILE) and the Lunar Environment heliosphere X\u2010ray Imager (LEXI) aim to make comprehensive global imaging of Earth's magnetosphere using soft X\u2010ray imager (SXI) in order to understand its dynamic response to solar wind impact. Short\u2010duration X\u2010ray images have a low signal\u2010to\u2010noise ratio (SNR), limited by cosmic background and Poisson noise. Longer integration times provide better SNR of magnetospheric structures but fail to capture the short\u2010term dynamics during the integration. Our study introduces a neural network method which is able to estimate the short\u2010term dynamics during a long integration, driven by OMNI solar wind data and simulated soft X\u2010ray images. Specifically, an adaptive X\u2010ray image estimator and a spatio\u2010temporal discriminator are used. It leverages X\u2010ray models like Magnetohydrodynamic (MHD) and Jorgensen & Sun model, driven by OMNI data to provide high\u2010temporal\u2010resolution prior information on magnetosphere motion, with SXI observation images acting as a posterior constraint on the magnetosphere's state. Experimental validation demonstrates apparent improvements in Peak signal\u2010to\u2010noise ratio (PSNR) and Structural Similarity (SSIM) compared to traditional linear and optical flow interpolation methods. The method's flexibility, considering input\u2010output consistency, enables easy extension to any interval (>3 min), meeting diverse application needs. In conclusion, our study presents a new approach to soft X\u2010ray image estimation based on neural networks, providing insights into magnetospheric dynamics as observed in soft X\u2010rays.",
        "6": "Dynamical Vision, ICCV 2005 and ECCV 2006 Workshops, WDV 2005 and WDV 2006, Beijing, China, October 21, 2005, Graz, Austria, May 13, 2006. Revised Papers. R. Vidal, A. Heyden, Yi Ma. WDV, 2007.\nNumber of citations: 1\nAbstract: None",
        "7": "AFTER THE REVOLUTIONS OF GESTALT AND ACTION CONTROL THEORIES: IS THERE A PSYCHOPHYSICS OF TASK-RELATED STRUCTURAL REPRESENTATIONS? A DISCUSSION BASED UPON EVIDENCE FROM VISUAL CATEGORIZATION. H. Geissler. , 2001.\nNumber of citations: 1\nAbstract: None",
        "8": "Domain generalization based on dynamic style enhancement and dual consistency constraint network for hyperspectral image cross-scene classification. Hongfei Cai, Haoyang Yu, Yao Liu, Xinran An, Chenchao Xiao, Jiaochan Hu. International Journal of Remote Sensing, 2025.\nNumber of citations: 0\nAbstract: ABSTRACT As the capacity to acquire hyperspectral data continues to grow, efficient cross-scene recognition has emerged as a critical challenge for classification tasks spanning different regions yet involving the same land cover categories. Due to the distribution differences between the source domain (SD) and target domain (TD), existing models often fail to maintain strong classification performance in target domains which lack prior information. Domain Generalization (DG), as a critical technique for cross-scene hyperspectral image classification, aims to train models using only SD data and directly generalize to other TDs, thus avoiding the dependence on TD data. However, current mainstream DG methods often expand sample diversity through data generation techniques that lack controlled factors, resulting in the loss of crucial semantic information. Moreover, when the potential structural information of the samples is overlooked, generated data may introduce unrealistic and ambiguous samples with interfering information. To address these issues, this paper proposes a hyperspectral image domain generalization classification network based on dynamic style enhancement and dual consistency constraints, DSEDC2net. Specifically, we design a generator based on dynamic style enhancement, utilizing semantic factors to ensure that generative domain (GD) samples retain essential semantic information from the SD while maintaining style diversity. Additionally, an improved auxiliary domain (AD) synthesis strategy guided by category feature manifolds is introduced to provide reliable auxiliary samples for extracting domain-invariant representations. Furthermore, structural consistency constraints are incorporated into the discriminator\u2019s adversarial training to ensure the latent structure of generated samples remains consistent with the SD samples, optimizing the learning of domain-invariant knowledge through dual consistency constraints. Extensive experiments on three cross-scene datasets with different variances (the cross-temporal Houston dataset, the cross-regional Pavia dataset, and the cross-sensor Yancheng dataset) demonstrate that the proposed method exhibits superior generalization ability across multiple datasets, validating its effectiveness and reliability.",
        "9": "SmokeSVD: Smoke Reconstruction from A Single View via Progressive Novel View Synthesis and Refinement with Diffusion Models. Chen Li, Shanshan Dong, Sheng Qiu, Jianmin Han, Zan Gao, Kemeng Huang, Taku Komura. arXiv.org, 2025.\nNumber of citations: 0\nAbstract: Reconstructing dynamic fluids from sparse views is a long-standing and challenging problem, due to the severe lack of 3D information from insufficient view coverage. While several pioneering approaches have attempted to address this issue using differentiable rendering or novel view synthesis, they are often limited by time-consuming optimization and refinement processes under ill-posed conditions. To tackle above challenges, we propose SmokeSVD, an efficient and effective framework to progressively generate and reconstruct dynamic smoke from a single video by integrating both the powerful generative capabilities from diffusion models and physically guided consistency optimization towards realistic appearance and dynamic evolution. Specifically, we first propose a physically guided side-view synthesizer based on diffusion models, which explicitly incorporates divergence and gradient guidance of velocity fields to generate visually realistic and spatio-temporally consistent side-view images frame by frame, significantly alleviating the ill-posedness of single-view reconstruction without imposing additional constraints. Subsequently, we determine a rough estimation of density field from the pair of front-view input and side-view synthetic image, and further refine 2D blurry novel-view images and 3D coarse-grained density field through an iterative process that progressively renders and enhances the images from increasing novel viewing angles, generating high-quality multi-view image sequences. Finally, we reconstruct and estimate the fine-grained density field, velocity field, and smoke source via differentiable advection by leveraging the Navier-Stokes equations. Extensive quantitative and qualitative experiments show that our approach achieves high-quality reconstruction and outperforms previous state-of-the-art techniques.",
        "10": "Symbolic and quantitative approaches to reasoning with uncertainty : 9th European Conference, ECSQARU 2007, Hammamet, Tunisia, October 31 - November 2, 2007 : proceedings. Quantitative Approaches to Reasoning, Khaled Mellouli. , 2007.\nNumber of citations: 0\nAbstract: None"
    }
}