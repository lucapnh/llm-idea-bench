{
    "query": "generative modeling with dynamic consistency constraints",
    "result": {
        "1": "Is Conditional Generative Modeling all you need for Decision-Making?. Anurag Ajay, Yilun Du, Abhi Gupta, J. Tenenbaum, T. Jaakkola, Pulkit Agrawal. International Conference on Learning Representations, 2022.\nNumber of citations: 435\nAbstract: Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone. We investigate whether these methods can directly address the problem of sequential decision-making. We view decision-making not through the lens of reinforcement learning (RL), but rather through conditional generative modeling. To our surprise, we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks. By modeling a policy as a return-conditional diffusion model, we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL. We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables: constraints and skills. Conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills. Our results illustrate that conditional generative modeling is a powerful tool for decision-making.",
        "2": "SV4D: Dynamic 3D Content Generation with Multi-Frame and Multi-View Consistency. Yiming Xie, Chun-Han Yao, Vikram S. Voleti, Huaizu Jiang, Varun Jampani. International Conference on Learning Representations, 2024.\nNumber of citations: 73\nAbstract: We present Stable Video 4D (SV4D), a latent video diffusion model for multi-frame and multi-view consistent dynamic 3D content generation. Unlike previous methods that rely on separately trained generative models for video generation and novel view synthesis, we design a unified diffusion model to generate novel view videos of dynamic 3D objects. Specifically, given a monocular reference video, SV4D generates novel views for each video frame that are temporally consistent. We then use the generated novel view videos to optimize an implicit 4D representation (dynamic NeRF) efficiently, without the need for cumbersome SDS-based optimization used in most prior works. To train our unified novel view video generation model, we curate a dynamic 3D object dataset from the existing Objaverse dataset. Extensive experimental results on multiple datasets and user studies demonstrate SV4D's state-of-the-art performance on novel-view video synthesis as well as 4D generation compared to prior works.",
        "3": "Generative Modeling of Regular and Irregular Time Series Data via Koopman VAEs. Ilan Naiman, N. Benjamin Erichson, Pu Ren, Lbnl Michael W. Mahoney ICSI, UC Berkeley, Omri Azencot. International Conference on Learning Representations, 2023.\nNumber of citations: 28\nAbstract: Generating realistic time series data is important for many engineering and scientific applications. Existing work tackles this problem using generative adversarial networks (GANs). However, GANs are unstable during training, and they can suffer from mode collapse. While variational autoencoders (VAEs) are known to be more robust to the these issues, they are (surprisingly) less considered for time series generation. In this work, we introduce Koopman VAE (KoVAE), a new generative framework that is based on a novel design for the model prior, and that can be optimized for either regular and irregular training data. Inspired by Koopman theory, we represent the latent conditional prior dynamics using a linear map. Our approach enhances generative modeling with two desired features: (i) incorporating domain knowledge can be achieved by leveraging spectral tools that prescribe constraints on the eigenvalues of the linear map; and (ii) studying the qualitative behavior and stability of the system can be performed using tools from dynamical systems theory. Our results show that KoVAE outperforms state-of-the-art GAN and VAE methods across several challenging synthetic and real-world time series generation benchmarks. Whether trained on regular or irregular data, KoVAE generates time series that improve both discriminative and predictive metrics. We also present visual evidence suggesting that KoVAE learns probability density functions that better approximate the empirical ground truth distribution.",
        "4": "DreamDrive: Generative 4D Scene Modeling from Street View Images. Jiageng Mao, Boyi Li, B. Ivanovic, Yuxiao Chen, Yan Wang, Yurong You, Chaowei Xiao, Danfei Xu, Marco Pavone, Yue Wang. IEEE International Conference on Robotics and Automation, 2024.\nNumber of citations: 11\nAbstract: Synthesizing photo-realistic visual observations from an ego vehicle's driving trajectory is a critical step towards scalable training of self-driving models. Reconstruction-based methods create 3D scenes from driving logs and synthesize geometry-consistent driving videos through neural rendering, but their dependence on costly object annotations limits their ability to generalize to in-the-wild driving scenarios. On the other hand, generative models can synthesize action-conditioned driving videos in a more generalizable way but often struggle with maintaining 3D visual consistency. In this paper, we present DreamDrive, a 4D spatial-temporal scene generation approach that combines the merits of generation and reconstruction, to synthesize generalizable 4D driving scenes and dynamic driving videos with 3D consistency. Specifically, we leverage the generative power of video diffusion models to synthesize a sequence of visual references and further elevate them to 4D with a novel hybrid Gaussian representation. Given a driving trajectory, we then render 3D-consistent driving videos via Gaussian splatting. The use of generative priors allows our method to produce high-quality 4D scenes from in-the-wild driving data, while neural rendering ensures 3D-consistent video generation from the 4D scenes. Extensive experiments on nuScenes and in-the-wild driving data demonstrate that DreamDrive can generate controllable and generalizable 4D driving scenes, synthesize novel views of driving videos with high fidelity and 3D consistency, decompose static and dynamic elements in a self-supervised manner, and enhance perception and planning tasks for autonomous driving.",
        "5": "Score-based Generative Models for Photoacoustic Image Reconstruction with Rotation Consistency Constraints. Shangqing Tong, Hengrong Lan, Liming Nie, Jianwen Luo, Fei Gao. arXiv.org, 2023.\nNumber of citations: 4\nAbstract: Photoacoustic tomography (PAT) is a newly emerged imaging modality which enables both high optical contrast and acoustic depth of penetration. Reconstructing images of photoacoustic tomography from limited amount of senser data is among one of the major challenges in photoacoustic imaging. Previous works based on deep learning were trained in supervised fashion, which directly map the input partially known sensor data to the ground truth reconstructed from full field of view. Recently, score-based generative models played an increasingly significant role in generative modeling. Leveraging this probabilistic model, we proposed Rotation Consistency Constrained Score-based Generative Model (RCC-SGM), which recovers the PAT images by iterative sampling between Langevin dynamics and a constraint term utilizing the rotation consistency between the images and the measurements. Our proposed method can generalize to different measurement processes (32.29 PSNR with 16 measurements under random sampling, whereas 28.50 for supervised counterpart), while supervised methods need to train on specific inverse mappings.",
        "6": "Smooth densities and generative modeling with unsupervised random forests. David Watson, Kristin Blesch, Jan Kapar, Marvin N. Wright. arXiv.org, 2022.\nNumber of citations: 2\nAbstract: None",
        "7": "Topology-Aware Piecewise Linearization of the AC Power Flow Through Generative Modeling. Young-Ho Cho, Hao Zhu. North American Power Symposium, 2023.\nNumber of citations: 2\nAbstract: Effective power flow modeling critically affects the ability to efficiently solve large-scale grid optimization problems, especially those with topology-related decision variables. In this work, we put forth a generative modeling approach to obtain a piecewise linear (PWL) approximation of AC power flow by training a simple neural network model from actual data samples. By using the ReLU activation, the NN models can produce a PWL mapping from the input voltage magnitudes and angles to the output power flow and injection. Our proposed generative PWL model uniquely accounts for the nonlinear and topology-related couplings of power flow models, and thus it can greatly improve the accuracy and consistency of output power variables. Most importantly, it enables to reformulate the nonlinear power flow and line status-related constraints into mixed-integer linear ones, such that one can efficiently solve grid topology optimization tasks like the AC optimal transmission switching (OTS) problem. Numerical tests using the IEEE 14- and 118-bus test systems have demonstrated the modeling accuracy of the proposed PWL approximation using a generative approach, as well as its ability in enabling competitive OTS solutions at very low computation order.",
        "8": "Sparse Causal Discovery with Generative Intervention for Unsupervised Graph Domain Adaptation. Junyu Luo, Yuhao Tang, Yiwei Fu, Xiao Luo, Zhizhuo Kou, Zhiping Xiao, Wei Ju, Wentao Zhang, Ming Zhang. arXiv.org, 2025.\nNumber of citations: 0\nAbstract: Unsupervised Graph Domain Adaptation (UGDA) leverages labeled source domain graphs to achieve effective performance in unlabeled target domains despite distribution shifts. However, existing methods often yield suboptimal results due to the entanglement of causal-spurious features and the failure of global alignment strategies. We propose SLOGAN (Sparse Causal Discovery with Generative Intervention), a novel approach that achieves stable graph representation transfer through sparse causal modeling and dynamic intervention mechanisms. Specifically, SLOGAN first constructs a sparse causal graph structure, leveraging mutual information bottleneck constraints to disentangle sparse, stable causal features while compressing domain-dependent spurious correlations through variational inference. To address residual spurious correlations, we innovatively design a generative intervention mechanism that breaks local spurious couplings through cross-domain feature recombination while maintaining causal feature semantic consistency via covariance constraints. Furthermore, to mitigate error accumulation in target domain pseudo-labels, we introduce a category-adaptive dynamic calibration strategy, ensuring stable discriminative learning. Extensive experiments on multiple real-world datasets demonstrate that SLOGAN significantly outperforms existing baselines.",
        "9": "Integrating CLIP with Dynamic Memory Generative Adversarial Networks to Enhance Semantic Consistency in Text-to-Image Generation. Dongxia Yan, Xien Cheng. 2024 5th International Conference on Computer, Big Data and Artificial Intelligence (ICCBD+AI), 2024.\nNumber of citations: 0\nAbstract: This paper focuses on the challenge of a lack of strong semantic coherence between generated images and their corresponding text descriptions, as well as the low quality of initial image generation and limited detail capture in the Dynamic Memory Generative Adversarial Networks model. To tackle these issues, we propose the DM-CLGAN model, an enhanced variant of DM-GAN. By incorporating the CLIP model, DM-CLGAN achieves improved alignment between textual and image features. Furthermore, it introduces the Text-Image Affine Combination Module and the Convolutional Block Attention Module to optimize image details and enhance the quality of the initially generated images. In the second stage of the model, the dynamic memory network further refines the generated images. We utilized a quantitative assessment of the model's performance on the CUB-200-2011 dataset by employing key metrics such as Inception Score and Fr\u00e9chet Inception Distance. The results of our study demonstrate that the generation quality exceeds that of other models, as confirmed by manual evaluation methods. Additionally, the model demonstrates exceptional generation capabilities on our custom-built ceramic wine bottle dataset, further validating the adaptability and generative efficacy of the proposed approach.",
        "10": "Verification of temporal consistency constraints in the evolution of software for intelligent unmanned systems driven by model checking. Chaoze Lu, Chenghao Li, Chenxia Liu, Xianrui Wu, Yimin Huang. Scientific Reports, 2025.\nNumber of citations: 0\nAbstract: As the application of intelligent unmanned systems in open environments becomes increasingly widespread, temporal consistency in the system\u2019s dynamic evolution process has become a critical issue for determining whether the system is safe and reliable. To address this issue, this paper proposes a model checking-based method for verifying the temporal consistency of software evolution in intelligent unmanned systems, accurately modeling and verifying the temporal behavior of individual agents and multi-agent evolution in the system. First, a temporal consistency constraint verification framework for the evolution of intelligent unmanned systems is presented, under which a global time automaton network model is constructed. The framework defines global and local clock variables, enabling a unified representation of both the overall system process and the time properties of individual agents. Secondly, the temporal consistency constraint patterns under the three evolution operations\u2014addition, deletion, and replacement\u2014are described. Based on this, a temporal consistency verification algorithm is designed, covering dynamic operations such as the addition, deletion, and replacement of agents. The algorithm performs temporal consistency checks for each state through a marking function and depth-first search, ensuring the system\u2019s temporal consistency. Finally, a case study on verifying the temporal consistency of an intelligent autonomous vehicle system is presented. Through a series of operations such as the addition, deletion, and replacement of agents, the effectiveness and applicability of the proposed method are verified. The results show that the method can effectively detect temporal inconsistencies in the evolution process, improving the reliability of system evolution in open environments."
    }
}