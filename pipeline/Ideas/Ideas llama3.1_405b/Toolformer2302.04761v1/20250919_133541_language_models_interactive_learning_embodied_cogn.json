{
    "query": "language models interactive learning embodied cognition",
    "result": {
        "1": "Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents. Wenlong Huang, P. Abbeel, Deepak Pathak, Igor Mordatch. International Conference on Machine Learning, 2022.\nNumber of citations: 1222\nAbstract: Can world knowledge learned by large language models (LLMs) be used to act in interactive environments? In this paper, we investigate the possibility of grounding high-level tasks, expressed in natural language (e.g.\"make breakfast\"), to a chosen set of actionable steps (e.g.\"open fridge\"). While prior work focused on learning from explicit step-by-step examples of how to act, we surprisingly find that if pre-trained LMs are large enough and prompted appropriately, they can effectively decompose high-level tasks into mid-level plans without any further training. However, the plans produced naively by LLMs often cannot map precisely to admissible actions. We propose a procedure that conditions on existing demonstrations and semantically translates the plans to admissible actions. Our evaluation in the recent VirtualHome environment shows that the resulting method substantially improves executability over the LLM baseline. The conducted human evaluation reveals a trade-off between executability and correctness but shows a promising sign towards extracting actionable knowledge from language models. Website at https://huangwl18.github.io/language-planner",
        "2": "Voyager: An Open-Ended Embodied Agent with Large Language Models. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi (Jim) Fan, Anima Anandkumar. Trans. Mach. Learn. Res., 2023.\nNumber of citations: 936\nAbstract: We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",
        "3": "Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents. Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, Yitao Liang. arXiv.org, 2023.\nNumber of citations: 374\nAbstract: We investigate the challenge of task planning for multi-task embodied agents in open-world environments. Two main difficulties are identified: 1) executing plans in an open-world environment (e.g., Minecraft) necessitates accurate and multi-step reasoning due to the long-term nature of tasks, and 2) as vanilla planners do not consider how easy the current agent can achieve a given sub-task when ordering parallel sub-goals within a complicated plan, the resulting plan could be inefficient or even infeasible. To this end, we propose\"$\\underline{D}$escribe, $\\underline{E}$xplain, $\\underline{P}$lan and $\\underline{S}$elect\"($\\textbf{DEPS}$), an interactive planning approach based on Large Language Models (LLMs). DEPS facilitates better error correction on initial LLM-generated $\\textit{plan}$ by integrating $\\textit{description}$ of the plan execution process and providing self-$\\textit{explanation}$ of feedback when encountering failures during the extended planning phases. Furthermore, it includes a goal $\\textit{selector}$, which is a trainable module that ranks parallel candidate sub-goals based on the estimated steps of completion, consequently refining the initial plan. Our experiments mark the milestone of the first zero-shot multi-task agent that can robustly accomplish 70+ Minecraft tasks and nearly double the overall performances. Further testing reveals our method's general effectiveness in popularly adopted non-open-ended domains as well (i.e., ALFWorld and tabletop manipulation). The ablation and exploratory studies detail how our design beats the counterparts and provide a promising update on the $\\texttt{ObtainDiamond}$ grand challenge with our approach. The code is released at https://github.com/CraftJarvis/MC-Planner.",
        "4": "Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning. Thomas Carta, Cl\u00e9ment Romac, Thomas Wolf, S. Lamprier, Olivier Sigaud, Pierre-Yves Oudeyer. International Conference on Machine Learning, 2023.\nNumber of citations: 202\nAbstract: Recent works successfully leveraged Large Language Models' (LLM) abilities to capture abstract knowledge about world's physics to solve decision-making problems. Yet, the alignment between LLMs' knowledge and the environment can be wrong and limit functional competence due to lack of grounding. In this paper, we study an approach (named GLAM) to achieve this alignment through functional grounding: we consider an agent using an LLM as a policy that is progressively updated as the agent interacts with the environment, leveraging online Reinforcement Learning to improve its performance to solve goals. Using an interactive textual environment designed to study higher-level forms of functional grounding, and a set of spatial and navigation tasks, we study several scientific questions: 1) Can LLMs boost sample efficiency for online learning of various RL tasks? 2) How can it boost different forms of generalization? 3) What is the impact of online learning? We study these questions by functionally grounding several variants (size, architecture) of FLAN-T5.",
        "5": "True Knowledge Comes from Practice: Aligning Large Language Models with Embodied Environments via Reinforcement Learning. Weihao Tan, Wentao Zhang, Shanqi Liu, Longtao Zheng, Xinrun Wang, Bo An. International Conference on Learning Representations, 2024.\nNumber of citations: 18\nAbstract: None",
        "6": "HOW GENERATIVE LANGUAGE MODELS CAN ENHANCE INTERACTIVE LEARNING WITH SOCIAL ROBOTS. Stefan Sonderegger. Cognition and Exploratory Learning in the Digital Age, 2022.\nNumber of citations: 8\nAbstract: The use of social robots in education is a growing area of research and the potential future applications are various. However, the conversational models behind current social robots and chatbot systems often rely on rule-based and retrieval-based methods. This limits the social robot to predefined responses and topics, thus hindering it from fluent communication and interaction. Generative language models such as GPT-3 could be beneficial in this context, e.g. for an improved conversation and open-ended question answering. This article presents an approach to utilizing generative language models to enhance interactive learning with educational social robots. The proposed model combines the technological possibilities of generative language models with the educational tasks of a social robot in the role of a tutor and learning partner. The implementation of the model in practice is illustrated by means of a use case consisting of different learning scenarios. The social robot generates explanations, questions, corrections, and answers based on the pre-trained GPT-3 model. By exploring the potential of generative language models for interactive learning with social robots on different levels of abstraction, the paper also aims to contribute to an understanding of the future relevance and possibilities that generative language models bring into education and educational technologies in general.",
        "7": "Characterizing an Analogical Concept Memory for Architectures Implementing the Common Model of Cognition. Shiwali Mohan, M. Klenk, Matthew Shreve, Kent Evans, Aaron Ang, John Maxwell. Social Science Research Network, 2020.\nNumber of citations: 1\nAbstract: Architectures that implement the Common Model of Cognition - Soar, ACT-R, and Sigma - have a prominent place in research on cognitive modeling as well as on designing complex intelligent agents. In this paper, we explore how computational models of analogical processing can be brought into these architectures to enable concept acquisition from examples obtained interactively. We propose a new analogical concept memory for Soar that augments its current system of declarative long-term memories. We frame the problem of concept learning as embedded within the larger context of interactive task learning (ITL) and embodied language processing (ELP). We demonstrate that the analogical learning methods implemented in the proposed memory can quickly learn a diverse types of novel concepts that are useful not only in recognition of a concept in the environment but also in action selection. Our approach has been instantiated in an implemented cognitive system \\textsc{Aileen} and evaluated on a simulated robotic domain.",
        "8": "Towards Embodied Cognition in Robots via Spatially Grounded Synthetic Worlds. Joel Currie, Gioele Migno, Enrico Piacenti, M. Giannaccini, Patric Bach, D. D. Tommaso, Agnieszka Wykowska. arXiv.org, 2025.\nNumber of citations: 0\nAbstract: We present a conceptual framework for training Vision-Language Models (VLMs) to perform Visual Perspective Taking (VPT), a core capability for embodied cognition essential for Human-Robot Interaction (HRI). As a first step toward this goal, we introduce a synthetic dataset, generated in NVIDIA Omniverse, that enables supervised learning for spatial reasoning tasks. Each instance includes an RGB image, a natural language description, and a ground-truth 4X4 transformation matrix representing object pose. We focus on inferring Z-axis distance as a foundational skill, with future extensions targeting full 6 Degrees Of Freedom (DOFs) reasoning. The dataset is publicly available to support further research. This work serves as a foundational step toward embodied AI systems capable of spatial understanding in interactive human-robot scenarios.",
        "9": "Analogical Concept Memory for Architectures Implementing the Common Model of Cognition. Shiwali Mohan, M. Klenk, Matthew Shreve, Kent Evans, Aaron Ang, John Maxwell. arXiv.org, 2022.\nNumber of citations: 0\nAbstract: Architectures that implement the Common Model of Cognition - Soar, ACT-R, and Sigma - have a prominent place in research on cognitive modeling as well as on designing complex intelligent agents. In this paper, we explore how computational models of analogical processing can be brought into these architectures to enable concept acquisition from examples obtained interactively. We propose a new analogical concept memory for Soar that augments its current system of declarative long-term memories. We frame the problem of concept learning as embedded within the larger context of interactive task learning (ITL) and embodied language processing (ELP). We demonstrate that the analogical learning methods implemented in the proposed memory can quickly learn a diverse types of novel concepts that are useful not only in recognition of a concept in the environment but also in action selection. Our approach has been instantiated in an implemented cognitive system AILEEN and evaluated on a simulated robotic domain.",
        "10": "The Use of Language Learning and Teaching Material from a Perspective of Embodied Cognition. Yongchun Zhao. Education, Language and Sociology Research, 2024.\nNumber of citations: 0\nAbstract: The use of Language Learning and Teaching (LLT) materials directly affects the realization of the function of LLT materials, language teaching and learning effects. The traditional teaching ideas that teach what has said on the textbook ignoring the dynamic interaction of students\u2019 body, cognition and environment, which caused physical and mental separation, and affects the embodied experience of language learning process. Teachers should change the way LLT materials are used to promote students\u2019 embodied experience and effectiveness of language learning in both cognitive and behavioral level. Based on the embodied cognition theory, this qualitative study will examine how an EFL teacher used the different LLT materials practice the embodied English language teaching in Primary School. This study also attempts to examine verify the acceptability and effectiveness of embodied English teaching through practice."
    }
}