{
    "query": "meta-learning for adaptive retrieval-augmented generation",
    "result": {
        "1": "MBA-RAG: a Bandit Approach for Adaptive Retrieval-Augmented Generation through Question Complexity. Xiaqiang Tang, Q. Gao, Jian Li, Nan Du, Qi Li, Sihong Xie. International Conference on Computational Linguistics, 2024.\nNumber of citations: 17\nAbstract: Retrieval Augmented Generation (RAG) has proven to be highly effective in boosting the generative performance of language model in knowledge-intensive tasks. However, existing RAG framework either indiscriminately perform retrieval or rely on rigid single-class classifiers to select retrieval methods, leading to inefficiencies and suboptimal performance across queries of varying complexity. To address these challenges, we propose a reinforcement learning-based framework that dynamically selects the most suitable retrieval strategy based on query complexity. % our solution Our approach leverages a multi-armed bandit algorithm, which treats each retrieval method as a distinct ``arm'' and adapts the selection process by balancing exploration and exploitation. Additionally, we introduce a dynamic reward function that balances accuracy and efficiency, penalizing methods that require more retrieval steps, even if they lead to a correct result. Our method achieves new state of the art results on multiple single-hop and multi-hop datasets while reducing retrieval costs. Our code are available at https://github.com/FUTUREEEEEE/MBA .",
        "2": "Enhancing Code Translation in Language Models with Few-Shot Learning via Retrieval-Augmented Generation. Manish Bhattarai, Javier E. Santos, Shawn Jones, Ayan Biswas, Boian Alexandrov, Dan O\u2019Malley. IEEE Conference on High Performance Extreme Computing, 2024.\nNumber of citations: 15\nAbstract: The advent of large language models (LLMs) has revolutionized the field of code translation, enabling automated translation between programming languages. Despite these advancements, the accuracy and reliability of these models often falter in complex translation tasks due to a lack of contextual understanding. This paper introduces a novel approach to enhance code translation through Few-Shot Learning augmented with retrieval-based techniques. By leveraging a repository of existing code translations, we dynamically retrieve the most relevant examples to guide the model in translating new code segments. Our method, based on Retrieval-Augmented Generation (RAG), significantly improves translation quality by providing contextual examples that the model can learn from in real-time. We chose RAG over traditional fine-tuning methods due to its ability to leverage existing codebases or a locally stored corpus of code, allowing it to dynamically adapt to diverse translation tasks without the need for extensive retraining. Extensive experiments on diverse datasets, using open LLM models such as Starcoder, Llama3-70B Instruct, CodeLlama-34B Instruct, Granite-34B Code Instruct, and Mixtral-8\u00d722B, and commercial LLM models such as GPT-3.5 turbo, and GPT-4o demonstrate the superiority of our approach over traditional zero-shot, particularly in translating between Fortran and C++. We also explored different numbers of shots (examples provided to the model during inference) \u2013 specifically 1, 2, and 3 shots \u2013 and various embedding models for RAG, including Nomic-Embed, Starencoder, and CodeBERT, to evaluate the robustness and effectiveness of our approach.",
        "3": "InstructRAG: Leveraging Retrieval-Augmented Generation on Instruction Graphs for LLM-Based Task Planning. Zheng Wang, Shu Xian Teo, Jun Jie Chew, Wei Shi. Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2025.\nNumber of citations: 3\nAbstract: Recent advancements in large language models (LLMs) have enabled their use as agents for planning complex tasks. Existing methods typically rely on a thought-action-observation (TAO) process to enhance LLM performance, but these approaches are often constrained by the LLMs' limited knowledge of complex tasks. Retrieval-augmented generation (RAG) offers new opportunities by leveraging external databases to ground generation in retrieved information. In this paper, we identify two key challenges (enlargability and transferability) in applying RAG to task planning. We propose InstructRAG, a novel solution within a multi-agent meta-reinforcement learning framework, to address these challenges. InstructRAG includes a graph to organize past instruction paths (sequences of correct actions), an RL-Agent with Reinforcement Learning to expand graph coverage for enlargability, and an ML-Agent with Meta-Learning to improve task generalization for transferability. The two agents are trained end-to-end to optimize overall planning performance. Our experiments on four widely used task planning datasets demonstrate that InstructRAG significantly enhances performance and adapts efficiently to new tasks, achieving up to a 19.2% improvement over the best existing approach.",
        "4": "How to Build an Adaptive AI Tutor for Any Course Using Knowledge Graph-Enhanced Retrieval-Augmented Generation (KG-RAG). Chenxi Dong, Yimin Yuan, Kan Chen, Shupei Cheng, Chujie Wen. International Conference on Educational and Information Technology, 2023.\nNumber of citations: 2\nAbstract: Integrating Large Language Models (LLMs) in Intelligent Tutoring Systems (ITS) presents transformative opportunities for personalized education. However, current implementations face two critical challenges: maintaining factual accuracy and delivering coherent, context-aware instruction. While Retrieval-Augmented Generation (RAG) partially addresses these issues, its reliance on pure semantic similarity limits its effectiveness in educational contexts where conceptual relationships are crucial. This paper introduces Knowledge Graph-enhanced Retrieval-Augmented Generation (KG-RAG), a novel framework that integrates structured knowledge representation with context-aware retrieval to enable more effective AI tutoring. We present three key contributions: (1) a novel architecture that grounds AI responses in structured domain knowledge, (2) empirical validation through controlled experiments (n=76) demonstrating significant learning improvements (35% increase in assessment scores, p<0.001), and (3) a comprehensive implementation framework addressing practical deployment considerations. These results establish KG-RAG as a robust solution for developing adaptable AI tutoring systems across diverse educational contexts.",
        "5": "Meta-Chunking: Learning Text Segmentation and Semantic Completion via Logical Perception. Jihao Zhao, Zhiyuan Ji, Yuchen Feng, Pengnian Qi, Simin Niu, Bo Tang, Feiyu Xiong, Zhiyu Li. , 2024.\nNumber of citations: 0\nAbstract: While Retrieval-Augmented Generation (RAG) has emerged as a promising paradigm for boosting large language models (LLMs) in knowledge-intensive tasks, it often overlooks the crucial aspect of text chunking within its workflow. This paper proposes the Meta-Chunking framework, which specifically enhances chunking quality through a dual strategy that identifies optimal segmentation points and preserves global information. Initially, breaking limitations of similarity-based chunking, we design two adaptive chunking techniques based on uncertainty, namely Perplexity Chunking and Margin Sampling Chunking, by utilizing the logical perception capabilities of LLMs. Given the inherent complexity across different texts, we integrate meta-chunk with dynamic merging, striking a balance between fine-grained and coarse-grained text chunking. Furthermore, we establish the global information compensation mechanism, encompassing a two-stage hierarchical summary generation process and a three-stage text chunk rewriting procedure focused on missing reflection, refinement, and completion. These components collectively strengthen the semantic integrity and contextual coherence of chunks. Extensive experiments demonstrate that Meta-Chunking effectively addresses challenges of the chunking task within the RAG system, providing LLMs with more logically coherent text chunks. Additionally, our methodology validates the feasibility of implementing high-quality chunking tasks with smaller-scale models, thereby eliminating the reliance on robust instruction-following capabilities.",
        "6": "On Automating Security Policies with Contemporary LLMs (Short Paper). Pablo Fern\u00e1ndez Saura, K. R. Jayaram, Vatche Isahagian, Jorge Bernal Bernab\u00e9, A. G\u00f3mez-Skarmeta. 2025 IEEE International Conference on Software Services Engineering (SSE), 2025.\nNumber of citations: 0\nAbstract: The complexity of modern computing environments and the growing sophistication of cyber threats necessitate a more robust, adaptive, and automated approach to security enforcement. In this paper, we present a framework leveraging large language models (LLMs) for automating attack mitigation policy compliance through an innovative combination of in-context learning and retrieval-augmented generation (RAG). We begin by describing how our system collects and manages both tool and API specifications, storing them in a vector database to enable efficient retrieval of relevant information. We then detail the architectural pipeline that first decomposes high-level mitigation policies into discrete tasks and subsequently translates each task into a set of actionable API calls. Our empirical evaluation, conducted using publicly available CTI policies in STIXv2 format and Windows API documen-tation, demonstrates significant improvements in precision, recall, and Fl-score when employing RAG compared to a non-RAG baseline.",
        "7": "LearnRAG: Implementing Retrieval-Augmented Generation for Adaptive Learning Systems. Richard Shan. Digital Signal Processing and Signal Processing Education Workshop, 2025.\nNumber of citations: 0\nAbstract: The rapid advancements in large language models have revolutionized natural language processing, yet their static knowledge bases limit their applicability in dynamic, domain-specific, and personalized contexts. Retrieval-Augmented Generation systems address this challenge by integrating retrieval mechanisms with generative models to deliver real-time, contextually enriched responses. This paper implements LearnRAG, an open-source RAG framework for personalized learning that is modular in architecture, hybrid in retrieval, and fine-tuned for generation to produce adaptive educational content. A holistic case study of LearnRAG showed scalability, efficiency, increasing learner engagement, and reducing educators' workload. Issues such as multimodal integration, content accuracy, and learning styles are discussed, and strategies for ethical deployment are developed. LearnRAG offers a robust, scalable, and adaptive platform to meet the evolving needs of learners and educators worldwide, representing a paradigm shift in GenAI-driven education.",
        "8": "AID-SQL: Adaptive In-Context Learning of Text-to-SQL with Difficulty-Aware Instruction and Retrieval-Augmented Generation. Xiuwen Li, Qifeng Cai, Yang Shu, Chenjuan Guo, Bin Yang. IEEE International Conference on Data Engineering, 2025.\nNumber of citations: 0\nAbstract: Recent research in Text-to-SQL translation has primarily adopted in-context learning methods leveraging large language models (LLMs), achieving significant progress. However, these methods face challenges in adapting to natural language questions of varying difficulty and the relevance of the few-shot examples provided. In this paper, we propose an adaptive in-context learning approach with difficulty-aware instruction and retrieval-augmented generation to enhance the performance of Text-to-SQL translation (AID-SQL). First, we introduce adaptive instructions for LLMs, which employ precise difficulty classification to apply difficulty-adaptive generative guidelines and chain of thought (CoT) templates for varying difficulty levels. We automatically incorporate few-shot examples retrieved through the knowledge base into the CoT template to construct CoT-enhanced examples, which improves the capability of LLMs with retrieval-augmented generation (RAG). Furthermore, considering that current RAG methods struggle to effectively measure the contribution of retrieved examples in solving the specific task of Text-to-SQL translation, we train a ranking model that can better bridge the semantic and structural gap between NL questions and SQL queries. This approach can better understand semantic information and allows for retrieving examples that are more beneficial to the final problem-solving. We evaluate our method on five benchmarks. Our method achieves competitive performance compared with existing methods.",
        "9": "MAO-ARAG: Multi-Agent Orchestration for Adaptive Retrieval-Augmented Generation. Yiqun Chen, Erhan Zhang, Lingyong Yan, Shuaiqiang Wang, Jizhou Huang, Dawei Yin, Jiaxin Mao. arXiv.org, 2025.\nNumber of citations: 0\nAbstract: In question-answering (QA) systems, Retrieval-Augmented Generation (RAG) has become pivotal in enhancing response accuracy and reducing hallucination issues. The architecture of RAG systems varies significantly, encompassing single-round RAG, iterative RAG, and reasoning RAG, each tailored to address different types of queries. Due to the varying complexity of real-world queries, a fixed RAG pipeline often struggles to balance performance and cost efficiency across different queries. To address this challenge, we propose an adaptive RAG framework called MAO-ARAG, which leverages multi-agent orchestration. Our adaptive RAG is conceived as a multi-turn framework. Specifically, we define multiple executor agents, representing typical RAG modules such as query reformulation agents, document selection agent, and generation agents. A planner agent intelligently selects and integrates the appropriate agents from these executors into a suitable workflow tailored for each query, striving for high-quality answers while maintaining reasonable costs. During each turn, the planner agent is trained using reinforcement learning, guided by an outcome-based reward (F1 score) and a cost-based penalty, continuously improving answer quality while keeping costs within a reasonable range. Experiments conducted on multiple QA datasets demonstrate that our approach, which dynamically plans workflows for each query, not only achieves high answer quality but also maintains both cost and latency within acceptable limits.The code of MAO-ARAG is on https://github.com/chenyiqun/Agentic-RAG.",
        "10": "Retrieval Augmented Generation for Document Query Automation using Open source LLMs. Khant Ko, Thwet Yin Nyein, Khine Khine Oo, Thant Zin Oo, T. Zin. International Conference on Advanced Infocomm Technology, 2024.\nNumber of citations: 0\nAbstract: Ollama provides access to powerful open-source language models that can be integrated into various applications. It supports local hosting, controlling the model's usage and data privacy. LLMs are large language models also known as deep learning models which are pre-trained on a vast amount of data. Integrating with retrieval augmented generation (RAG) can improve the efficiency of the LLM applications by retrieving custom data. The specific website or input of a particular document file can be integrated into the system. The document queries are automatically added to the Excel file using UiPath Automation. Firstly, the proposed system prompts by directly passing through the two LLM models: the Phi3 model by Microsoft with 3 billion parameters and the Llama 3.1 model by Meta with 8 billion parameters for text input and output, which can be accessed from Ollama released by Meta. To achieve a desired output, the system can also prompt by passing through retrieval augmented generation (RAG). Finally, analyze the results of the two outputs by directly using LLM models and embedding them with RAG. According to the evaluation result, RAG integration with llama3.1 improves response quality and relevance for custom data. Phi3 is better in the latency evaluation result."
    }
}