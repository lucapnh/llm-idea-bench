{
    "query": "multi-agent systems + conversation programming + tool use + RAG + evaluation",
    "result": {
        "1": "Evaluation of a Conversation Management Toolkit for Multi Agent Programming. David Lillis, Rem W. Collier, Howell R. Jordan. International Workshop on Programming Multi-Agent Systems, 2014.\nNumber of citations: 7\nAbstract: The Agent Conversation Reasoning Engine (ACRE) is intended to aid agent developers to improve the management and reliability of agent communication. To evaluate its effectiveness, a problem scenario was created that could be used to compare code written with and without the use of ACRE by groups of test subjects. \nThis paper describes the requirements that the evaluation scenario was intended to meet and how these motivated the design of the problem. Two experiments were conducted with two separate sets of students and their solutions were analysed using a combination of simple objective metrics and subjective analysis. The analysis suggested that ACRE by default prevents some common problems arising that would limit the reliability and extensibility of conversation-handling code. \nAs ACRE has to date been integrated only with the Agent Factory multi agent framework, it was necessary to verify that the problems identified are not unique to that platform. Thus a comparison was made with best practice communication code written for the Jason platform, in order to demonstrate the wider applicability of a system such as ACRE.",
        "2": "Enhancing LLM Code Generation: A Systematic Evaluation of Multi-Agent Collaboration and Runtime Debugging for Improved Accuracy, Reliability, and Latency. Nazmus Ashrafi, Salah Bouktif, Mohammed Mediani. arXiv.org, 2025.\nNumber of citations: 3\nAbstract: The use of large language models (LLMs) for automated code generation has emerged as a significant focus within AI research. As these pretrained models continue to evolve, their ability to understand and generate complex code structures has opened new possibilities for automating intricate programming tasks for the sake of accurate code generation. Although contemporary foundational models demonstrate promoting results, researchers continue to explore optimal post-training strategies to enhance code quality. These include supervised fine-tuning, retrieval-augmented generation (RAG), debugging, and many others. In this paper, we combine two widely used approaches namely multi-agent collaboration and runtime execution information-based debugging, for improving code generation functionality, reliability, and practical applicability. We perform an empirical study in order to extend the evaluation of the individual strategies as well as the proposed composition of the activities of both strategies. Our study use 19 LLMs to examines the performance of individual and the proposed strategies, offering comprehensive insights into how different programming activities compositions and training paradigms influence code generation effectiveness. In particular, we implement a chained system that combines both strategies to assess their combined impact on functional accuracy, code reliability, and generation latency using two benchmark datasets commonly used for code generation. Our findings provide valuable insights for organizations seeking robust AI-driven coding solutions by guiding them in selecting models that can better adapt to complex post-training strategies, ultimately fostering the adoption of more effective and reliable code generation technologies.",
        "3": "Feasibility Evaluation of Milling Designs Using Multi-Agent Systems. S. Plappert, C. Becker, P. Gembarski, R. Lachmayer. Proceedings of the Design Society, 2022.\nNumber of citations: 3\nAbstract: Abstract During product development, many decisions have to be made that affect the entire product life cycle and often lead to errors that cause additional effort. To proactively support the engineer in evaluating his design in a CAD program, in this paper an approach to evaluate milling designs using a multi-agent system (MAS) is presented. The CommonKADS method is used and the MAS is validated against an application example of a gearbox housing that has been checked for design guidelines, standards, and tool or machine portfolios.",
        "4": "MCPToolBench++: A Large Scale AI Agent Model Context Protocol MCP Tool Use Benchmark. Shiqing Fan, Xichen Ding, Liang Zhang, Linjian Mo. arXiv.org, 2025.\nNumber of citations: 2\nAbstract: LLMs'capabilities are enhanced by using function calls to integrate various data sources or API results into the context window. Typical tools include search, web crawlers, maps, financial data, file systems, and browser usage, etc. Integrating these data sources or functions requires a standardized method. The Model Context Protocol (MCP) provides a standardized way to supply context to LLMs. However, the evaluation of LLMs and AI Agents'MCP tool use abilities suffer from several issues. First, there's a lack of comprehensive datasets or benchmarks to evaluate various MCP tools. Second, the diverse formats of response from MCP tool call execution further increase the difficulty of evaluation. Additionally, unlike existing tool-use benchmarks with high success rates in functions like programming and math functions, the success rate of real-world MCP tool is not guaranteed and varies across different MCP servers. Furthermore, the LLMs'context window also limits the number of available tools that can be called in a single run, because the textual descriptions of tool and the parameters have long token length for an LLM to process all at once. To help address the challenges of evaluating LLMs'performance on calling MCP tools, we propose MCPToolBench++, a large-scale, multi-domain AI Agent tool use benchmark. As of July 2025, this benchmark is build upon marketplace of over 4k MCP servers from more than 40 categories, collected from the MCP marketplaces and GitHub communities. The datasets consist of both single-step and multi-step tool calls across different categories. We evaluated SOTA LLMs with agentic abilities on this benchmark and reported the results.",
        "5": "TOWARDS MULTI-AGENT PLATFORM DEVELOPMENT. Oleksandr Karataiev. Computer Systems and Information Technologies, 2024.\nNumber of citations: 1\nAbstract: This paper focuses on the design and evaluation of a FIPA standard compliant multi-agent platform. The relevance of the topic is due to the growing need for flexible, reliable, and efficient software solutions capable of solving complex intelligent problems in distributed environments. The study is dedicated to the problem of developing and evaluating an agent platform using the Kotlin programming language. The main goal of this work is to design and implement a modular, scalable, and adaptive agent platform. The existing frameworks for the development of multi-agent systems are reviewed, the key components of such systems are highlighted, and the advantages of using Kotlin in the context of a multi-agent architecture are discussed. The scientific contribution of the paper is the creation of a modern FIPA-compliant multi-agent platform that exploits the advantages of the Kotlin language. The performance and resource intensity of the developed system are analyzed, and the platform's compliance with FIPA standards and its interoperability are evaluated. Two different metrics are used to ensure the quality of the system. One of the metrics is the percentage of covered code. This metric is measured using the kover library.\u00a0 We achieved 71.4% coverage of classes and 57.1% coverage of commands. Further coverage is complicated by the use of multi-threaded technologies. The second metric is the system's score for comments from the sonarlint evaluation tool. During development, 16 comments were identified and fixed. This allows us to achieve a high level of code quality and ensure quality for the future. The study demonstrates the potential of integrating modern language capabilities with the multi-agent paradigm, opening new perspectives for the development of efficient and scalable solutions in the area of distributed intelligent systems.",
        "6": "AURA: Agent for Understanding, Reasoning, and Automated Tool Use in Voice-Driven Tasks. L. Maben, Gayathri Ganesh Lakshmy, Srijith Radhakrishnan, Siddhant Arora, Shinji Watanabe. arXiv.org, 2025.\nNumber of citations: 0\nAbstract: Despite advances in language and speech technologies, no open-source system enables full speech-to-speech, multi-turn dialogue with integrated tool use and agentic reasoning. We introduce AURA (Agent for Understanding, Reasoning, and Automated Tool Use), the first open-source, speech-native assistant capable of completing complex, goal-driven tasks through dynamic tool invocation and multi-turn conversation. AURA combines open-weight ASR, TTS, and LLMs in a cascaded pipeline and supports tools such as calendar booking, contact lookup, web search, and email. Its modular design allows easy integration of new tools using natural language prompts and action classes. On VoiceBench, AURA scores 92.75% on OpenBookQA-outperforming all open-weight systems and nearing GPT-4o-and 4.39 on AlpacaEval, competitive with other open-weight systems. Human evaluation shows 90% task success on complex, multi-turn speech tasks.",
        "7": "Building Advanced RAG Q&A with Multiple Data Sources Using Langchain: A Multi-Search Agent RAG Application in Ubiquitous Learning. Manel Guettala, Samir Bourekkache, O. Kazar, S. Harous. International Conference on Compute and Data Analysis, 2024.\nNumber of citations: 0\nAbstract: The integration of Large Language Models (LLMs) in Question-Answering (QA) systems has made significant progress, yet they often fail to generate precise answers for queries beyond their training data and hallucinating. To address this, our study develops an advanced Retrieval-Augmented Generation (RAG) pipeline method using the LangChain framework, featuring a decision-making agent that dynamically selects the most effective tools and data sources for accurate and contextually relevant responses. By incorporating multiple data sources and diverse tools, our system mitigates the limitations of traditional LLMs, enhancing their effectiveness in ubiquitous learning environments. Evaluation through various case studies shows significant improvements in accuracy, relevance, and contextual appropriateness. The multi-search agent RAG system efficiently retrieves and synthesizes information, supporting continuous and context-aware learning and enriching the user experience. This research advances AI-based educational technologies, delivering a powerful solution for information retrieval and synthesis, as well as laying the foundations for intelligent question-and-answer systems of the future.",
        "8": "ChaTCL: LLM-Based Multi-Agent RAG Framework for TCL Script Generation. Yibo Rui, Yuanhang Li, Rui Wang, Ruiqi Chen, Yanxiang Zhu, Zhixiong Di, Xi Wang, Ming Ling. 2025 International Symposium of Electronics Design Automation (ISEDA), 2025.\nNumber of citations: 0\nAbstract: None",
        "9": "QAgent: An LLM-based Multi-Agent System for Autonomous OpenQASM programming. Zhenxiao Fu, Fan Chen, Lei Jiang. , 2025.\nNumber of citations: 0\nAbstract: Noisy Intermediate-Scale Quantum (NISQ) devices have begun to exhibit early quantum advantages on classically intractable problems, spanning physics simulations to Gaussian boson sampling. Yet, realizing these benefits remains challenging for non-experts, primarily due to the complexities of programming in Open Quantum Assembly Language (OpenQASM). Although Large Language Model (LLM)-based agents have shown promise in automating classical programming workflows, their quantum counterparts have largely been restricted to specialized tasks such as quantum chemistry or error correction. In this paper, we present QAgent, an LLM-powered multi-agent system that fully automates OpenQASM programming. By integrating task planning, in-context few-shot learning, retrieval-augmented generation (RAG) for long-term context, predefined generation tools, and chain-of-thought (CoT) reasoning, the agents systematically improve both compilation and functional correctness. Our evaluations demonstrate substantial improvements: across multiple LLMs of varying sizes, QAgent enhances the accuracy of QASM code generation by 71.6\\% compared to previous static LLM-based approaches. We envision this multi-agent system as a key enabler for democratizing quantum programming, bridging expertise gaps, and accelerating the practical adoption of quantum computing.",
        "10": "Methods of aggregation of fuzzy estimations and analysis by multi-agent systems of large volumes of information. A. O. Zhukov, I. N. Kartsan, A.V. Nerush, E.V. Okunev. IV All-Russian (National) Scientific Conference \"Russian Science, Innovation, Education\", 2025.\nNumber of citations: 0\nAbstract: In this work, issues related to the development of methodological foundations for building monitoring and decision support\nsystems for managing applied scientific research and space experiments are considered. Various approaches to the integration of\nmathematical models into information systems aimed at analyzing complex multi-criteria tasks are presented. Particular attention is\ngiven to the processing of qualitative, quantitative, and fuzzy expert evaluation data in different formats. The possibility of minimizing\ninformation loss during the analysis of hierarchically organized structures is examined. New methods for aggregating fuzzy evaluations\nand analyzing large data volumes using multi-agent systems have been developed. At the initial stage of the study, source data were\nverified and computational experiments were conducted, demonstrating the effectiveness of the proposed approaches. As a result, a\nmodel for planning applied scientific research was developed. Its application has shown the potential to improve the accuracy and\nspeed of analysis when forming space experiment programs. The obtained results can be used in the development of intelligent project\nmanagement systems in high-tech industries. A promising direction for application is the creation of tools for efficiency assessment,\nstate monitoring, and resource management in experimental planning within the framework of research and development projects."
    }
}