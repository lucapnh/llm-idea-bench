{
    "query": "reinforcement learning via sequence modeling for multi-agent systems",
    "result": {
        "1": "Cooperative Learning of Multi-Agent Systems Via Reinforcement Learning. Xin Wang, Chen Zhao, Tingwen Huang, P. Chakrabarti, J\u00fcrgen Kurths. IEEE Transactions on Signal and Information Processing over Networks, 2023.\nNumber of citations: 24\nAbstract: In many specific scenarios, accurateand practical cooperative learning is a commonly encountered challenge in multi-agent systems. Thus, the current investigation focuses on cooperative learning algorithms for multi-agent systems and underpins an alternate data-based neural network reinforcement learning framework. To achieve the data-based learning optimization, the proposed cooperative learning framework, which comprises two layers, introduces a virtual learning objective. The followers learn the behaviors of the virtual objects in the first layer based on the adaptive neural networks (NNs). Specifically, the actor and critic NNs are applied to acquire cooperative behaviors and assess this layer's long-term utility function. Then another layer realizes the tracking performance between the virtual objects and the leader by introducing the local data-based performance index. Then, we formulate a resulting deterministic optimization problem and resolve it effectively with the policy iteration algorithm. This intuitive cooperative learning algorithm also preserves good robustness properties and eliminates the dependence on the prior knowledge of the multi-agent system model in the solution process. Finally, a multi-robot formation system demonstrates this promising development's practical appeal and highly effective outcome.",
        "2": "Multi-Behavior Multi-Agent Reinforcement Learning for Informed Search via Offline Training. Songjun Huang, Chuanneng Sun, Ruo-Qian Wang, D. Pompili. 2024 20th International Conference on Distributed Computing in Smart Systems and the Internet of Things (DCOSS-IoT), 2024.\nNumber of citations: 11\nAbstract: In modern informed search missions, Multi-Robot Systems (MRSs) are playing more and more important roles due to their flexibility in exploring environments. Reinforcement learning (RL) is now widely used as a decision-making method for MRS. However, existing RL-based and conventional model-based frameworks cannot deal with some challenges posed by the realworld environment. To address these challenges, a Multi-Behavior Multi-Agent Reinforcement Learning (MBMARL) framework via offline reinforcement learning method was developed. In this framework, each agent is deployed with multiple behavior policies to let the agent have choices on behaviors given a state. The proposed framework is compared with traditional reinforcement learning frameworks, including Multi-Agent Actor Critic (MAAC) and REINFORCE. The result shows that MBMARL outperforms others in both aspects of total reward and convergence time.",
        "3": "Socialbots on Fire: Modeling Adversarial Behaviors of Socialbots via Multi-Agent Hierarchical Reinforcement Learning. Thai Le, tql. The Web Conference, 2021.\nNumber of citations: 8\nAbstract: Socialbots are software-driven user accounts on social platforms, acting autonomously (mimicking human behavior), with the aims to influence the opinions of other users or spread targeted misinformation for particular goals. As socialbots undermine the ecosystem of social platforms, they are often considered harmful. As such, there have been several computational efforts to auto-detect the socialbots. However, to our best knowledge, the adversarial nature of these socialbots has not yet been studied. This begs a question \u201ccan adversaries, controlling socialbots, exploit AI techniques to their advantage?\u201d To this question, we successfully demonstrate that indeed it is possible for adversaries to exploit computational learning mechanism such as reinforcement learning (RL) to maximize the influence of socialbots while avoiding being detected. We first formulate the adversarial socialbot learning as a cooperative game between two functional hierarchical RL agents. While one agent curates a sequence of activities that can avoid the detection, the other agent aims to maximize network influence by selectively connecting with right users. Our proposed policy networks train with a vast amount of synthetic graphs and generalize better than baselines on unseen real-life graphs both in terms of maximizing network influence (up to +18%) and sustainable stealthiness (up to +40% undetectability) under a strong bot detector (90% detection accuracy). During inference, the complexity of our approach scales linearly, independent of a network\u2019s structure and the virality of news. This makes our attack very practical in a real-life setting.",
        "4": "Enhancing Multi-Agent Systems via Reinforcement Learning with LLM-Based Planner and Graph-Based Policy. Ziqi Jia, Junjie Li, Xiaoyang Qu, Jianzong Wang. IEEE International Conference on Robotics and Automation, 2025.\nNumber of citations: 3\nAbstract: Multi-agent systems (MAS) have shown great potential in executing complex tasks, but coordination and safety remain significant challenges. Multi-Agent Reinforcement Learning (MARL) offers a promising framework for agent collaboration, but it faces difficulties in handling complex tasks and designing reward functions. The introduction of Large Language Models (LLMs) has brought stronger reasoning and cognitive abilities to MAS, but existing LLM-based systems struggle to respond quickly and accurately in dynamic environments. To address these challenges, we propose LLM-based Graph Collaboration MARL (LGC-MARL), a framework that efficiently combines LLMs and MARL. This framework decomposes complex tasks into executable subtasks and achieves efficient collaboration among multiple agents through graph-based coordination. Specifically, LGC-MARL consists of two main components: an LLM planner and a graph-based collaboration meta policy. The LLM planner transforms complex task instructions into a series of executable subtasks, evaluates the rationality of these subtasks using a critic model, and generates an action dependency graph. The graph-based collaboration meta policy facilitates communication and collaboration among agents based on the action dependency graph, and adapts to new task environments through meta-learning. Experimental results on the AI2-THOR simulation platform demonstrate the superior performance and scalability of LGC-MARL in completing various complex tasks.",
        "5": "MAGT-toll: A multi-agent reinforcement learning approach to dynamic traffic congestion pricing. Jiaming Lu, Chuanyang Hong, Rui Wang. PLoS ONE, 2024.\nNumber of citations: 3\nAbstract: Modern urban centers have one of the most critical challenges of congestion. Traditional electronic toll collection systems attempt to mitigate this issue through pre-defined static congestion pricing methods; however, they are inadequate in addressing the dynamic fluctuations in traffic demand. Dynamic congestion pricing has been identified as a promising approach, yet its implementation is hindered by the computational complexity involved in optimizing long-term objectives and the necessity for coordination across the traffic network. To address these challenges, we propose a novel dynamic traffic congestion pricing model utilizing multi-agent reinforcement learning with a transformer architecture. This architecture capitalizes on its encoder-decoder structure to transform the multi-agent reinforcement learning problem into a sequence modeling task. Drawing on insights from research on graph transformers, our model incorporates agent structures and positional encoding to enhance adaptability to traffic flow dynamics and network coordination. We have developed a microsimulation-based environment to implement a discrete toll-rate congestion pricing scheme on actual urban roads. Our extensive experimental results across diverse traffic demand scenarios demonstrate substantial improvements in congestion metrics and reductions in travel time, thereby effectively alleviating traffic congestion.",
        "6": "Modeling reciprocal adaptation in HCI: a Multi-Agent Reinforcement Learning Approach. Tanay Gupta, Julien Gori. CHI Extended Abstracts, 2023.\nNumber of citations: 2\nAbstract: Adaptation between users and computers is difficult because of the reciprocal long-term adaptation between the user and an adaptive tool. In this work in progress, we present a novel method for designing adaptive systems, by simulating reciprocal adaptation between a user and an intelligent tool via Multi-Agent Reinforcement Learning (MARL). We report on a simple target-selection simulation in which a simulated, rational user tries to reach a target and is aided by an intelligent tool that manipulates the user\u2019s environment. The user and the tool\u2019s behavior are jointly determined by an adaptive algorithm that maximizes their utility. Our approach shows that both agents are capable of learning realistic policies, with the user and tool combination being 30% more effective than the user alone.",
        "7": "Efficient Replay Memory Architectures in Multi-Agent Reinforcement Learning for Traffic Congestion Control. Mukul Chodhary, Kevin Octavian, SooJean Han. 2024 IEEE 27th International Conference on Intelligent Transportation Systems (ITSC), 2024.\nNumber of citations: 1\nAbstract: Episodic control, inspired by the role of episodic memory in the human brain, has been shown to improve the sample inefficiency of model-free reinforcement learning by reusing high-return past experiences. However, the memory growth of episodic control is undesirable in large-scale multi-agent problems such as vehicle traffic management. This paper proposes a novel replay memory architecture called Dual-Memory Integrated Learning, to augment to multi-agent reinforcement learning methods for congestion control via adaptive light signal scheduling. Our dual-memory architecture mimics two core capabilities of human decision-making. First, it relies on diverse types of memory\u2013semantic and episodic, short-term and long-term\u2013in order to remember high-return states that occur often in the network and filter out states that don't. Second, it employs equivalence classes to group together similar state-action pairs and that can be controlled using the same action (i.e., light signal sequence). Theoretical analyses establish memory growth bounds, and simulation experiments on several intersection networks showcase improved congestion performance (e.g., vehicle throughput) from our method.",
        "8": "Coordinated Control Optimization of Nuclear Steam Supply Systems via Multi-Agent Reinforcement Learning. Tianhao Zhang, Zhonghua Cheng, Zhe Dong, Xiaojin Huang. Energies, 2025.\nNumber of citations: 0\nAbstract: Nuclear steam supply systems (NSSSs) are critical to achieving safe, efficient, and flexible nuclear power generation. While deep reinforcement learning (DRL) has shown potential in optimizing NSSS control, existing single-agent approaches apply the same optimization strategies to all subsystems. This simplification ignores subsystem-specific control requirements and limits both optimization efficacy and adaptability. To resolve this gap, we propose a multi-agent reinforcement learning (MARL) framework that independently optimizes each subsystem while ensuring global coordination. Our approach extends the current NSSS optimization framework from a single-agent model to a multi-agent one and introduces a novel MARL method to foster effective exploration and stability during optimization. Experimental findings demonstrate that our method significantly outperforms DRL-based approaches in optimizing thermal power and outlet steam temperature control. This research pioneers the application of MARL to NSSS optimization, paving the way for advanced nuclear power control systems.",
        "9": "Scaling Up Multi-Agent Reinforcement Learning via Graph Decomposition Invariant Network. Hongyi Fu, Jianmin Ji. 2024 5th International Seminar on Artificial Intelligence, Networking and Information Technology (AINIT), 2024.\nNumber of citations: 0\nAbstract: This paper focuses on the dilemma faced by largescale multi-agent systems. With the increase of agent size, the policy search space grows exponentially, reaching the task complexity that is difficult for the current mainstream multiagent algorithms to deal with, so that the optimal strategy is converged in large-scale multi-agent systems. In order to make the learning process of large-scale multi-agent tasks more smooth, many researchers have introduced the method of course learning, setting the scale from small to large courses, compared with learning training from scratch, this way can continue to inherit the knowledge learned in the smaller scale tasks, making the training smoother. However, in order to realize the multiagent course learning from small to large scale, we face the following difficulties. First, the neural network structure we designed can adapt to the observation input of agents of different scales, rather than only applicable to a fixed scale. Second, in the multi-stage course learning, due to the network's inefficient representation of multi-agent systems, the knowledge learned in small-scale courses cannot be effectively utilized in large-scale courses, and even the problem of catastrophic forgetting occurs in the neural network. Specifically, we view multi-agent systems from a graph perspective and derive the Graph Decomposition Invariant (GDI) for the first time. We prove that neural network architectures satisfying the GDI are of great help in realizing efficient course learning. We design a model architecture called GDI model suitable for multi-agent curriculum learning, which satisfies the graph decomposition invariant proposed by us. Finally, we designed small-scale to large-scale multi-agent tasks on two experimental platforms, StarCraft and Neural MMO, and conducted a large number of comparative experiments, which verified that our proposed GDI model has a great promotion effect on course learning.",
        "10": "Diffusion-Based Multi-Agent Reinforcement Learning with Communication. Xinyue Qi, Jianhang Tang, Jiangming Jin, Yang Zhang. Asia-Pacific Conference on Wearable Computing Systems, 2024.\nNumber of citations: 0\nAbstract: Multi-agent systems (MAS) have been widely used as a modeling tool to analyze the behaviors of members in groups, such as swarms, autonomous vehicle fleets, and smart Internet of Things devices. With their distributed architecture and model-free nature, reinforcement learning (RL) is often deployed to address issues of decision-making and action-taking in multi-agent systems to optimize overall system performance. However, as multi-agent systems involve agent coordination and interactions, this leads to extensive communication and computation overhead, resulting in decreased performance and slower convergence of multi-agent reinforcement learning. This work integrates a diffusion model into RL training to accelerate decision-making, taking into account communication among system agents. The effectiveness of this method is validated in a grid-based multi-agent predator-prey system scenario, where agents interact and confront each other via mutual communication networks. Experimental results show significant improvements in terms of learning efficiency and task performance."
    }
}