{
    "query": "neural-symbolic learning for interpretable retrieval-augmented generation",
    "result": {
        "1": "Unlocking the Potential of Generative AI through Neuro-Symbolic Architectures: Benefits and Limitations. Oualid Bougzime, Samir Jabbar, Christophe Cruz, Fr\u00e9d\u00e9ric Demoly. arXiv.org, 2025.\nNumber of citations: 4\nAbstract: Neuro-symbolic artificial intelligence (NSAI) represents a transformative approach in artificial intelligence (AI) by combining deep learning's ability to handle large-scale and unstructured data with the structured reasoning of symbolic methods. By leveraging their complementary strengths, NSAI enhances generalization, reasoning, and scalability while addressing key challenges such as transparency and data efficiency. This paper systematically studies diverse NSAI architectures, highlighting their unique approaches to integrating neural and symbolic components. It examines the alignment of contemporary AI techniques such as retrieval-augmented generation, graph neural networks, reinforcement learning, and multi-agent systems with NSAI paradigms. This study then evaluates these architectures against comprehensive set of criteria, including generalization, reasoning capabilities, transferability, and interpretability, therefore providing a comparative analysis of their respective strengths and limitations. Notably, the Neuro>Symbolic<Neuro model consistently outperforms its counterparts across all evaluation metrics. This result aligns with state-of-the-art research that highlight the efficacy of such architectures in harnessing advanced technologies like multi-agent systems.",
        "2": "VAIV bio-discovery service using transformer model and retrieval augmented generation. Seonho Kim, Juntae Yoon. BMC Bioinformatics, 2024.\nNumber of citations: 3\nAbstract: Background There has been a considerable advancement in AI technologies like LLM and machine learning to support biomedical knowledge discovery. Main body We propose a novel biomedical neural search service called \u2018VAIV Bio-Discovery\u2019, which supports enhanced knowledge discovery and document search on unstructured text such as PubMed. It mainly handles with information related to chemical compound/drugs, gene/proteins, diseases, and their interactions (chemical compounds/drugs-proteins/gene including drugs-targets, drug-drug, and drug-disease). To provide comprehensive knowledge, the system offers four search options: basic search, entity and interaction search, and natural language search. We employ T5slim_dec, which adapts the autoregressive generation task of the T5 (text-to-text transfer transformer) to the interaction extraction task by removing the self-attention layer in the decoder block. It also assists in interpreting research findings by summarizing the retrieved search results for a given natural language query with Retrieval Augmented Generation (RAG). The search engine is built with a hybrid method that combines neural search with the probabilistic search, BM25. Conclusion As a result, our system can better understand the context, semantics and relationships between terms within the document, enhancing search accuracy. This research contributes to the rapidly evolving biomedical field by introducing a new service to access and discover relevant knowledge.",
        "3": "Scientific Hypothesis Generation and Validation: Methods, Datasets, and Future Directions. Adithya Kulkarni, Fatimah Alotaibi, Xinyue Zeng, Longfeng Wu, Tong Zeng, Barry Menglong Yao, Minqian Liu, Shuaicheng Zhang, Lifu Huang, Dawei Zhou. arXiv.org, 2025.\nNumber of citations: 3\nAbstract: Large Language Models (LLMs) are transforming scientific hypothesis generation and validation by enabling information synthesis, latent relationship discovery, and reasoning augmentation. This survey provides a structured overview of LLM-driven approaches, including symbolic frameworks, generative models, hybrid systems, and multi-agent architectures. We examine techniques such as retrieval-augmented generation, knowledge-graph completion, simulation, causal inference, and tool-assisted reasoning, highlighting trade-offs in interpretability, novelty, and domain alignment. We contrast early symbolic discovery systems (e.g., BACON, KEKADA) with modern LLM pipelines that leverage in-context learning and domain adaptation via fine-tuning, retrieval, and symbolic grounding. For validation, we review simulation, human-AI collaboration, causal modeling, and uncertainty quantification, emphasizing iterative assessment in open-world contexts. The survey maps datasets across biomedicine, materials science, environmental science, and social science, introducing new resources like AHTech and CSKG-600. Finally, we outline a roadmap emphasizing novelty-aware generation, multimodal-symbolic integration, human-in-the-loop systems, and ethical safeguards, positioning LLMs as agents for principled, scalable scientific discovery.",
        "4": "Understanding Synthetic Context Extension via Retrieval Heads. Xinyu Zhao, Fangcong Yin, Greg Durrett. arXiv.org, 2024.\nNumber of citations: 3\nAbstract: Long-context LLMs are increasingly in demand for applications such as retrieval-augmented generation. To defray the cost of pretraining LLMs over long contexts, recent work takes an approach of synthetic context extension: fine-tuning LLMs with synthetically generated long-context data in a post-training stage. However, it remains unclear how and why this synthetic context extension imparts abilities for downstream long-context tasks. In this paper, we investigate fine-tuning on synthetic data for three long-context tasks that require retrieval and reasoning. We vary the realism of\"needle\"concepts to be retrieved and diversity of the surrounding\"haystack\"context, from using LLMs to construct synthetic documents to using templated relations and creating symbolic datasets. We find that models trained on synthetic data fall short of the real data, but surprisingly, the mismatch can be interpreted and even predicted in terms of a special set of attention heads that are responsible for retrieval over long context, retrieval heads (Wu et al., 2024). The retrieval heads learned on synthetic data have high overlap with retrieval heads learned on real data, and there is a strong correlation between the recall of heads learned and the downstream performance of a model. Furthermore, with attention knockout and activation patching, we mechanistically show that retrieval heads are necessary and explain model performance, although they are not totally sufficient. Our results shed light on how to interpret synthetic data fine-tuning performance and how to approach creating better data for learning real-world capabilities over long contexts.",
        "5": "Multimedia Graph Codes for Fast and Semantic Retrieval-Augmented Generation. Stefan Wagenpfeil. Electronics, 2025.\nNumber of citations: 0\nAbstract: Retrieval-Augmented Generation (RAG) has become a central approach to enhance the factual consistency and domain specificity of large language models (LLMs) by incorporating external context at inference time. However, most existing RAG systems rely on dense vector-based similarity, which fails to capture complex semantic structures, relational dependencies, and multimodal content. In this paper, we introduce Graph Codes\u2014a matrix-based encoding of Multimedia Feature Graphs\u2014as an alternative retrieval paradigm. Graph Codes preserve semantic topology by explicitly encoding entities and their typed relationships from multimodal documents, enabling structure-aware and interpretable retrieval. We evaluate our system in two domains: multimodal scene understanding (200 annotated image-question pairs) and clinical question answering (150 real-world medical queries with 10,000 structured knowledge snippets). Results show that our method outperforms dense retrieval baselines in precision (+9\u201315%), reduces hallucination rates by over 30%, and yields higher expert-rated answer quality. Theoretically, this work demonstrates that symbolic similarity over typed semantic graphs provides a more faithful alignment mechanism than latent embeddings. Practically, it enables interpretable, modality-agnostic retrieval pipelines deployable in high-stakes domains such as medicine or law. We conclude that Graph Code-based RAG bridges the gap between structured knowledge representation and neural generation, offering a robust and explainable alternative to existing approaches.",
        "6": "Retrieval-Augmented AI Chatbot for Real-Time News Summarization and Fact Verification. Dr.R. Baghia Laxmi, Hema Samanth S, Meipriyan P. International Research Journal on Advanced Engineering Hub (IRJAEH), 2025.\nNumber of citations: 0\nAbstract: Conventional news bot application relies heavily on great datasets and deep learning models for computing. This paper presents a new approach for news retrieval and summarization using AI-powered Sparse Predictive Hierarchies. In contrast to deep learning- based methods, SPH allows for incremental learning, with a low footprint that makes it lightweight, adaptive, and suitable for dynamic environments. The systematic chatbot exploits the abilities of Retrieval-Augmented Generation to search for news articles most relevant to the input query and generates context-aware responses. Neuro-symbolic reasoning supplements the ability of the chatbot to process news with better interpretability and decision-making. This approach enhances adaptability and reduces latency and computational costs, making it ideal for deployment in resource-constrained devices and real-time applications. Experimental results demonstrate that the chatbot is faster and more contextually accurate than its conventional deep learning counterparts.",
        "7": "Improving postoperative length of stay forecasting with retrieval-augmented prediction.. Brian H. Park, Chun-Nan Hsu, Austin Nguyen, Ying Q Zhou, Rodney A Gabriel. JAMIA Journal of the American Medical Informatics Association, 2025.\nNumber of citations: 0\nAbstract: None",
        "8": "Neuro-Symbolic Generative AI for Explainable Reasoning. Awolesi Abolanle Ogunboyo. International Journal of Science and Research Archive, 2025.\nNumber of citations: 0\nAbstract: The integration of neural and symbolic systems termed neuro-symbolic AI presents a compelling path toward explainable reasoning in Artificial Intelligence (AI). While deep learning models excel at pattern recognition and generative capabilities, their opaque decision-making process has raised concerns about transparency, interpretability, and trustworthiness. This research investigates the convergence of generative AI and neuro-symbolic architectures to enhance explainable reasoning. Employing a mixed-methods methodology grounded in empirical evaluation, knowledge representation, and symbolic rule induction, the study presents a hybrid framework where large language models (LLMs) are augmented with symbolic reasoning layers, allowing for natural language generation with traceable logic paths. Experimental results on benchmark datasets such as CLEVR, e-SNLI, and RuleTakers demonstrate substantial improvements in logical coherence, reasoning accuracy, and explanation fidelity over purely neural baselines. The study further explores implications for regulated domains, including healthcare, law, and cybersecurity. This work provides a foundation for future AI systems that are powerful in generation and transparent in justification, offering an interpretable-by-design approach to responsible AI.",
        "9": "Semantic Mastery: Enhancing LLMs with Advanced Natural Language Understanding. Mohanakrishnan Hariharan. arXiv.org, 2025.\nNumber of citations: 0\nAbstract: Large language models (LLMs) have greatly improved their capability in performing NLP tasks. However, deeper semantic understanding, contextual coherence, and more subtle reasoning are still difficult to obtain. The paper discusses state-of-the-art methodologies that advance LLMs with more advanced NLU techniques, such as semantic parsing, knowledge integration, and contextual reinforcement learning. We analyze the use of structured knowledge graphs, retrieval-augmented generation (RAG), and fine-tuning strategies that match models with human-level understanding. Furthermore, we address the incorporation of transformer-based architectures, contrastive learning, and hybrid symbolic-neural methods that address problems like hallucinations, ambiguity, and inconsistency in the factual perspectives involved in performing complex NLP tasks, such as question-answering text summarization and dialogue generation. Our findings show the importance of semantic precision for enhancing AI-driven language systems and suggest future research directions to bridge the gap between statistical language models and true natural language understanding.",
        "10": "A Comprehensive Review of Supervised Fine-Tuning for Large Language Models in Creative Applications and Content Moderation. Matan P, P. Velvizhy. International Congress on Information and Communication Technology, 2025.\nNumber of citations: 0\nAbstract: This article examines the pivotal role of Supervised Fine-Tuning (SFT) in adapting Large Language Models (LLMs) for specialized tasks, with a focus on story generation, content moderation, and domain-specific applications. SFT refines LLMs by leveraging labeled data to enhance task-specific performance, yet challenges such as data scarcity, maintaining diversity, and ensuring coherence persist. In story generation, balancing creativity with narrative consistency remains complex, though symbolic techniques and retrieval-augmented frameworks are advancing solutions. Content moderation benefits from LLMs' ability to detect harmful content, but challenges related to scalability, detection, and interpretability remains unresolved. Emerging trends, including data-efficient fine-tuning, self-supervised learning, and collaborative multi-LLM approaches, are addressing these limitations. This review highlights the need for ethical and culturally aware moderation frameworks while emphasizing future research directions focused on optimizing fine-tuning processes, improving generative quality, and enhancing model transparency. These advancements will drive both innovative and responsible AI applications across diverse domains."
    }
}