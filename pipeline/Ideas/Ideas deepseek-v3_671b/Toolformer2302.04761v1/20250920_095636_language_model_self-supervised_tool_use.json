{
    "query": "language model self-supervised tool use",
    "result": {
        "1": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations. Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut. International Conference on Learning Representations, 2019.\nNumber of citations: 6623\nAbstract: Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \\squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at this https URL.",
        "2": "GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction. Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li, Ying Shan. Neural Information Processing Systems, 2023.\nNumber of citations: 248\nAbstract: This paper aims to efficiently enable Large Language Models (LLMs) to use multimodal tools. Advanced proprietary LLMs, such as ChatGPT and GPT-4, have shown great potential for tool usage through sophisticated prompt engineering. Nevertheless, these models typically rely on prohibitive computational costs and publicly inaccessible data. To address these challenges, we propose the GPT4Tools based on self-instruct to enable open-source LLMs, such as LLaMA and OPT, to use tools. It generates an instruction-following dataset by prompting an advanced teacher with various multi-modal contexts. By using the Low-Rank Adaptation (LoRA) optimization, our approach facilitates the open-source LLMs to solve a range of visual problems, including visual comprehension and image generation. Moreover, we provide a benchmark to evaluate the ability of LLMs to use tools, which is performed in both zero-shot and fine-tuning ways. Extensive experiments demonstrate the effectiveness of our method on various language models, which not only significantly improves the accuracy of invoking seen tools, but also enables the zero-shot capacity for unseen tools. The code and demo are available at https://github.com/StevenGrove/GPT4Tools.",
        "3": "ToRL: Scaling Tool-Integrated RL. Xuefeng Li, Haoyang Zou, Pengfei Liu. arXiv.org, 2025.\nNumber of citations: 35\nAbstract: We introduce ToRL (Tool-Integrated Reinforcement Learning), a framework for training large language models (LLMs) to autonomously use computational tools via reinforcement learning. Unlike supervised fine-tuning, ToRL allows models to explore and discover optimal strategies for tool use. Experiments with Qwen2.5-Math models show significant improvements: ToRL-7B reaches 43.3\\% accuracy on AIME~24, surpassing reinforcement learning without tool integration by 14\\% and the best existing Tool-Integrated Reasoning (TIR) model by 17\\%. Further analysis reveals emergent behaviors such as strategic tool invocation, self-regulation of ineffective code, and dynamic adaptation between computational and analytical reasoning, all arising purely through reward-driven learning.",
        "4": "GRENADE: Graph-Centric Language Model for Self-Supervised Representation Learning on Text-Attributed Graphs. Yichuan Li, Kaize Ding, Kyumin Lee. Conference on Empirical Methods in Natural Language Processing, 2023.\nNumber of citations: 27\nAbstract: Self-supervised representation learning on text-attributed graphs, which aims to create expressive and generalizable representations for various downstream tasks, has received increasing research attention lately. However, existing methods either struggle to capture the full extent of structural context information or rely on task-specific training labels, which largely hampers their effectiveness and generalizability in practice. To solve the problem of self-supervised representation learning on text-attributed graphs, we develop a novel Graph-Centric Language model -- GRENADE. Specifically, GRENADE exploits the synergistic effect of both pre-trained language model and graph neural network by optimizing with two specialized self-supervised learning algorithms: graph-centric contrastive learning and graph-centric knowledge alignment. The proposed graph-centric self-supervised learning algorithms effectively help GRENADE to capture informative textual semantics as well as structural context information on text-attributed graphs. Through extensive experiments, GRENADE shows its superiority over state-of-the-art methods. Implementation is available at \\url{https://github.com/bigheiniu/GRENADE}.",
        "5": "SMART: Self-Aware Agent for Tool Overuse Mitigation. Cheng Qian, Emre Can Acikgoz, Hongru Wang, Xiusi Chen, Avirup Sil, Dilek Hakkani-Tur, Gokhan Tur, Heng Ji. Annual Meeting of the Association for Computational Linguistics, 2025.\nNumber of citations: 15\nAbstract: Current Large Language Model (LLM) agents demonstrate strong reasoning and tool use capabilities, but often lack self-awareness, failing to balance these approaches effectively. This imbalance leads to Tool Overuse, where models unnecessarily rely on external tools for tasks solvable with parametric knowledge, increasing computational overhead. Inspired by human metacognition, we introduce SMART (Strategic Model-Aware Reasoning with Tools), a paradigm that enhances an agent's self-awareness to optimize task handling and reduce tool overuse. To support this paradigm, we introduce SMART-ER, a dataset spanning three domains, where reasoning alternates between parametric knowledge and tool-dependent steps, with each step enriched by rationales explaining when tools are necessary. Through supervised training, we develop SMARTAgent, a family of models that dynamically balance parametric knowledge and tool use. Evaluations show that SMARTAgent reduces tool use by 24% while improving performance by over 37%, enabling 7B-scale models to match its 70B counterpart and GPT-4o. Additionally, SMARTAgent generalizes to out-of-distribution test data like GSM8K and MINTQA, maintaining accuracy with just one-fifth the tool calls. These highlight the potential of strategic tool use to enhance reasoning, mitigate overuse, and bridge the gap between model size and performance, advancing intelligent and resource-efficient agent designs.",
        "6": "Self-Training Large Language Models for Tool-Use Without Demonstrations. Ne Luo, Aryo Pradipta Gema, Xuanli He, Emile van Krieken, Pietro Lesci, Pasquale Minervini. North American Chapter of the Association for Computational Linguistics, 2025.\nNumber of citations: 4\nAbstract: Large language models (LLMs) remain prone to factual inaccuracies and computational errors, including hallucinations and mistakes in mathematical reasoning. Recent work augmented LLMs with tools to mitigate these shortcomings, but often requires curated gold tool-use demonstrations. In this paper, we investigate whether LLMs can learn to use tools without demonstrations. First, we analyse zero-shot prompting strategies to guide LLMs in tool utilisation. Second, we propose a self-training method to synthesise tool-use traces using the LLM itself. We compare supervised fine-tuning and preference fine-tuning techniques for fine-tuning the model on datasets constructed using existing Question Answering (QA) datasets, i.e., TriviaQA and GSM8K. Experiments show that tool-use enhances performance on a long-tail knowledge task: 3.7% on PopQA, which is used solely for evaluation, but leads to mixed results on other datasets, i.e., TriviaQA, GSM8K, and NQ-Open. Our findings highlight the potential and challenges of integrating external tools into LLMs without demonstrations.",
        "7": "HintMiner: Automatic Question Hints Mining From Q&A Web Posts with Language Model via Self-Supervised Learning. Zhenyu Zhang, JiuDong Yang. International Conference on Artificial Intelligence and Statistics, 2024.\nNumber of citations: 1\nAbstract: None",
        "8": "Fake Review Detection using Supervised and Semi-Supervised Learning with Natural Language Processing Techniques in Python. K. Mahesh, K. Hari Priya, K. V. S Meghana, K. Om Sai Vinay, N. Anil Chakravarthy. International Journal of Advanced Research in Science, Communication and Technology, 2023.\nNumber of citations: 0\nAbstract: This research paper explores the use of supervised and semi-supervised learning techniques along with natural language processing in Python for detecting fake reviews. The study discusses the importance of detecting fake reviews and its impact on businesses and customers. The proposed approach involves extracting relevant features from text data using various natural language processing techniques and training supervised learning models such as logistic regression, support vector machines, and random forests. Additionally, a semi-supervised learning technique called self-training is employed to improve the model's performance using unlabeled data. The effectiveness of the proposed approach is evaluated on a dataset of reviews from Amazon and Yelp, and the results show that the models achieve high accuracy in detecting fake reviews. The study concludes that the proposed approach can be a useful tool for businesses and customers to identify and filter out fake reviews.",
        "9": "Video-based learning of sign languages: one pre-train to fit them all. Maxim Novopoltsev, Aleksandr Tulenkov, R.A. Murtazin, Roman Akhidov, Iuliia Zemtsova, Emilia Bojarskaja, Daria Bondarenko, Andrey V. Savchenko, Ilya Makarov. 2024 IEEE International Conference on Data Mining Workshops (ICDMW), 2024.\nNumber of citations: 0\nAbstract: This paper presents a novel system for recognizing sign language from video, addressing a critical need for improved communication accessibility for the deaf and hard-of-hearing communities. We developed a foundation model for Isolated Sign Language Recognition that uses self-supervised pre-training to address data scarcity issues. By applying the VideoMAE algorithm and a specially prepared dataset of American Sign Language videos, we created a vision transformer for video classification that performs exceptionally well. Our model achieves state-of-the-art results for Greek (GSL) and Russian (Slovo) and comparable results for American (WLASL) and Turkish (AUTSL) sign languages. The fine-tuning process was efficient, with optimal performance in under forty epochs for each language. We also built a sign language learning tool integrated our model, showcasing its practical use in educational settings.",
        "10": "Improving Automated Assessment of English Spoken Discourse Using Transfer Learning Models-Wav2Vec 2.0. N. S. Hameed, S. Vijayakumar, K. Rajaraman, V. V. Kumar, Tamilarasan P, M. Faizal. 2025 Fifth International Conference on Advances in Electrical, Computing, Communication and Sustainable Technologies (ICAECT), 2025.\nNumber of citations: 0\nAbstract: Automated Assessment of English Spoken Discourse is invaluable tool used in the process of learning foreign languages, teaching and communication as it is capable of delivering a large number of assessments of spoken language without significant human interferences. However, common approaches may yield some issues in terms of accuracy, methods' applicability to a variety of accents and languages, and the scalability of the techniques. Conventional techniques including HMM-GMM and contemporary DL techniques like the CNN-RNN triangulate unsatisfactory results due to the inability to capture more intricate spoken discourse since they are unable to pick tune from the din. In response to these difficulties, this research proposal presents the following framework using Wav2Vec 2.0, a self-supervised learning model involved in speech signals. The proposed method makes use of Wav2Vec 2.0 to capture rich representations directly from the raw input data to minimize dependence on labelled data and improve robustness to different patterns of speech. Applicable in Python, the model is fine-tuned in spoken discourse datasets and is assessed based on primary metrics of performance; test performance: 99.1%. These results are much better than traditional and baseline methods and clearly demonstrate the high stability and potential of the model. This way, the study that addresses prior limitations and offers a stable solution contributes to the development of the automated speech assessment. Further work that could be suggested is the selection of multimodal integration approaches and real time as a way to increase more the scores of the different models and the flexibility of the system that has been proposes in education and work environments."
    }
}