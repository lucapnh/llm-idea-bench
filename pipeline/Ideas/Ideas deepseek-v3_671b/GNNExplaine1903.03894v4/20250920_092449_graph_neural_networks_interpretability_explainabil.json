{
    "query": "graph neural networks interpretability explainability",
    "result": {
        "1": "Explainability in Graph Neural Networks: A Taxonomic Survey. Hao Yuan, Haiyang Yu, Shurui Gui, Shuiwang Ji. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020.\nNumber of citations: 663\nAbstract: Deep learning methods are achieving ever-increasing performance on many artificial intelligence tasks. A major limitation of deep models is that they are not amenable to interpretability. This limitation can be circumvented by developing post hoc techniques to explain predictions, giving rise to the area of explainability. Recently, explainability of deep models on images and texts has achieved significant progress. In the area of graph data, graph neural networks (GNNs) and their explainability are experiencing rapid developments. However, there is neither a unified treatment of GNN explainability methods, nor a standard benchmark and testbed for evaluations. In this survey, we provide a unified and taxonomic view of current GNN explainability methods. Our unified and taxonomic treatments of this subject shed lights on the commonalities and differences of existing methods and set the stage for further methodological developments. To facilitate evaluations, we provide a testbed for GNN explainability, including datasets, common algorithms and evaluation metrics. Furthermore, we conduct comprehensive experiments to compare and analyze the performance of many techniques. Altogether, this work provides a unified methodological treatment of GNN explainability and a standardized testbed for evaluations.",
        "2": "A Comprehensive Survey on Trustworthy Graph Neural Networks: Privacy, Robustness, Fairness, and Explainability. Enyan Dai, Tianxiang Zhao, Huaisheng Zhu, Jun Xu, Zhimeng Guo, Hui Liu, Jiliang Tang, Suhang Wang. Machine Intelligence Research, 2022.\nNumber of citations: 156\nAbstract: Graph neural networks (GNNs) have made rapid developments in the recent years. Due to their great ability in modeling graph-structured data, GNNs are vastly used in various applications, including high-stakes scenarios such as financial analysis, traffic predictions, and drug discovery. Despite their great potential in benefiting humans in the real world, recent study shows that GNNs can leak private information, are vulnerable to adversarial attacks, can inherit and magnify societal bias from training data and lack interpretability, which have risk of causing unintentional harm to the users and society. For example, existing works demonstrate that attackers can fool the GNNs to give the outcome they desire with unnoticeable perturbation on training graph. GNNs trained on social networks may embed the discrimination in their decision process, strengthening the undesirable societal bias. Consequently, trust-worthy GNNs in various aspects are emerging to prevent the harm from GNN models and increase the users\u2019 trust in GNNs. In this paper, we give a comprehensive survey of GNNs in the computational aspects of privacy, robustness, fairness, and explainability. For each aspect, we give the taxonomy of the related methods and formulate the general frameworks for the multiple categories of trustworthy GNNs. We also discuss the future research directions of each aspect and connections between these aspects to help achieve trustworthiness.",
        "3": "Global Concept-Based Interpretability for Graph Neural Networks via Neuron Analysis. Xuanyuan Han, Pietro Barbiero, Dobrik Georgiev, Lucie Charlotte Magister, Pietro Li'o. AAAI Conference on Artificial Intelligence, 2022.\nNumber of citations: 47\nAbstract: Graph neural networks (GNNs) are highly effective on a variety of graph-related tasks; however, they lack interpretability and transparency. Current explainability approaches are typically local and treat GNNs as black-boxes. They do not look inside the model, inhibiting human trust in the model and explanations. Motivated by the ability of neurons to detect high-level semantic concepts in vision models, we perform a novel analysis on the behaviour of individual GNN neurons to answer questions about GNN interpretability. We propose a novel approach for producing global explanations for GNNs using neuron-level concepts to enable practitioners to have a high-level view of the model. Specifically, (i) to the best of our knowledge, this is the first work which shows that GNN neurons act as concept detectors and have strong alignment with concepts formulated as logical compositions of node degree and neighbourhood properties; (ii) we quantitatively assess the importance of detected concepts, and identify a trade-off between training duration and neuron-level interpretability; (iii) we demonstrate that our global explainability approach has advantages over the current state-of-the-art -- we can disentangle the explanation into individual interpretable concepts backed by logical descriptions, which reduces potential for bias and improves user-friendliness.",
        "4": "Kolmogorov-Arnold Graph Neural Networks. Gianluca De Carlo, A. Mastropietro, Aris Anagnostopoulos. arXiv.org, 2024.\nNumber of citations: 30\nAbstract: Graph neural networks (GNNs) excel in learning from network-like data but often lack interpretability, making their application challenging in domains requiring transparent decision-making. We propose the Graph Kolmogorov-Arnold Network (GKAN), a novel GNN model leveraging spline-based activation functions on edges to enhance both accuracy and interpretability. Our experiments on five benchmark datasets demonstrate that GKAN outperforms state-of-the-art GNN models in node classification, link prediction, and graph classification tasks. In addition to the improved accuracy, GKAN's design inherently provides clear insights into the model's decision-making process, eliminating the need for post-hoc explainability techniques. This paper discusses the methodology, performance, and interpretability of GKAN, highlighting its potential for applications in domains where interpretability is crucial.",
        "5": "Heterogeneous Graph Neural Networks with Post-hoc Explanations for Multi-modal and Explainable Land Use Inference. Xuehao Zhai, Junqi Jiang, Adam Dejl, Antonio Rago, Fangce Guo, Francesca Toni, Aruna Sivakumar. Information Fusion, 2024.\nNumber of citations: 5\nAbstract: Urban land use inference is a critically important task that aids in city planning and policy-making. Recently, the increased use of sensor and location technologies has facilitated the collection of multi-modal mobility data, offering valuable insights into daily activity patterns. Many studies have adopted advanced data-driven techniques to explore the potential of these multi-modal mobility data in land use inference. However, existing studies often process samples independently, ignoring the spatial correlations among neighbouring objects and heterogeneity among different services. Furthermore, the inherently low interpretability of complex deep learning methods poses a significant barrier in urban planning, where transparency and extrapolability are crucial for making long-term policy decisions. To overcome these challenges, we introduce an explainable framework for inferring land use that synergises heterogeneous graph neural networks (HGNs) with Explainable AI techniques, enhancing both accuracy and explainability. The empirical experiments demonstrate that the proposed HGNs significantly outperform baseline graph neural networks for all six land-use indicators, especially in terms of 'office' and 'sustenance'. As explanations, we consider feature attribution and counterfactual explanations. The analysis of feature attribution explanations shows that the symmetrical nature of the `residence' and 'work' categories predicted by the framework aligns well with the commuter's 'work' and 'recreation' activities in London. The analysis of the counterfactual explanations reveals that variations in node features and types are primarily responsible for the differences observed between the predicted land use distribution and the ideal mixed state. These analyses demonstrate that the proposed HGNs can suitably support urban stakeholders in their urban planning and policy-making.",
        "6": "Interpretability of Hybrid Feature Using Graph Neural Networks from Mental Arithmetic Based EEG. Min-Kyung Jung, Hakseung Kim, Seho Lee, Jung-Bin Kim, Dong-Joo Kim. Balkan Conference in Informatics, 2023.\nNumber of citations: 2\nAbstract: A high cognitive load could significantly impair problem-solving skills. Electroencephalogram (EEG)-based real-time assessment of mental workload is feasible, and graph neural networks (GNN) can classify brain activity patterns during cognitively demanding tasks with high accuracy. However, previous GNN studies pertaining to mental workload classification lack explainability. This study utilized a state-of-the-art GNN variant with GNNexplainer to find relevant connectivity during mental arithmetic (MA) tasks. In this endeavor, MA EEG recordings were retrieved from an openaccess database. The signals were transformed to graph data through the envelope correlation and power spectral density (PSD), and subjected to GNN with hierarchical graph pooling with a structure learning model to classify MA and baseline (BL). The model accuracy was $85.57 \\pm 6.27$ and $96.26 \\pm 4.14$% for the connectivity dataset and the PSD and the connectivity feature, respectively. Among the connections between nodes identified as important by GNNExplainer, two notable edge patterns were found as 1) from the left centro-parietal region to left frontal regions, and 2) the frontoparietal connection. The results indicate 1) the GNN model performance could be improved using the connectivity and PSD feature together, and 2) characteristic patterns of the connectome and PSD could be important for MA classification. The connectivity analysis by the \u2018\u2018explainable\u2019\u2019 GNN model could be beneficial in future brain activity pattern studies.",
        "7": "Neural Epistemic Network Analysis: Combining Graph Neural Networks and Epistemic Network Analysis to Model Collaborative Processes. Zheng Fang, Weiqing Wang, Guanliang Chen, Z. Swiecki. International Conference on Learning Analytics and Knowledge, 2024.\nNumber of citations: 2\nAbstract: None",
        "8": "GraphXAIN: Narratives to Explain Graph Neural Networks. Mateusz Cedro, David Martens. arXiv.org, 2024.\nNumber of citations: 2\nAbstract: Graph Neural Networks (GNNs) are a powerful technique for machine learning on graph-structured data, yet they pose challenges in interpretability. Existing GNN explanation methods usually yield technical outputs, such as subgraphs and feature importance scores, that are difficult for non-data scientists to understand and thereby violate the purpose of explanations. Motivated by recent Explainable AI (XAI) research, we propose GraphXAIN, a method that generates natural language narratives explaining GNN predictions. GraphXAIN is a model- and explainer-agnostic method that uses Large Language Models (LLMs) to translate explanatory subgraphs and feature importance scores into coherent, story-like explanations of GNN decision-making processes. Evaluations on real-world datasets demonstrate GraphXAIN's ability to improve graph explanations. A survey of machine learning researchers and practitioners reveals that GraphXAIN enhances four explainability dimensions: understandability, satisfaction, convincingness, and suitability for communicating model predictions. When combined with another graph explainer method, GraphXAIN further improves trustworthiness, insightfulness, confidence, and usability. Notably, 95% of participants found GraphXAIN to be a valuable addition to the GNN explanation method. By incorporating natural language narratives, our approach serves both graph practitioners and non-expert users by providing clearer and more effective explanations.",
        "9": "Towards Trustworthy Graph Neural Networks and Their Applications in Recommender Systems. Longfeng Wu. BigData Congress [Services Society], 2024.\nNumber of citations: 1\nAbstract: Graph Neural Networks have demonstrated remarkable success in modeling graph-structured data and are increasingly applied in various domains, including recommendation, drug discovery, and financial analysis. However, concerns regarding their trustworthiness, due to issues like data noise sensitivity, interpretability, and fairness, hinder widespread adoption. This research aims to enhance GNNs trustworthiness from three critical perspectives: reliability, explainability, and fairness. We introduce novel methods, including incorporating logical reasoning and model calibration techniques to ensure robust and reliable predictions, employing neural architecture to search for effective explanations, and analyzing the uncertainty quantification in fair GNNs with various strategies. This work establishes foundational insights and strategies for developing trustworthy GNNs, paving the way for equitable and transparent predictions in high-stakes applications.",
        "10": "Towards Faithful Class-level Self-explainability in Graph Neural Networks by Subgraph Dependencies. Fanzhen Liu, Xiaoxia Ma, Jian Yang, A. Abuadbba, Kristen Moore, Surya Nepal, C\u00e9cile Paris, Quan Z. Sheng, Jia Wu. arXiv.org, 2025.\nNumber of citations: 0\nAbstract: Enhancing the interpretability of graph neural networks (GNNs) is crucial to ensure their safe and fair deployment. Recent work has introduced self-explainable GNNs that generate explanations as part of training, improving both faithfulness and efficiency. Some of these models, such as ProtGNN and PGIB, learn class-specific prototypes, offering a potential pathway toward class-level explanations. However, their evaluations focus solely on instance-level explanations, leaving open the question of whether these prototypes meaningfully generalize across instances of the same class. In this paper, we introduce GraphOracle, a novel self-explainable GNN framework designed to generate and evaluate class-level explanations for GNNs. Our model jointly learns a GNN classifier and a set of structured, sparse subgraphs that are discriminative for each class. We propose a novel integrated training that captures graph$\\unicode{x2013}$subgraph$\\unicode{x2013}$prediction dependencies efficiently and faithfully, validated through a masking-based evaluation strategy. This strategy enables us to retroactively assess whether prior methods like ProtGNN and PGIB deliver effective class-level explanations. Our results show that they do not. In contrast, GraphOracle achieves superior fidelity, explainability, and scalability across a range of graph classification tasks. We further demonstrate that GraphOracle avoids the computational bottlenecks of previous methods$\\unicode{x2014}$like Monte Carlo Tree Search$\\unicode{x2014}$by using entropy-regularized subgraph selection and lightweight random walk extraction, enabling faster and more scalable training. These findings position GraphOracle as a practical and principled solution for faithful class-level self-explainability in GNNs."
    }
}