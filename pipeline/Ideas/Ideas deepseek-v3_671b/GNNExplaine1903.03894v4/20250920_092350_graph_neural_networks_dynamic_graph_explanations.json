{
    "query": "graph neural networks dynamic graph explanations",
    "result": {
        "1": "An Explainer for Temporal Graph Neural Networks. Wenchong He, Minh N. Vu, Zhe Jiang, M. Thai. Global Communications Conference, 2022.\nNumber of citations: 18\nAbstract: Temporal graph neural networks (TGNNs) have been widely used for modeling time-evolving graph-related tasks due to their ability to capture both graph topology dependency and non-linear temporal dynamic. The explanation of TGNNs is of vital importance for a transparent and trustworthy model. However, the complex topology structure and temporal depen-dency make explaining TGNN models very challenging. In this paper, we propose a novel explainer framework for TGNN models. Given a time series on a graph to be explained, the framework can identify dominant explanations in the form of a probabilistic graphical model in a time period. Case studies on the transportation domain demonstrate that the proposed approach can discover dynamic dependency structures in a road network for a time period.",
        "2": "TrustGuard: GNN-Based Robust and Explainable Trust Evaluation With Dynamicity Support. Jie Wang, Zheng Yan, Jiahe Lan, Elisa Bertino, W. Pedrycz. IEEE Transactions on Dependable and Secure Computing, 2023.\nNumber of citations: 17\nAbstract: Trust evaluation assesses trust relationships between entities and facilitates decision-making. Machine Learning (ML) shows great potential for trust evaluation owing to its learning capabilities. In recent years, Graph Neural Networks (GNNs), as a new ML paradigm, have demonstrated superiority in dealing with graph data. This has motivated researchers to explore their use in trust evaluation, as trust relationships among entities can be modeled as a graph. However, current trust evaluation methods that employ GNNs fail to fully satisfy the dynamic nature of trust, overlook the adverse effects of trust-related attacks, and cannot provide convincing explanations on evaluation results. To address these problems, we propose TrustGuard, a GNN-based accurate trust evaluation model that supports trust dynamicity, is robust against typical attacks, and provides explanations through visualization. Specifically, TrustGuard is designed with a layered architecture that contains a snapshot input layer, a spatial aggregation layer, a temporal aggregation layer, and a prediction layer. Among them, the spatial aggregation layer adopts a defense mechanism to robustly aggregate local trust, and the temporal aggregation layer applies an attention mechanism for effective learning of temporal patterns. Extensive experiments on two real-world datasets show that TrustGuard outperforms state-of-the-art GNN-based trust evaluation models with respect to trust prediction across single-timeslot and multi-timeslot, even in the presence of attacks. In addition, TrustGuard can explain its evaluation results by visualizing both spatial and temporal views.",
        "3": "Self-Explainable Temporal Graph Networks based on Graph Information Bottleneck. Sangwoo Seo, Sungwon Kim, Jihyeong Jung, Yoonho Lee, Chanyoung Park. Knowledge Discovery and Data Mining, 2024.\nNumber of citations: 5\nAbstract: Temporal Graph Neural Networks (TGNN) have the ability to capture both the graph topology and dynamic dependencies of interactions within a graph over time. There has been a growing need to explain the predictions of TGNN models due to the difficulty in identifying how past events influence their predictions. Since the explanation model for a static graph cannot be readily applied to temporal graphs due to its inability to capture temporal dependencies, recent studies proposed explanation models for temporal graphs. However, existing explanation models for temporal graphs rely on post-hoc explanations, requiring separate models for prediction and explanation, which is limited in two aspects: efficiency and accuracy of explanation. In this work, we propose a novel built-in explanation framework for temporal graphs, called Self-Explainable Temporal Graph Networks based on Graph Information Bottleneck (TGIB). TGIB provides explanations for event occurrences by introducing stochasticity in each temporal event based on the Information Bottleneck theory. Experimental results demonstrate the superiority of TGIB in terms of both the link prediction performance and explainability compared to state-of-the-art methods. This is the first work that simultaneously performs prediction and explanation for temporal graphs in an end-to-end manner. The source code of TGIB is available at https://github.com/sang-woo-seo/TGIB.",
        "4": "Causality-Inspired Spatial-Temporal Explanations for Dynamic Graph Neural Networks. Kesen Zhao, Liang Zhang. International Conference on Learning Representations, 2024.\nNumber of citations: 3\nAbstract: None",
        "5": "Physics-Informed Explainable Continual Learning on Graphs. Ciyuan Peng, Tao Tang, Qiuyang Yin, Xiaomei Bai, Suryani Lim, Charu C. Aggarwal. IEEE Transactions on Neural Networks and Learning Systems, 2024.\nNumber of citations: 3\nAbstract: Temporal graph learning has attracted great attention with its ability to deal with dynamic graphs. Although current methods are reasonably accurate, most of them are unexplainable due to their black-box nature. It remains a challenge to explain how temporal graph learning models adapt to information evolution. Furthermore, with the increasing application of artificial intelligence in various scientific domains, such as chemistry and biomedicine, the importance of delivering not only precise outcomes but also offering explanations regarding the learning models becomes paramount. This transparency aids users in comprehending the decision-making procedures and instills greater confidence in the generated models. To address this issue, this article proposes a novel physics-informed explainable continual learning (PiECL), focusing on temporal graphs. Our proposed method utilizes physical and mathematical algorithms to quantify the disturbance of new data to previous knowledge for obtaining changed information over time. As the proposed model is based on theories in physics, it can provide a transparent underlying mechanism for information evolution detection, thus enhancing explainability. The experimental results on three real-world datasets demonstrate that PiECL can explain the learning process, and the generated model outperforms other state-of-the-art methods. PiECL shows tremendous potential for explaining temporal graph learning in various scientific contexts.",
        "6": "[Re] Explaining Temporal Graph Models through an Explorer-Navigator Framework. Mikl\u00f3s Hamar, Matey Krastev, Kristiyan Hristov, David Beglou. Trans. Mach. Learn. Res., 2024.\nNumber of citations: 2\nAbstract: None",
        "7": "SIG: Efficient Self-Interpretable Graph Neural Network for Continuous-time Dynamic Graphs. Lanting Fang, Yulian Yang, Kai Wang, Shanshan Feng, Kaiyu Feng, Jie Gui, Shuliang Wang, Y. Ong. arXiv.org, 2024.\nNumber of citations: 1\nAbstract: While dynamic graph neural networks have shown promise in various applications, explaining their predictions on continuous-time dynamic graphs (CTDGs) is difficult. This paper investigates a new research task: self-interpretable GNNs for CTDGs. We aim to predict future links within the dynamic graph while simultaneously providing causal explanations for these predictions. There are two key challenges: (1) capturing the underlying structural and temporal information that remains consistent across both independent and identically distributed (IID) and out-of-distribution (OOD) data, and (2) efficiently generating high-quality link prediction results and explanations. To tackle these challenges, we propose a novel causal inference model, namely the Independent and Confounded Causal Model (ICCM). ICCM is then integrated into a deep learning architecture that considers both effectiveness and efficiency. Extensive experiments demonstrate that our proposed model significantly outperforms existing methods across link prediction accuracy, explanation quality, and robustness to shortcut features. Our code and datasets are anonymously released at https://github.com/2024SIG/SIG.",
        "8": "MDM: Visual Explanations for Neural Networks via Multiple Dynamic Mask. Yitao Peng, Longzhen Yang, Yihang Liu, Lianghua He. arXiv.org, 2022.\nNumber of citations: 1\nAbstract: None",
        "9": "Towards Self-Interpretable Graph Neural Networks via Augmentation-Contrastive Learning. Yizhen Li, Yang Zhang, Xiao Yao. 2025 6th International Conference on Computer Vision, Image and Deep Learning (CVIDL), 2025.\nNumber of citations: 0\nAbstract: Graph Neural Networks have demonstrated superior performance in downstream tasks, yet the decision-making mechanism lacks interpretability. Self-interpretable methods have been proven to unveil the internal decision-making mechanism and obtain reliable explanations. However, the conflict between task and interpretability performance is a widely recognized challenge. Additionally, most methods lack a collaborative interpretation of the graph structure and node features. This paper proposes a self-interpretable ACIG model based on a dynamic masking-contrastive learning mechanism, which improves task accuracy and the credibility of explanations through joint structure-feature modeling. Attention mechanism is used to learn importance probabilities, and then dynamic masking is applied to generate augmented samples for contrastive learning. End-to-end optimization is adopted to explicitly associate the importance probabilities with model performance, and sparse regularization is introduced to enhance computational efficiency and the readability of interpretations. Experiments have verified that ACIG jointly improves task performance and interpretability accuracy, providing concise and precise decision-making bases for downstream tasks.",
        "10": "InLN: Knowledge-aware Incremental Leveling Network for Dynamic Advertising. Xujia Li, Jingshu Peng, Lei Chen. Knowledge Discovery and Data Mining, 2024.\nNumber of citations: 0\nAbstract: In today's fast-paced world, advertisers are increasingly demanding real-time and accurate personalized ad delivery based on dynamic preference modeling, which emphasizes the temporality existing in both user preference and product characteristics. Meanwhile, with the development of graph neural networks (GNNs), E-commerce knowledge graphs (KG) with rich semantic relatedness are invoked to improve accuracy and provide appropriate explanations to encourage advertisers' willingness to invest in ad expenses. However, it is still challenging for existing methods to comprehensively consider both time-series interactions and graph-structured knowledge triples in a unified model, i.e., the case in knowledge-aware dynamic advertising. The interaction graph between users and products changes rapidly over time, while the knowledge in KG remains relatively stable. This results in an uneven distribution of temporal and semantic information, causing existing GNNs to fail in this scenario. In this work, we quantitatively define the above phenomenon as temporal unevenness and introduce the Incremental Leveling Network (InLN) with three novel techniques: the periodic-focusing window for node-level dynamic modeling, the biased temporal walk for subgraph-level dynamic modeling and the incremental leveling mechanism for KG updating. Verified by comprehensive and intensive experiments, InLN outperforms nine baseline models in three tasks by substantial margins, reaching up to a 9.9% improvement and averaging a 5.7% increase."
    }
}