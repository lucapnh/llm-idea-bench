{
    "query": "Gaussian Approximation Potentials novel applications",
    "result": {
        "1": "Gaussian approximation potentials: Theory, software implementation and application examples.. Sascha Klawohn, James P Darby, J. Kermode, G\u00e1bor Cs\u00e1nyi, Miguel A. Caro, Albert P. Bart'ok. Journal of Chemical Physics, 2023.\nNumber of citations: 40\nAbstract: Gaussian Approximation Potentials (GAPs) are a class of Machine Learned Interatomic Potentials routinely used to model materials and molecular systems on the atomic scale. The software implementation provides the means for both fitting models using ab\u00a0initio data and using the resulting potentials in atomic simulations. Details of the GAP theory, algorithms and software are presented, together with detailed usage examples to help new and existing users. We review some recent developments to the GAP framework, including Message Passing Interface parallelisation of the fitting code enabling its use on thousands of central processing unit cores and compression of descriptors to eliminate the poor scaling with the number of different chemical elements.",
        "2": "Data-efficient iterative training of Gaussian approximation potentials: Application to surface structure determination of rutile IrO2 and RuO2.. Jakob Timmermann, Yonghyuk Lee, Carsten G. Staacke, Johannes T. Margraf, C. Scheurer, K. Reuter. Journal of Chemical Physics, 2021.\nNumber of citations: 22\nAbstract: Machine-learning interatomic potentials, such as Gaussian Approximation Potentials (GAPs), constitute a powerful class of surrogate models to computationally involved first-principles calculations. At a similar predictive quality but significantly reduced cost, they could leverage otherwise barely tractable extensive sampling as in global surface structure determination (SSD). This efficiency is jeopardized though, if an a priori unknown structural and chemical search space as in SSD requires an excessive number of first-principles data for the GAP training. To this end, we present a general and data-efficient iterative training protocol that blends the creation of new training data with the actual surface exploration process. Demonstrating this protocol with the SSD of low-index facets of rutile IrO2 and RuO2, the involved simulated annealing on the basis of the refining GAP identifies a number of unknown terminations even in the restricted sub-space of (1 \u00d7 1) surface unit cells. Particularly in an O-poor environment, some of these, then metal-rich terminations, are thermodynamically most stable and are reminiscent of complexions as discussed for complex ceramic materials.",
        "3": "The Unreasonable Effectiveness of Gaussian Score Approximation for Diffusion Models and its Applications. Binxu Wang, John J. Vastola. Trans. Mach. Learn. Res., 2024.\nNumber of citations: 17\nAbstract: By learning the gradient of smoothed data distributions, diffusion models can iteratively generate samples from complex distributions. The learned score function enables their generalization capabilities, but how the learned score relates to the score of the underlying data manifold remains largely unclear. Here, we aim to elucidate this relationship by comparing learned neural scores to the scores of two kinds of analytically tractable distributions: Gaussians and Gaussian mixtures. The simplicity of the Gaussian model makes it theoretically attractive, and we show that it admits a closed-form solution and predicts many qualitative aspects of sample generation dynamics. We claim that the learned neural score is dominated by its linear (Gaussian) approximation for moderate to high noise scales, and supply both theoretical and empirical arguments to support this claim. Moreover, the Gaussian approximation empirically works for a larger range of noise scales than naive theory suggests it should, and is preferentially learned early in training. At smaller noise scales, we observe that learned scores are better described by a coarse-grained (Gaussian mixture) approximation of training data than by the score of the training distribution, a finding consistent with generalization. Our findings enable us to precisely predict the initial phase of trained models' sampling trajectories through their Gaussian approximations. We show that this allows the skipping of the first 15-30% of sampling steps while maintaining high sample quality (with a near state-of-the-art FID score of 1.93 on CIFAR-10 unconditional generation). This forms the foundation of a novel hybrid sampling method, termed analytical teleportation, which can seamlessly integrate with and accelerate existing samplers, including DPM-Solver-v3 and UniPC. Our findings suggest ways to improve the design and training of diffusion models.",
        "4": "Gaussian random field approximation via Stein's method with applications to wide random neural networks. K. Balasubramanian, L. Goldstein, Nathan Ross, A. Salim. Applied and Computational Harmonic Analysis, 2023.\nNumber of citations: 12\nAbstract: We derive upper bounds on the Wasserstein distance ($W_1$), with respect to $\\sup$-norm, between any continuous $\\mathbb{R}^d$ valued random field indexed by the $n$-sphere and the Gaussian, based on Stein's method. We develop a novel Gaussian smoothing technique that allows us to transfer a bound in a smoother metric to the $W_1$ distance. The smoothing is based on covariance functions constructed using powers of Laplacian operators, designed so that the associated Gaussian process has a tractable Cameron-Martin or Reproducing Kernel Hilbert Space. This feature enables us to move beyond one dimensional interval-based index sets that were previously considered in the literature. Specializing our general result, we obtain the first bounds on the Gaussian random field approximation of wide random neural networks of any depth and Lipschitz activation functions at the random field level. Our bounds are explicitly expressed in terms of the widths of the network and moments of the random weights. We also obtain tighter bounds when the activation function has three bounded derivatives.",
        "5": "New, Simple and Accurate Approximation for the Gaussian Q Function With Applications. Dharmendra Sadhwani, Aditya Powari, Naman Mehta. IEEE Communications Letters, 2022.\nNumber of citations: 8\nAbstract: In this letter, we propose novel, accurate approximation for the Gaussian <inline-formula> <tex-math notation=\"LaTeX\">$Q$ </tex-math></inline-formula> function which is expressed as the sum of simple exponentials. To do so, we use the composite Gauss quadrature numerical integration method incorporating a special mid point rule. The nuances on the accuracy as well as an insight on the tractability of the proposed approximation is exhaustively presented in this letter. We show that this approximation facilitates the symbol error probability of square quadrature amplitude modulation technique over the versatile Fluctuating Beckmann fading model and the practical Fisher-Snedecor <inline-formula> <tex-math notation=\"LaTeX\">$\\mathcal {F}$ </tex-math></inline-formula> distribution. Lastly, the analysis is justified with the help of Monte-Carlo simulations.",
        "6": "Gaussian Approximation Potentials for iron from extended first-principles database (Data Download). D. Dragoni, T. Daff, G\u00e1bor Cs\u00e1nyi, N. Marzari. , 2017.\nNumber of citations: 5\nAbstract: None",
        "7": "Piece-wise approximation for Gaussian Q -function and its applications. Jyoti Gupta, Ashish Goel. , 2022.\nNumber of citations: 2\nAbstract: None",
        "8": "A Nonparametric Approach for Estimating the Effective Sample Size in Gaussian Approximation of Expected Value of Sample Information. Linke Li, Hawre Jalal, Anna Heath. Medical decision making, 2024.\nNumber of citations: 1\nAbstract: The effective sample size (ESS) measures the informational value of a probability distribution in terms of an equivalent number of study participants. The ESS plays a crucial role in estimating the expected value of sample information (EVSI) through the Gaussian approximation approach. Despite the significance of ESS, except for a limited number of scenarios, existing ESS estimation methods within the Gaussian approximation framework are either computationally expensive or potentially inaccurate. To address these limitations, we propose a novel approach that estimates the ESS using the summary statistics of generated datasets and nonparametric regression methods. The simulation experiments suggest that the proposed method provides accurate ESS estimates at a low computational cost, making it an efficient and practical way to quantify the information contained in the probability distribution of a parameter. Overall, determining the ESS can help analysts understand the uncertainty levels in complex prior distributions in the probability analyses of decision models and perform efficient EVSI calculations. Highlights Effective sample size (ESS) quantifies the informational value of probability distributions, essential for calculating the expected value of sample information (EVSI) using the Gaussian approximation approach. However, current ESS estimation methods are limited by high computational demands and potential inaccuracies. We propose a novel ESS estimation method that uses summary statistics and nonparametric regression models to efficiently and accurately estimate ESS. The effectiveness and accuracy of our method are validated through simulations, demonstrating significant improvements in computational efficiency and estimation accuracy.",
        "9": "Hydrogen Bonding: Ab Initio Accuracy From Fast Interatomic Gaussian Approximation Potentials. M. \u00d6eren. , 2018.\nNumber of citations: 0\nAbstract: None",
        "10": "Gaussian Approximation and Output Analysis for High-Dimensional MCMC. Ardjen Pengel, Jun Yang, Zhou Zhou. , 2024.\nNumber of citations: 0\nAbstract: The widespread use of Markov Chain Monte Carlo (MCMC) methods for high-dimensional applications has motivated research into the scalability of these algorithms with respect to the dimension of the problem. Despite this, numerous problems concerning output analysis in high-dimensional settings have remained unaddressed. We present novel quantitative Gaussian approximation results for a broad range of MCMC algorithms. Notably, we analyse the dependency of the obtained approximation errors on the dimension of both the target distribution and the feature space. We demonstrate how these Gaussian approximations can be applied in output analysis. This includes determining the simulation effort required to guarantee Markov chain central limit theorems and consistent estimation of the variance and effective sample size in high-dimensional settings. We give quantitative convergence bounds for termination criteria and show that the termination time of a wide class of MCMC algorithms scales polynomially in dimension while ensuring a desired level of precision. Our results offer guidance to practitioners for obtaining appropriate standard errors and deciding the minimum simulation effort of MCMC algorithms in both multivariate and high-dimensional settings."
    }
}