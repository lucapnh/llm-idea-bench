{
    "query": "large language models adaptive problem solving",
    "result": {
        "1": "Adaption-of-Thought: Learning Question Difficulty Improves Large Language Models for Reasoning. Mayi Xu, Yongqi Li, Ke Sun, Tieyun Qian. Conference on Empirical Methods in Natural Language Processing, 2024.\nNumber of citations: 11\nAbstract: Large language models (LLMs) have shown excellent capability for solving reasoning problems. Existing approaches do not differentiate the question difficulty when designing prompting methods for them. Clearly, a simple method cannot elicit sufficient knowledge from LLMs to answer a hard question. Meanwhile, a sophisticated one will force the LLM to generate redundant or even inaccurate intermediate steps toward a simple question. Consequently, the performance of existing methods fluctuates among various questions.In this work, we propose Adaption-of-Thought (AdoT), an adaptive method to improve LLMs for the reasoning problem, which first measures the question difficulty and then tailors demonstration set construction and difficulty-adapted retrieval strategies for the adaptive demonstration construction. Experimental results on three reasoning tasks prove the superiority of our proposed method, showing an absolute improvement of up to 5.5% on arithmetic reasoning, 7.4% on symbolic reasoning, and 2.3% on commonsense reasoning. Our codes and implementation details are available at: https://github.com/NLPGM/AdoT",
        "2": "Toward Adaptive Reasoning in Large Language Models with Thought Rollback. Sijia Chen, Baochun Li. International Conference on Machine Learning, 2024.\nNumber of citations: 10\nAbstract: Large language models (LLMs) have been routinely used to solve various tasks using step-by-step reasoning. However, the structure of intermediate reasoning steps, or thoughts, is rigid and unidirectional, such as chains, trees, or acyclic-directed graphs. Consequently, the resulting inflexible and forward-only reasoning may not address challenging tasks and fail when the LLM frequently gives false responses, i.e., ``hallucinations''. This paper proposes a new reasoning framework, called Thought Rollback (TR), allowing LLMs to adaptively build thought structure while maintaining effective reasoning toward problem-solving under ``hallucinations''. The core mechanism of TR is rolling back thoughts, which allows LLMs to perform error analysis on thoughts, and thus roll back to any previously mistaken thought for revision. Subsequently, by including such trial-and-error in the prompt to guide the LLM, each rollback leads to one more reliable reasoning path. Therefore, starting with a simple prompt without human annotations, LLM with TR adaptively and gradually explores thoughts for a correct solution. Comprehensive experiments on mathematical problems and multi-task reasoning demonstrate the state-of-the-art performance of TR in terms of problem-solving rate and interaction cost. For instance, the solving rate of GPT-4 with TR outperforms the current best by $9\\%$ on the MATH dataset.",
        "3": "Adaptive Resource Allocation Optimization Using Large Language Models in Dynamic Wireless Environments. Hyeonho Noh, B. Shim, Hyun Jong Yang. IEEE Transactions on Vehicular Technology, 2025.\nNumber of citations: 6\nAbstract: Deep learning (DL) has made notable progress in addressing complex radio access network control challenges that conventional analytic methods have struggled to solve. However, DL has shown limitations in solving constrained NP-hard problems often encountered in network optimization, such as those involving quality of service (QoS) or discrete variables like user indices. Current solutions rely on domain-specific architectures or heuristic techniques, and a general DL approach for constrained optimization remains undeveloped. Moreover, even minor changes in communication objectives demand time-consuming retraining, limiting their adaptability to dynamic environments where task objectives, constraints, environmental factors, and communication scenarios frequently change. To address these challenges, we propose a large language model for resource allocation optimizer (LLM-RAO), a novel approach that harnesses the capabilities of LLMs to address the complex resource allocation problem while adhering to QoS constraints. By employing a prompt-based tuning strategy to flexibly convey ever-changing task descriptions and requirements to the LLM, LLM-RAO demonstrates robust performance and seamless adaptability in dynamic environments without requiring extensive retraining. Simulation results reveal that LLM-RAO achieves up to a 40% performance enhancement compared to conventional DL methods and up to an $80$\\% improvement over analytical approaches. Moreover, in scenarios with fluctuating communication objectives, LLM-RAO attains up to 2.9 times the performance of traditional DL-based networks.",
        "4": "A Demonstration of Adaptive Collaboration of Large Language Models for Medical Decision-Making. Y. Kim, Chanwoo Park, H. Jeong, Cristina Grau-Vilchez, Yik Siu Chan, Xuhai Xu, Daniel McDuff, Hyeonhoon Lee, C. Breazeal, Hae Won Park. arXiv.org, 2024.\nNumber of citations: 2\nAbstract: Medical Decision-Making (MDM) is a multi-faceted process that requires clinicians to assess complex multi-modal patient data patient, often collaboratively. Large Language Models (LLMs) promise to streamline this process by synthesizing vast medical knowledge and multi-modal health data. However, single-agent are often ill-suited for nuanced medical contexts requiring adaptable, collaborative problem-solving. Our MDAgents addresses this need by dynamically assigning collaboration structures to LLMs based on task complexity, mimicking real-world clinical collaboration and decision-making. This framework improves diagnostic accuracy and supports adaptive responses in complex, real-world medical scenarios, making it a valuable tool for clinicians in various healthcare settings, and at the same time, being more efficient in terms of computing cost than static multi-agent decision making methods.",
        "5": "Semantic Exploration with Adaptive Gating for Efficient Problem Solving with Language Models. Sungjae Lee, Hyejin Park, Jaechang Kim, Jungseul Ok. Annual Meeting of the Association for Computational Linguistics, 2025.\nNumber of citations: 2\nAbstract: Recent advancements in large language models (LLMs) have shown remarkable potential in various complex tasks requiring multi-step reasoning methods like tree search to explore diverse reasoning paths. However, existing methods often suffer from computational inefficiency and redundancy. First, they overlook the diversity of task difficulties, leading to unnecessarily extensive searches even for easy tasks. Second, they neglect the semantics of reasoning paths, resulting in redundant exploration of semantically identical paths. To address these limitations, we propose Semantic Exploration with Adaptive Gating (SEAG), a computationally efficient method. SEAG employs an adaptive gating mechanism that dynamically decides whether to conduct a tree search, based on the confidence level of answers from a preceding simple reasoning method. Furthermore, its tree-based exploration consolidates semantically identical reasoning steps, reducing redundant explorations while maintaining or even improving accuracy. Our extensive experiments demonstrate that SEAG significantly improves accuracy by 4.3% on average while requiring only 31% of computational costs compared to existing tree search-based methods on complex reasoning benchmarks including GSM8K and ARC with diverse language models such as Llama2, Llama3, and Mistral. Our code is available at https://github.com/ml-postech/SEAG-semantic-exploration-with-adaptive-gating .",
        "6": "36th International Conference on Design Theory and Methodology (DTM), 2024.\nNumber of citations: 0\nAbstract: \n Large Language Models (LLMs) have emerged as a pivotal technology in the evolving world. With respect to design, the significance of LLMs lies in their transformative potential for collaborating design teams. However, it is not known whether LLMs can emulate cognitive and social attributes which are known to be important during design, such as cognitive style. This research evaluates the efficacy of LLMs to emulate aspects of Kirton\u2019s adaption innovation inventory, which characterizes approaches to problem-solving. Specifically, we use LLMs to generate solutions for three design problems using two different cognitive style prompts (adaptive and innovative), and evaluate the solutions with respect to feasibility and paradigm relatedness. We found that solutions generated using the adaptive prompt tend to display higher feasibility and are paradigm preserving, while solutions generated using the innovative prompts were more paradigm modifying. This aligns with prior work and expectations based on KAI theory. Ultimately, these results demonstrate that LLMs can be prompted to accurately emulate cognitive style.",
        "7": "Optimizing Mathematical Problem-Solving Reasoning Chains and Personalized Explanations Using Large Language Models: A Study in Applied Mathematics Education. Biao Ye, Yue Xi, Qiwen Zhao. Journal of AI-Powered Medical Innovations (International online ISSN 3078-1930), 2024.\nNumber of citations: 1\nAbstract: This study investigates the optimization of mathematical problem-solving through Large Language Models (LLMs), focusing on developing enhanced reasoning chains and personalized explanations in applied mathematics education. The research implements a novel framework integrating LLM-based reasoning chain generation with adaptive personalization algorithms, demonstrating significant improvements in student learning outcomes. Through a comprehensive experimental evaluation involving 2,854 students across different proficiency levels, the system achieved a 98.7% accuracy rate in mathematical problem-solving and a 92.3% user satisfaction rate. Implementing personalized explanation systems resulted in a 27.8% improvement in student comprehension and a 31.5% increase in engagement rates. Performance analysis revealed robust scalability, maintaining response times below 312ms under peak loads of 850 requests per second. The findings demonstrate the effectiveness of LLM-based approaches in enhancing mathematics education through automated reasoning chain generation and personalized instruction. The research contributes to advancing AI-assisted educational technologies and provides valuable insights for developing intelligent tutoring systems in STEM education.",
        "8": "Comparison of Problem-solving Performance Across Mathematical Domains with Large Language Models. Nikolaiev A, Derevianchenko O. Artificial Intelligence, 2024.\nNumber of citations: 0\nAbstract: This study investigates problem-solving performance across four mathematical domains, using statistical techniques to analyse domain-specific differences. By leveraging the NuminaMath-TIR dataset, we categorized problems into algebra, geometry, number theory, and combinatorics, selecting 8,000 problems for the analysis. Models including GPT-4o-mini, Mathstral-7B, Qwen2.5-Math-7B, and Llama-3.1-8B-Instruct were applied to assess answer correctness. Significant differences in solution accuracy were identified, with algebra showing the highest correctness rates and combinatorics the lowest. The results highlight the impact of domain on model performance and suggest the potential for tool-integrated reasoning (TIR) techniques to enhance consistency across domains. Future work can explore targeted model training improvements, aiming to optimize educational technologies and adaptive learning systems",
        "9": "LLMs-guided adaptive compensator: Bringing Adaptivity to Automatic Control Systems with Large Language Models. Zhongchao Zhou, Yuxi Lu, Yaonan Zhu, Yifan Zhao, Bin He, Liang He, Wenwen Yu, Yusuke Iwasawa. arXiv.org, 2025.\nNumber of citations: 0\nAbstract: With rapid advances in code generation, reasoning, and problem-solving, Large Language Models (LLMs) are increasingly applied in robotics. Most existing work focuses on high-level tasks such as task decomposition. A few studies have explored the use of LLMs in feedback controller design; however, these efforts are restricted to overly simplified systems, fixed-structure gain tuning, and lack real-world validation. To further investigate LLMs in automatic control, this work targets a key subfield: adaptive control. Inspired by the framework of model reference adaptive control (MRAC), we propose an LLM-guided adaptive compensator framework that avoids designing controllers from scratch. Instead, the LLMs are prompted using the discrepancies between an unknown system and a reference system to design a compensator that aligns the response of the unknown system with that of the reference, thereby achieving adaptivity. Experiments evaluate five methods: LLM-guided adaptive compensator, LLM-guided adaptive controller, indirect adaptive control, learning-based adaptive control, and MRAC, on soft and humanoid robots in both simulated and real-world environments. Results show that the LLM-guided adaptive compensator outperforms traditional adaptive controllers and significantly reduces reasoning complexity compared to the LLM-guided adaptive controller. The Lyapunov-based analysis and reasoning-path inspection demonstrate that the LLM-guided adaptive compensator enables a more structured design process by transforming mathematical derivation into a reasoning task, while exhibiting strong generalizability, adaptability, and robustness. This study opens a new direction for applying LLMs in the field of automatic control, offering greater deployability and practicality compared to vision-language models.",
        "10": "Emulating Cognitive Style in Large Language Models. Vasvi Agarwal, Kathryn Jablokow, Christopher Mccomb. Volume"
    }
}