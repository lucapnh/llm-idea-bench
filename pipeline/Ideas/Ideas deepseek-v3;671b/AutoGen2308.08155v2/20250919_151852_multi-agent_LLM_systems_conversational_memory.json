{
    "query": "multi-agent LLM systems conversational memory",
    "result": {
        "1": "Why Do Multi-Agent LLM Systems Fail?. Mert Cemri, Melissa Z. Pan, Shuyi Yang, Lakshya A Agrawal, Bhavya Chopra, Rishabh Tiwari, Kurt Keutzer, Aditya Parameswaran, Dan Klein, K. Ramchandran, Matei Zaharia, Joseph E. Gonzalez, Ion Stoica. arXiv.org, 2025.\nNumber of citations: 70\nAbstract: Despite growing enthusiasm for Multi-Agent LLM Systems (MAS), their performance gains on popular benchmarks often remain minimal compared with single-agent frameworks. This gap highlights the need to systematically analyze the challenges hindering MAS effectiveness. We present MAST (Multi-Agent System Failure Taxonomy), the first empirically grounded taxonomy designed to understand MAS failures. We analyze seven popular MAS frameworks across over 200 tasks, involving six expert human annotators. Through this process, we identify 14 unique failure modes, organized into 3 overarching categories, (i) specification issues, (ii) inter-agent misalignment, and (iii) task verification. MAST emerges iteratively from rigorous inter-annotator agreement studies, achieving a Cohen's Kappa score of 0.88. To support scalable evaluation, we develop a validated LLM-as-a-Judge pipeline integrated with MAST. We leverage two case studies to demonstrate MAST's practical utility in analyzing failures and guiding MAS development. Our findings reveal that identified failures require more complex solutions, highlighting a clear roadmap for future research. We open source our comprehensive dataset and LLM annotator to facilitate further development of MAS.",
        "2": "LLM Multi-Agent Systems: Challenges and Open Problems. Shanshan Han, Qifan Zhang, Yuhang Yao, Weizhao Jin, Zhaozhuo Xu, Chaoyang He. arXiv.org, 2024.\nNumber of citations: 67\nAbstract: This paper explores multi-agent systems and identify challenges that remain inadequately addressed. By leveraging the diverse capabilities and roles of individual agents, multi-agent systems can tackle complex tasks through agent collaboration. We discuss optimizing task allocation, fostering robust reasoning through iterative debates, managing complex and layered context information, and enhancing memory management to support the intricate interactions within multi-agent systems. We also explore potential applications of multi-agent systems in blockchain systems to shed light on their future development and application in real-world distributed systems.",
        "3": "From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models. Na Liu, Liangyu Chen, Xiaoyu Tian, Wei Zou, Kaijiang Chen, Ming Cui. arXiv.org, 2024.\nNumber of citations: 38\nAbstract: This paper introduces RAISE (Reasoning and Acting through Scratchpad and Examples), an advanced architecture enhancing the integration of Large Language Models (LLMs) like GPT-4 into conversational agents. RAISE, an enhancement of the ReAct framework, incorporates a dual-component memory system, mirroring human short-term and long-term memory, to maintain context and continuity in conversations. It entails a comprehensive agent construction scenario, including phases like Conversation Selection, Scene Extraction, CoT Completion, and Scene Augmentation, leading to the LLMs Training phase. This approach appears to enhance agent controllability and adaptability in complex, multi-turn dialogues. Our preliminary evaluations in a real estate sales context suggest that RAISE has some advantages over traditional agents, indicating its potential for broader applications. This work contributes to the AI field by providing a robust framework for developing more context-aware and versatile conversational agents.",
        "4": "Agents Under Siege: Breaking Pragmatic Multi-Agent LLM Systems with Optimized Prompt Attacks. Rana Muhammad Shahroz Khan, Zhen Tan, Sukwon Yun, Charles Flemming, Tianlong Chen. Annual Meeting of the Association for Computational Linguistics, 2025.\nNumber of citations: 7\nAbstract: Most discussions about Large Language Model (LLM) safety have focused on single-agent settings but multi-agent LLM systems now create novel adversarial risks because their behavior depends on communication between agents and decentralized reasoning. In this work, we innovatively focus on attacking pragmatic systems that have constrains such as limited token bandwidth, latency between message delivery, and defense mechanisms. We design a $\\textit{permutation-invariant adversarial attack}$ that optimizes prompt distribution across latency and bandwidth-constraint network topologies to bypass distributed safety mechanisms within the system. Formulating the attack path as a problem of $\\textit{maximum-flow minimum-cost}$, coupled with the novel $\\textit{Permutation-Invariant Evasion Loss (PIEL)}$, we leverage graph-based optimization to maximize attack success rate while minimizing detection risk. Evaluating across models including $\\texttt{Llama}$, $\\texttt{Mistral}$, $\\texttt{Gemma}$, $\\texttt{DeepSeek}$ and other variants on various datasets like $\\texttt{JailBreakBench}$ and $\\texttt{AdversarialBench}$, our method outperforms conventional attacks by up to $7\\times$, exposing critical vulnerabilities in multi-agent systems. Moreover, we demonstrate that existing defenses, including variants of $\\texttt{Llama-Guard}$ and $\\texttt{PromptGuard}$, fail to prohibit our attack, emphasizing the urgent need for multi-agent specific safety mechanisms.",
        "5": "Seven Security Challenges That Must be Solved in Cross-domain Multi-agent LLM Systems. Ronny Ko, Jiseong Jeong, Shuyuan Zheng, Chuan Xiao, Taewan Kim, Makoto Onizuka, Wonyong Shin. arXiv.org, 2025.\nNumber of citations: 3\nAbstract: Large language models (LLMs) are rapidly evolving into autonomous agents that cooperate across organizational boundaries, enabling joint disaster response, supply-chain optimization, and other tasks that demand decentralized expertise without surrendering data ownership. Yet, cross-domain collaboration shatters the unified trust assumptions behind current alignment and containment techniques. An agent benign in isolation may, when receiving messages from an untrusted peer, leak secrets or violate policy, producing risks driven by emergent multi-agent dynamics rather than classical software bugs. This position paper maps the security agenda for cross-domain multi-agent LLM systems. We introduce seven categories of novel security challenges, for each of which we also present plausible attacks, security evaluation metrics, and future research guidelines.",
        "6": "Providing Task Execution Capabilities in LLM-Based Conversational Assistants. Nickolas Anselmo Carneiro Mororo, Jorge Luiz Ara\u00fajo, Rafael Bomfim, Vasco Furtado. Brazilian Symposium on Information Systems, 2025.\nNumber of citations: 1\nAbstract: Context: Conversational assistants (CAs) powered by Large Language Models (LLMs) excel in generating coherent responses, but struggle with task execution requiring long-term memory or temporal awareness. This research addresses such limitations, particularly in healthcare applications where task continuity is crucial. Problem: Existing CAs often fail to meet user expectations in scenarios requiring long-term task management, such as scheduling reminders or monitoring chronic health conditions. This lack of temporal reasoning and memory continuity undermines user trust and engagement. Proposed Solution: We propose a multi-agent system integrating LLMs with structured data processing, that enables CAs to recognize, manage, and execute user requests, such as scheduling reminders or tracking tasks over time. The solution bridges the gap between conversational fluency and actionable outcomes. IT Theory: The study draws on theories of Human-Computer Interaction (HCI) and Temporal Awareness in Information Technology for allowing structured task execution. Method: The research combines proof-of-concept implementation with healthcare case studies. Two experimental phases evaluated the solution: a pilot test with 35 users and an expanded trial with 437 participants focusing on chronic disease management. Summarization of Results: Results demonstrated an improvement in task fulfillment and user engagement. The assistant successfully addressed 82% of task requests in simulations, with reduced user frustration and longer interactions during real-world trials. Contributions and Impact on IT: The study enhances the capabilities of conversational assistants by introducing temporal reasoning and structured task execution, making them more effective in managing real-world tasks. These advancements improve the applicability of CAs, particularly in domains like healthcare, where accurate and timely task management is critical.",
        "7": "RecUserSim: A Realistic and Diverse User Simulator for Evaluating Conversational Recommender Systems. Luyu Chen, Quanyu Dai, Zeyu Zhang, Xueyang Feng, Mingyu Zhang, Pengcheng Tang, Xu Chen, Yue Zhu, Zhenhua Dong. The Web Conference, 2025.\nNumber of citations: 1\nAbstract: Conversational recommender systems (CRS) enhance user experience through multi-turn interactions, yet evaluating CRS remains challenging. User simulators can provide comprehensive evaluations through interactions with CRS, but building realistic and diverse simulators is difficult. While recent work leverages large language models (LLMs) to simulate user interactions, they still fall short in emulating individual real users across diverse scenarios and lack explicit rating mechanisms for quantitative evaluation. To address these gaps, we propose RecUserSim, an LLM agent-based user simulator with enhanced simulation realism and diversity while providing explicit scores. RecUserSim features several key modules: a profile module for defining realistic and diverse user personas, a memory module for tracking interaction history and discovering unknown preferences, and a core action module inspired by Bounded Rationality theory that enables nuanced decision-making while generating more fine-grained actions and personalized responses. To further enhance output control, a refinement module is designed to fine-tune final responses. Experiments demonstrate that RecUserSim generates diverse, controllable outputs and produces realistic, high-quality dialogues, even with smaller base LLMs. The ratings generated by RecUserSim show high consistency across different base LLMs, highlighting its effectiveness for CRS evaluation.",
        "8": "Cooperative Scheduling and Hierarchical Memory Model for Multi-Agent Systems. Huhai Zou, Rongzhen Li, Tianhao Sun, Fei Wang, Ta-Hsin Li, Kai Liu. Asia, 2024.\nNumber of citations: 0\nAbstract: Large Language Models (LLMs) are leading a technological revolution. This gives agents based on LLMs renewed vitality, and multi-agent collaboration is showing the potential to foster new forms of intelligence. However, current multi-agent systems face two major challenges: the first is the issue of resource coordination and scheduling within multi-agent systems, and the second is the limitation of the context window in large models, which hinders the practical application of agents in long-term conversational scenarios, urgently requiring improvements in memory capabilities. To address these challenges, we propose a collaborative scheduling strategy and hierarchical memory model for LLM-based multi-agent systems inspired by operating systems. First, we design a time-sharing scheduling strategy, analogous to process scheduling in operating systems, which divides the resource usage cycle into finer-grained single-step workflows, allocating independent resource windows to different agents to reduce resource contention and conflicts. Second, we introduce a hierarchical memory model based on the multi-level cache architecture of operating systems, segmenting the agents' memory into core memory, main memory, and vague memory areas, thereby significantly improving memory retention and retrieval efficiency in LLM - based agents when handling complex tasks. Experimental results demonstrate that our proposed method achieves efficient resource allocation in multi-agent systems while significantly enhancing the memory capabilities of agents and overall system performance.",
        "9": "Chorus of the Past: Toward Designing a Multi-agent Conversational Reminiscence System with Digital Artifacts for Older Adults. Jingwei Sun, Zhongyue Zhang, Mengyang Wang, Nianlong Li, Zhangwei Lu, Yan Xiang, Liuxin Zhang, Yu Zhang, Qianying Wang, Mingming Fan. International Conference on Human Factors in Computing Systems, 2025.\nNumber of citations: 0\nAbstract: Reminiscence has been shown to provide benefits for older adults, but traditionally relies on personal photos as memory cues and interactions with real people who may not always be available. We present ReminiBuddy, a novel LLM-powered multi-agent conversational system, which allows older adults to engage with two distinct agents\u2014one embodying an older identity and the other a younger identity\u2014while using not only personal photos but also 3D models of generic nostalgic objects as memory cues. Our study, with older adult participants, found that the conversational approach both enjoyable and beneficial for reminiscence. While the younger agent was perceived as more emotionally engaging, the older one fostered greater resonance in content. Personal photos prompted autobiographical memories, whereas 3D generic nostalgic objects evoked shared memories of an era, contributing to a more multifaceted reminiscence experience. We further present design implications for better supporting older adults in reminiscing with LLM-powered conversational agents.",
        "10": "ScamAgents: How AI Agents Can Simulate Human-Level Scam Calls. Sanket Badhe. arXiv.org, 2025.\nNumber of citations: 0\nAbstract: Large Language Models (LLMs) have demonstrated impressive fluency and reasoning capabilities, but their potential for misuse has raised growing concern. In this paper, we present ScamAgent, an autonomous multi-turn agent built on top of LLMs, capable of generating highly realistic scam call scripts that simulate real-world fraud scenarios. Unlike prior work focused on single-shot prompt misuse, ScamAgent maintains dialogue memory, adapts dynamically to simulated user responses, and employs deceptive persuasion strategies across conversational turns. We show that current LLM safety guardrails, including refusal mechanisms and content filters, are ineffective against such agent-based threats. Even models with strong prompt-level safeguards can be bypassed when prompts are decomposed, disguised, or delivered incrementally within an agent framework. We further demonstrate the transformation of scam scripts into lifelike voice calls using modern text-to-speech systems, completing a fully automated scam pipeline. Our findings highlight an urgent need for multi-turn safety auditing, agent-level control frameworks, and new methods to detect and disrupt conversational deception powered by generative AI."
    }
}