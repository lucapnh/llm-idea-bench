{
    "query": "consistency models zero-shot tasks",
    "result": {
        "1": "Consistency Models. Yang Song, Prafulla Dhariwal, Mark Chen, I. Sutskever. International Conference on Machine Learning, 2023.\nNumber of citations: 1127\nAbstract: Diffusion models have significantly advanced the fields of image, audio, and video generation, but they depend on an iterative sampling process that causes slow generation. To overcome this limitation, we propose consistency models, a new family of models that generate high quality samples by directly mapping noise to data. They support fast one-step generation by design, while still allowing multistep sampling to trade compute for sample quality. They also support zero-shot data editing, such as image inpainting, colorization, and super-resolution, without requiring explicit training on these tasks. Consistency models can be trained either by distilling pre-trained diffusion models, or as standalone generative models altogether. Through extensive experiments, we demonstrate that they outperform existing distillation techniques for diffusion models in one- and few-step sampling, achieving the new state-of-the-art FID of 3.55 on CIFAR-10 and 6.20 on ImageNet 64x64 for one-step generation. When trained in isolation, consistency models become a new family of generative models that can outperform existing one-step, non-adversarial generative models on standard benchmarks such as CIFAR-10, ImageNet 64x64 and LSUN 256x256.",
        "2": "Zero-Shot Image Restoration Using Denoising Diffusion Null-Space Model. Yinhuai Wang, Jiwen Yu, Jian Zhang. International Conference on Learning Representations, 2022.\nNumber of citations: 501\nAbstract: Most existing Image Restoration (IR) models are task-specific, which can not be generalized to different degradation operators. In this work, we propose the Denoising Diffusion Null-Space Model (DDNM), a novel zero-shot framework for arbitrary linear IR problems, including but not limited to image super-resolution, colorization, inpainting, compressed sensing, and deblurring. DDNM only needs a pre-trained off-the-shelf diffusion model as the generative prior, without any extra training or network modifications. By refining only the null-space contents during the reverse diffusion process, we can yield diverse results satisfying both data consistency and realness. We further propose an enhanced and robust version, dubbed DDNM+, to support noisy restoration and improve restoration quality for hard tasks. Our experiments on several IR tasks reveal that DDNM outperforms other state-of-the-art zero-shot IR methods. We also demonstrate that DDNM+ can solve complex real-world applications, e.g., old photo restoration.",
        "3": "Benchmarking Zero-Shot Recognition with Vision-Language Models: Challenges on Granularity and Specificity. Zhenlin Xu, Yi Zhu, Tiffany Deng, Abhay Mittal, Yanbei Chen, Manchen Wang, P. Favaro, Joseph Tighe, Davide Modolo. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2023.\nNumber of citations: 12\nAbstract: This paper presents novel benchmarks for evaluating vision-language models (VLMs) in zero-shot recognition, focusing on granularity and specificity. Although VLMs excel in tasks like image captioning, they face challenges in open-world settings. Our benchmarks test VLMs\u2019 consistency in understanding concepts across semantic granularity levels and their response to varying text specificity. Findings show that VLMs favor moderately fine-grained concepts and struggle with specificity, often misjudging texts that differ from their training data. Extensive evaluations reveal limitations in current VLMs, particularly in distinguishing between correct and subtly incorrect descriptions. While fine-tuning offers some improvements, it doesn\u2019t fully address these issues, highlighting the need for VLMs with enhanced generalization capabilities for real-world applications. This study provides insights into VLM limitations and suggests directions for developing more robust models.",
        "4": "Test-Time Low Rank Adaptation via Confidence Maximization for Zero-Shot Generalization of Vision-Language Models. Raza Imam, Hanan Gani, Muhammad Huzaifa, Karthik Nandakumar. IEEE Workshop/Winter Conference on Applications of Computer Vision, 2024.\nNumber of citations: 10\nAbstract: The conventional modus operandi for adapting pre-trained vision-language models (VLMs) during test-time involves tuning learnable prompts, i.e., test-time prompt tuning. This paper introduces Test-Time Low-rank adaptation (TTL) as an alternative to prompt tuning for zero-shot generalization of large-scale VLMs. Taking inspiration from recent advancements in efficiently fine-tuning large language models, TTL offers a test-time parameter-efficient adaptation approach that updates the attention weights of the transformer encoder by maximizing prediction confidence. The self-supervised confidence maximization objective is specified using a weighted entropy loss that enforces consistency among predictions of augmented samples. TTL introduces only a small amount of trainable parameters for low-rank adapters in the model space while keeping the prompts and backbone frozen. Extensive experiments on a variety of natural distribution and cross-domain tasks show that TTL can outperform other techniques for test-time optimization of VLMs in strict zero-shot settings. Specifically, TTL outperforms test-time prompt tuning baselines with a significant improvement on average. Our code is available at https://github.com/Razaimam45/TTLTest-Time-Low-Rank-Adaptation.",
        "5": "EVE: Efficient zero-shot text-based Video Editing with Depth Map Guidance and Temporal Consistency Constraints. Yutao Chen, Xingning Dong, Tian Gan, Chunluan Zhou, Ming Yang, Qingpei Guo. International Joint Conference on Artificial Intelligence, 2023.\nNumber of citations: 5\nAbstract: Motivated by the superior performance of image diffusion models, more and more researchers strive to extend these models to the text-based video editing task. Nevertheless, current video editing tasks mainly suffer from the dilemma between the high fine-tuning cost and the limited generation capacity. Compared with images, we conjecture that videos necessitate more constraints to preserve the temporal consistency during editing. Towards this end, we propose EVE, a robust and Efficient zero-shot Video Editing method. Under the guidance of depth maps and temporal consistency constraints, EVE derives satisfactory video editing results with an affordable computational and time cost. Moreover, recognizing the absence of a publicly available video editing dataset for fair comparisons, we construct a new benchmark named ZVE-50 dataset. Through comprehensive experimentation, we validate that EVE achieves a satisfactory trade-off between performance and efficiency. Codebase, datasets, and video editing demos are available at https://github.com/alipay/Ant-Multi-Modal-Framework/blob/main/prj/EVE.",
        "6": "Evaluating Zero-Shot Multilingual Aspect-Based Sentiment Analysis with Large Language Models. Chengyan Wu, Bolei Ma, Zheyu Zhang, Ningyuan Deng, Yanqing He, Yun Xue. International Journal of Machine Learning and Cybernetics, 2024.\nNumber of citations: 4\nAbstract: Aspect-based sentiment analysis (ABSA), a sequence labeling task, has attracted increasing attention in multilingual contexts. While previous research has focused largely on fine-tuning or training models specifically for ABSA, we evaluate large language models (LLMs) under zero-shot conditions to explore their potential to tackle this challenge with minimal task-specific adaptation. We conduct a comprehensive empirical evaluation of a series of LLMs on multilingual ABSA tasks, investigating various prompting strategies, including vanilla zero-shot, chain-of-thought (CoT), self-improvement, self-debate, and self-consistency, across nine different models. Results indicate that while LLMs show promise in handling multilingual ABSA, they generally fall short of fine-tuned, task-specific models. Notably, simpler zero-shot prompts often outperform more complex strategies, especially in high-resource languages like English. These findings underscore the need for further refinement of LLM-based approaches to effectively address ABSA task across diverse languages.",
        "7": "Zero-Shot Image Restoration Using Few-Step Guidance of Consistency Models (and Beyond). Tomer Garber, Tom Tirer. Computer Vision and Pattern Recognition, 2024.\nNumber of citations: 2\nAbstract: In recent years, it has become popular to tackle image restoration tasks with a single pretrained diffusion model (DM) and data-fidelity guidance, instead of training a dedicated deep neural network per task. However, such \"zero-shot\" restoration schemes currently require many Neural Function Evaluations (NFEs) for performing well, which may be attributed to the many NFEs needed in the original generative functionality of the DMs. Recently, faster variants of DMs have been explored for image generation. These include Consistency Models (CMs), which can generate samples via a couple of NFEs. However, existing works that use guided CMs for restoration still require tens of NFEs or fine-tuning of the model per task that leads to performance drop if the assumptions during the fine-tuning are not accurate. In this paper, we propose a zero-shot restoration scheme that uses CMs and operates well with as little as 4 NFEs. It is based on a wise combination of several ingredients: better initialization, back-projection guidance, and above all a novel noise injection mechanism. We demonstrate the advantages of our approach for image super-resolution, deblurring and inpainting. Interestingly, we show that the usefulness of our noise injection technique goes beyond CMs: it can also mitigate the performance degradation of existing guided DM methods when reducing their NFE count. The source code is available at https://github.com/tirer-lab/CM4IR",
        "8": "Consistency Framework For Zero-Shot Image Captioning. Yinfeng Wang, Tingyu Wang, Lei Zhang. 2024 4th International Conference on Neural Networks, Information and Communication (NNICE), 2024.\nNumber of citations: 1\nAbstract: Recently, Large-scale pre-trained Visual Language Models (VLMs) have shown great zero-shot capability in various downstream tasks. Nonetheless, they are not capable of generating caption given an image. To explore the adaptation to zero-shot image-captioning, recent works follow the paradigm that utilize a pre-trained Large Language Models (LLMs) as the language decoder, then using the text embedding encoded by VLMs as a substitute for image. However, when predicting on images, model usually fails to correctly comprehend visual content, leading to the wrong prediction of objects that do not actually exist in the images, i.e., object hallucination. The cause of this phenomenon is that, during the training process, the model fails to adequately receive and integrate information from the image modality. This results in the crucial image-related information being missing during training. To address the above issue, in this work, we propose our Visual Augment Decoding Network (VAD) for zero-shot image captioning. We initially use retrieval model to search the relevant image from unpaired dataset, thus visual information is introduced during the training phase. Additionally, we employ an entity-aware textual prompt, guiding the LLMs to better comprehend image content. Experimental results demonstrate competitive performance across both in-domain and cross-domain captioning in multiple datasets, validating its generalization capabilities and superiority.",
        "9": "Zero-shot Factual Consistency Evaluation Across Domains. Raunak Agarwal. arXiv.org, 2024.\nNumber of citations: 0\nAbstract: This work addresses the challenge of factual consistency in text generation systems. We unify the tasks of Natural Language Inference, Summarization Evaluation, Factuality Verification and Factual Consistency Evaluation to train models capable of evaluating the factual consistency of source-target pairs across diverse domains. We rigorously evaluate these against eight baselines on a comprehensive benchmark suite comprising 22 datasets that span various tasks, domains, and document lengths. Results demonstrate that our method achieves state-of-the-art performance on this heterogeneous benchmark while addressing efficiency concerns and attaining cross-domain generalization.",
        "10": "Beyond Training Data: Zero-Shot Mathematical Problem Solving with Large Language Models. Sagar Hitendra Parmar, Megha Goriya. 2024 IEEE Conference on Engineering Informatics (ICEI), 2024.\nNumber of citations: 0\nAbstract: This study investigates the potential of Large Language Models (LLMs) for mathematical problem solving using a zero-shot multi-path self-consistency technique combined with programmatic solution verification. We create Python code that is evaluated by a majority vote by providing multiple reasoning pathways using differences in temperature and top-k values, as well as improving inquiry comprehension through phrasing. When applied to the GSM8K and MATH datasets, this approach achieved $\\mathbf{45 \\%}$ accuracy on GSM8K and $\\mathbf{14.57 \\%}$ on MATH with the Granite 8B code model. These findings show that even in the absence of task-specific training data, LLMs can greatly enhance their performance by breaking down complicated tasks and employing a consistent multipath technique."
    }
}