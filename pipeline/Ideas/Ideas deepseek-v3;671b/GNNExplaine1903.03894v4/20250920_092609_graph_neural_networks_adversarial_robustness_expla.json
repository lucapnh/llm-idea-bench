{
    "query": "graph neural networks adversarial robustness explanations",
    "result": {
        "1": "Certified Robustness of Graph Neural Networks against Adversarial Structural Perturbation. Binghui Wang, Jinyuan Jia, Xiaoyu Cao, N. Gong. Knowledge Discovery and Data Mining, 2020.\nNumber of citations: 82\nAbstract: Graph neural networks (GNNs) have recently gained much attention for node and graph classification tasks on graph-structured data. However, multiple recent works showed that an attacker can easily make GNNs predict incorrectly via perturbing the graph structure, i.e., adding or deleting edges in the graph. We aim to defend against such attacks via developing certifiably robust GNNs. Specifically, we prove the first certified robustness guarantee of any GNN for both node and graph classifications against structural perturbation. Moreover, we show that our certified robustness guarantee is tight. Our results are based on a recently proposed technique called randomized smoothing, which we extend to graph data. We also empirically evaluate our method for both node and graph classifications on multiple GNNs and multiple benchmark datasets. For instance, on the Cora dataset, Graph Convolutional Network with our randomized smoothing can achieve a certified accuracy of 0.49 when the attacker can arbitrarily add/delete at most 15 edges in the graph.",
        "2": "Explainability and Adversarial Attacks for Trustworthy QA. Rachneet Sachdeva, Haritz Puerto, Tim Baumg\u00e4rtner, Sewin Tariverdian, Hao Zhang, Kexin Wang, H. Saad, Leonardo F. R. Ribeiro, Iryna Gurevych. AACL, 2022.\nNumber of citations: 2\nAbstract: Question Answering (QA) systems are increasingly deployed in applications where they support real-world decisions. However, state-of-the-art models rely on deep neural networks, which are difficult to interpret by humans. Inherently interpretable models or post hoc explainability methods can help users to comprehend how a model arrives at its prediction and, if successful, increase their trust in the system. Furthermore, researchers can leverage these insights to develop new methods that are more accurate and less biased. In this paper, we introduce SQuARE v2, the new version of SQuARE, to provide an explainability infrastructure for comparing models based on methods such as saliency maps and graph-based explanations. While saliency maps are useful to inspect the importance of each input token for the model\u2019s prediction, graph-based explanations from external Knowledge Graphs enable the users to verify the reasoning behind the model prediction. In addition, we provide multiple adversarial attacks to compare the robustness of QA models. With these explainability methods and adversarial attacks, we aim to ease the research on trustworthy QA models. SQuARE is available on https://square.ukp-lab.de.",
        "3": "Collective Robustness Certificates: Exploiting Interdependence in Graph Neural Networks. Jan Schuchardt, Aleksandar Bojchevski, Johannes Gasteiger, Stephan G\u00fcnnemann. arXiv.org, 2023.\nNumber of citations: 27\nAbstract: In tasks like node classification, image segmentation, and named-entity recognition we have a classifier that simultaneously outputs multiple predictions (a vector of labels) based on a single input, i.e. a single graph, image, or document respectively. Existing adversarial robustness certificates consider each prediction independently and are thus overly pessimistic for such tasks. They implicitly assume that an adversary can use different perturbed inputs to attack different predictions, ignoring the fact that we have a single shared input. We propose the first collective robustness certificate which computes the number of predictions that are simultaneously guaranteed to remain stable under perturbation, i.e. cannot be attacked. We focus on Graph Neural Networks and leverage their locality property - perturbations only affect the predictions in a close neighborhood - to fuse multiple single-node certificates into a drastically stronger collective certificate. For example, on the Citeseer dataset our collective certificate for node classification increases the average number of certifiable feature perturbations from $7$ to $351$.",
        "4": "Adversarial Robustness in Graph Neural Networks: A Hamiltonian Approach. Kai Zhao, Qiyu Kang, Yang Song, Rui She, Sijie Wang, Wee Peng Tay. Neural Information Processing Systems, 2023.\nNumber of citations: 26\nAbstract: Graph neural networks (GNNs) are vulnerable to adversarial perturbations, including those that affect both node features and graph topology. This paper investigates GNNs derived from diverse neural flows, concentrating on their connection to various stability notions such as BIBO stability, Lyapunov stability, structural stability, and conservative stability. We argue that Lyapunov stability, despite its common use, does not necessarily ensure adversarial robustness. Inspired by physics principles, we advocate for the use of conservative Hamiltonian neural flows to construct GNNs that are robust to adversarial attacks. The adversarial robustness of different neural flow GNNs is empirically compared on several benchmark datasets under a variety of adversarial attacks. Extensive numerical experiments demonstrate that GNNs leveraging conservative Hamiltonian flows with Lyapunov stability substantially improve robustness against adversarial perturbations. The implementation code of experiments is available at https://github.com/zknus/NeurIPS-2023-HANG-Robustness.",
        "5": "Evaluating Post-hoc Explanations for Graph Neural Networks via Robustness Analysis. Junfeng Fang, Wei Liu, Yuan Gao, Zemin Liu, An Zhang, Xiang Wang, Xiangnan He. Neural Information Processing Systems, 2023.\nNumber of citations: 25\nAbstract: None",
        "6": "Can Large Language Models Improve the Adversarial Robustness of Graph Neural Networks?. Zhongjian Zhang, Xiao Wang, Huichi Zhou, Yue Yu, Mengmei Zhang, Cheng Yang, Chuan Shi. Knowledge Discovery and Data Mining, 2024.\nNumber of citations: 12\nAbstract: Graph neural networks (GNNs) are vulnerable to adversarial attacks, especially for topology perturbations, and many methods that improve the robustness of GNNs have received considerable attention. Recently, we have witnessed the significant success of large language models (LLMs), leading many to explore the great potential of LLMs on GNNs. However, they mainly focus on improving the performance of GNNs by utilizing LLMs to enhance the node features. Therefore, we ask: Will the robustness of GNNs also be enhanced with the powerful understanding and inference capabilities of LLMs? By presenting the empirical results, we find that despite that LLMs can improve the robustness of GNNs, there is still an average decrease of 23.1% in accuracy, implying that the GNNs remain extremely vulnerable against topology attacks. Therefore, another question is how to extend the capabilities of LLMs on graph adversarial robustness. In this paper, we propose an LLM-based robust graph structure inference framework, LLM4RGNN. The source code in https://github.com/zhongjian-zhang/LLM4RGNN, which distills the inference capabilities of GPT-4 into a local LLM for identifying malicious edges and an LM-based edge predictor for finding missing important edges, so as to recover a robust graph structure. Extensive experiments demonstrate that LLM4RGNN consistently improves the robustness across various GNNs. Even in some cases where the perturbation ratio increases to 40%, the accuracy of GNNs is still better than that on the clean graph.",
        "7": "UKP-SQuARE v",
        "8": "Improve Robustness of Graph Neural Networks: Multi-hop Neighbors Meet Homophily-based Truncation Defense. Yujie Gu, Huiyan Sun, Yansong Wang, Haobo Shi. IEEE International Joint Conference on Neural Network, 2023.\nNumber of citations: 2\nAbstract: Graph Neural Networks (GNNs), as promising deep learning approaches, have been applied in various areas. However, it is also known that they are vulnerable to adversarial attacks, which raises many concerns in real application. Regarding Graph Structure Attacks (GSAs), though existing homophily-based truncation defenses have showed strong defense capacity, they suffer the problem of losing much effective neighborhood information in the process of removing adversarial edges, causing a limited performance on both clean and attacked graphs. In this paper, we consider the question: Can we capture more effective neighborhood information by utilizing the higher-order network to help improve the performance of the homophily-based truncation defense? To answer it, we first explore the impacts of different GSAs on the 1-hop and 2-hop networks. We theoretically and empirically find that the 2-hop network also has a strong information retention capacity like the 1-hop network after many GSAs. Motivated by this, we combine the 2-hop network with the homophily-based truncation defense, constructing a stronger defender, MHR-GCN. It integrates effective neighborhood information from both 1-hop and 2-hop networks. Extensive experiments demonstrate that MHR-GCN significantly improves the performance of the truncation defense and outperforms the state-of-the-arts under various GSA settings, especially when the graph is heavily perturbed.",
        "9": "Enhancing Robustness of Graph Neural Networks on Social Media with Explainable Inverse Reinforcement Learning. Yuefei Lyu, Chaozhuo Li, Sihong Xie, Xi Zhang. Neural Information Processing Systems, 2024.\nNumber of citations: 1\nAbstract: None",
        "10": "Explaining Vision GNNs: A Semantic and Visual Analysis of Graph-based Image Classification. Nikolaos Chaidos, Angeliki Dimitriou, Nikolaos Spanos, Athanasios Voulodimos, G. Stamou. arXiv.org, 2025.\nNumber of citations: 1\nAbstract: Graph Neural Networks (GNNs) have emerged as an efficient alternative to convolutional approaches for vision tasks such as image classification, leveraging patch-based representations instead of raw pixels. These methods construct graphs where image patches serve as nodes, and edges are established based on patch similarity or classification relevance. Despite their efficiency, the explainability of GNN-based vision models remains underexplored, even though graphs are naturally interpretable. In this work, we analyze the semantic consistency of the graphs formed at different layers of GNN-based image classifiers, focusing on how well they preserve object structures and meaningful relationships. A comprehensive analysis is presented by quantifying the extent to which inter-layer graph connections reflect semantic similarity and spatial coherence. Explanations from standard and adversarial settings are also compared to assess whether they reflect the classifiers' robustness. Additionally, we visualize the flow of information across layers through heatmap-based visualization techniques, thereby highlighting the models' explainability. Our findings demonstrate that the decision-making processes of these models can be effectively explained, while also revealing that their reasoning does not necessarily align with human perception, especially in deeper layers."
    }
}