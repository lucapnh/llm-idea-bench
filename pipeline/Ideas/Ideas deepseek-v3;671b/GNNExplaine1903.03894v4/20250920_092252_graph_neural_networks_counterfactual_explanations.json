{
    "query": "graph neural networks counterfactual explanations",
    "result": {
        "1": "CF-GNNExplainer: Counterfactual Explanations for Graph Neural Networks. Ana Lucic, Maartje ter Hoeve, Gabriele Tolomei, M. de Rijke, F. Silvestri. International Conference on Artificial Intelligence and Statistics, 2021.\nNumber of citations: 154\nAbstract: Given the increasing promise of graph neural networks (GNNs) in real-world applications, several methods have been developed for explaining their predictions. Existing methods for interpreting predictions from GNNs have primarily focused on generating subgraphs that are especially relevant for a particular prediction. However, such methods are not counterfactual (CF) in nature: given a prediction, we want to understand how the prediction can be changed in order to achieve an alternative outcome. In this work, we propose a method for generating CF explanations for GNNs: the minimal perturbation to the input (graph) data such that the prediction changes. Using only edge deletions, we find that our method, CF-GNNExplainer, can generate CF explanations for the majority of instances across three widely used datasets for GNN explanations, while removing less than 3 edges on average, with at least 94\\% accuracy. This indicates that CF-GNNExplainer primarily removes edges that are crucial for the original predictions, resulting in minimal CF explanations.",
        "2": "Robust Counterfactual Explanations on Graph Neural Networks. Mohit Bajaj, Lingyang Chu, Zihui Xue, J. Pei, Lanjun Wang, P. C. Lam, Yong Zhang. Neural Information Processing Systems, 2021.\nNumber of citations: 105\nAbstract: Massive deployment of Graph Neural Networks (GNNs) in high-stake applications generates a strong demand for explanations that are robust to noise and align well with human intuition. Most existing methods generate explanations by identifying a subgraph of an input graph that has a strong correlation with the prediction. These explanations are not robust to noise because independently optimizing the correlation for a single input can easily overfit noise. Moreover, they do not align well with human intuition because removing an identified subgraph from an input graph does not necessarily change the prediction result. In this paper, we propose a novel method to generate robust counterfactual explanations on GNNs by explicitly modelling the common decision logic of GNNs on similar input graphs. Our explanations are naturally robust to noise because they are produced from the common decision boundaries of a GNN that govern the predictions of many similar input graphs. The explanations also align well with human intuition because removing the set of edges identified by an explanation from the input graph changes the prediction significantly. Exhaustive experiments on many public datasets demonstrate the superior performance of our method.",
        "3": "Graph Neural Networks for Vulnerability Detection: A Counterfactual Explanation. Zhaoyang Chu, Yao Wan, Qian Li, Yang Wu, Hongyu Zhang, Yulei Sui, Guandong Xu, Hai Jin. International Symposium on Software Testing and Analysis, 2024.\nNumber of citations: 19\nAbstract: Vulnerability detection is crucial for ensuring the security and reliability of software systems. Recently, Graph Neural Networks (GNNs) have emerged as a prominent code embedding approach for vulnerability detection, owing to their ability to capture the underlying semantic structure of source code. However, GNNs face significant challenges in explainability due to their inherently black-box nature. To this end, several factual reasoning-based explainers have been proposed. These explainers provide explanations for the predictions made by GNNs by analyzing the key features that contribute to the outcomes. We argue that these factual reasoning-based explanations cannot answer critical what-if questions: \"What would happen to the GNN's decision if we were to alter the code graph into alternative structures?\" Inspired by advancements of counterfactual reasoning in artificial intelligence, we propose CFExplainer, a novel counterfactual explainer for GNN-based vulnerability detection. Unlike factual reasoning-based explainers, CFExplainer seeks the minimal perturbation to the input code graph that leads to a change in the prediction, thereby addressing the what-if questions for vulnerability detection. We term this perturbation a counterfactual explanation, which can pinpoint the root causes of the detected vulnerability and furnish valuable insights for developers to undertake appropriate actions for fixing the vulnerability. Extensive experiments on four GNN-based vulnerability detection models demonstrate the effectiveness of CFExplainer over existing state-of-the-art factual reasoning-based explainers.",
        "4": "Game-theoretic Counterfactual Explanation for Graph Neural Networks. Chirag Chhablani, Sarthak Jain, Akshay Channesh, Ian A. Kash, Sourav Medya. The Web Conference, 2024.\nNumber of citations: 8\nAbstract: Graph Neural Networks (GNNs) have been a powerful tool for node classification tasks in complex networks. However, their decision-making processes remain a black-box to users, making it challenging to understand the reasoning behind their predictions. Counterfactual explanations (CFE) have shown promise in enhancing the interpretability of machine learning models. Prior approaches to compute CFE for GNNS often are learning-based approaches that require training additional graphs. In this paper, we propose a semivalue-based, non-learning approach to generate CFE for node classification tasks, eliminating the need for any additional training. Our results reveals that computing Banzhaf values requires lower sample complexity in identifying the counterfactual explanations compared to other popular methods such as computing Shapley values. Our empirical evidence indicates computing Banzhaf values can achieve up to a fourfold speed up compared to Shapley values. We also design a thresholding method for computing Banzhaf values and show theoretical and empirical results on its robustness in noisy environments, making it superior to Shapley values. Furthermore, the thresholded Banzhaf values are shown to enhance efficiency without compromising the quality (i.e., fidelity) in the explanations in three popular graph datasets.",
        "5": "Generating Robust Counterfactual Witnesses for Graph Neural Networks. Dazhuo Qiu, Mengying Wang, Arijit Khan, Yinghui Wu. IEEE International Conference on Data Engineering, 2024.\nNumber of citations: 3\nAbstract: This paper introduces a new class of explanation structures, called robust counterfactual witnesses (RCWs), to provide robust, both counterfactual and factual explanations for graph neural networks. Given a graph neural network $\\mathcal{M}$, a robust counterfactual witness refers to the fraction of a graph $G$ that are counterfactual and factual explanation of the results of $\\mathcal{M}$ over $G$, but also remains so for any \u201cdisturbed\u201d $G$ by flipping up to $k$ of its node pairs. We establish the hardness results, from tractable results to co-NP-hardness, for verifying and generating robust counterfactual witnesses. We study such structures for GNN-based node classification, and present efficient algorithms to verify and generate RCWs. We also provide a parallel algorithm to verify and generate RCWs for large graphs with scalability guarantees. We experimentally verify our explanation generation process for benchmark datasets, and showcase their applications.",
        "6": "InduCE: Inductive Counterfactual Explanations for Graph Neural Networks. S. Verma, Burouj Armgaan, Sourav Medya, Sayan Ranu. Trans. Mach. Learn. Res., 2024.\nNumber of citations: 2\nAbstract: None",
        "7": "User-friendly, Interactive, and Configurable Explanations for Graph Neural Networks with Graph Views. Tingyang Chen, Dazhuo Qiu, Yinghui Wu, Arijit Khan, Xiangyu Ke, Yunjun Gao. SIGMOD Conference Companion, 2024.\nNumber of citations: 2\nAbstract: Explaining the behavior of graph neural networks (GNNs) has become critical due to their \"black-box'' nature, especially in the context of analytical tasks such as graph classification. Current approaches are limited to providing explanations for individual instances or specific class labels and may return large explanation structures that are hard to access, nor directly queryable. In this paper, we present GVEX [1] (Graph Views for GNN EXplanation) -- our system developed to offer user-friendly, interactive, and configurable explanations for GNNs based on graph views. GVEX provides a configuration component to enable users to easily select a desired number of important nodes from different classes, thereby generating explanations tailored to multiple classes of interest. Furthermore, GVEX generates high-quality explanation subgraphs by identifying important nodes exploiting factual and counterfactual properties and by computing their aggregated influence on the remaining nodes following the GNN message passing paradigm. Lastly, GVEX performs a summarize step on top of lower-tier explanation structures to generate higher-tier graph patterns that offer direct access for users with (domain-aware) queries. Our demonstration will highlight (1) a novel two-tier explanation structure called explanation views, consisting of graph patterns and a set of explanation subgraphs, which provide high-quality explanations for GNNs; (2) the system's intuitive GUI facilitates user interaction to configure personalized settings, e.g., classes of interest and explanation size, and compare with other explanation algorithms; (3) GVEX generates queryable explanations, making it easy for human experts to access and inspect with domain knowledge. Our demonstration video is at: https://youtu.be/q9d7ldulIuQ.",
        "8": "GNNAnatomy: Systematic Generation and Evaluation of Multi-Level Explanations for Graph Neural Networks. Hsiao-Ying Lu, Yiran Li, Ujwal Pratap Krishna Kaluvakolanu Thyagarajan, Kwan-Liu Ma. arXiv.org, 2024.\nNumber of citations: 1\nAbstract: None",
        "9": "Interpretable Pulmonary Disease Diagnosis with Graph Neural Network and Counterfactual Explanations. Jiahong Li, Yiyuan Chen, Yichi Wang, Yiqiang Ye, Min Sun, Haopan Ren, Weibin Cheng, Haodi Zhang. 2023 2nd International Conference on Sensing, Measurement, Communication and Internet of Things Technologies (SMC-IoT), 2023.\nNumber of citations: 0\nAbstract: With recent fast development of artificial intelligence and related techniques, computer-aided diagnosis is increasingly emerging. During the process of automated diagnosis, the importance of explainable decision making has grown significantly. Traditional black-box models have often hindered the practical implementation of automated diagnosis algorithms. To address these challenges, we propose a novel framework that seamlessly integrates prototype learning with Graph Neural Networks (GNN) while also providing post-hoc explanations. On one hand, our model achieves intrinsic interpretability by reasoning based on the similarity calculations with prototypes for each disease. On the other hand, it employs counterfactual reasoning on graphs to pinpoint the most significant features for post-hoc explanations, supporting the diagnosis process. Building on these strengths, we integrate vision-language models to effectively harness multimodal patient information, thereby capturing a more detailed understanding of their medical condition. Our experiments on real Chinese EMRs of Pulmonary diseases demonstrate that our method not only delivers precise diagnoses but also accurately identifies medical findings substantiating the diagnoses.",
        "10": "Empowering Counterfactual Reasoning over Graph Neural Networks through Inductivity. S. Verma, Burouj Armgaan, Sourav Medya, Sayan Ranu. arXiv.org, 2023.\nNumber of citations: 0\nAbstract: Graph neural networks (GNNs) have various practical applications, such as drug discovery, recommendation engines, and chip design. However, GNNs lack transparency as they cannot provide understandable explanations for their predictions. To address this issue, counterfactual reasoning is used. The main goal is to make minimal changes to the input graph of a GNN in order to alter its prediction. While several algorithms have been proposed for counterfactual explanations of GNNs, most of them have two main drawbacks. Firstly, they only consider edge deletions as perturbations. Secondly, the counterfactual explanation models are transductive, meaning they do not generalize to unseen data. In this study, we introduce an inductive algorithm called INDUCE, which overcomes these limitations. By conducting extensive experiments on several datasets, we demonstrate that incorporating edge additions leads to better counterfactual results compared to the existing methods. Moreover, the inductive modeling approach allows INDUCE to directly predict counterfactual perturbations without requiring instance-specific training. This results in significant computational speed improvements compared to baseline methods and enables scalable counterfactual analysis for GNNs."
    }
}