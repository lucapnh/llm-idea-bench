{
    "query": "graph neural networks explainability mutual information",
    "result": {
        "1": "A Comprehensive Survey on Trustworthy Graph Neural Networks: Privacy, Robustness, Fairness, and Explainability. Enyan Dai, Tianxiang Zhao, Huaisheng Zhu, Jun Xu, Zhimeng Guo, Hui Liu, Jiliang Tang, Suhang Wang. Machine Intelligence Research, 2022.\nNumber of citations: 156\nAbstract: Graph neural networks (GNNs) have made rapid developments in the recent years. Due to their great ability in modeling graph-structured data, GNNs are vastly used in various applications, including high-stakes scenarios such as financial analysis, traffic predictions, and drug discovery. Despite their great potential in benefiting humans in the real world, recent study shows that GNNs can leak private information, are vulnerable to adversarial attacks, can inherit and magnify societal bias from training data and lack interpretability, which have risk of causing unintentional harm to the users and society. For example, existing works demonstrate that attackers can fool the GNNs to give the outcome they desire with unnoticeable perturbation on training graph. GNNs trained on social networks may embed the discrimination in their decision process, strengthening the undesirable societal bias. Consequently, trust-worthy GNNs in various aspects are emerging to prevent the harm from GNN models and increase the users\u2019 trust in GNNs. In this paper, we give a comprehensive survey of GNNs in the computational aspects of privacy, robustness, fairness, and explainability. For each aspect, we give the taxonomy of the related methods and formulate the general frameworks for the multiple categories of trustworthy GNNs. We also discuss the future research directions of each aspect and connections between these aspects to help achieve trustworthiness.",
        "2": "Graph neural networks with deep mutual learning for designing multi-modal recommendation systems. Jianing Li, Chao-Peng Yang, Guanhua Ye, Quoc Viet Hung Nguyen. Information Sciences, 2024.\nNumber of citations: 31\nAbstract: None",
        "3": "TempME: Towards the Explainability of Temporal Graph Neural Networks via Motif Discovery. Jialin Chen, Rex Ying. Neural Information Processing Systems, 2023.\nNumber of citations: 25\nAbstract: Temporal graphs are widely used to model dynamic systems with time-varying interactions. In real-world scenarios, the underlying mechanisms of generating future interactions in dynamic systems are typically governed by a set of recurring substructures within the graph, known as temporal motifs. Despite the success and prevalence of current temporal graph neural networks (TGNN), it remains uncertain which temporal motifs are recognized as the significant indications that trigger a certain prediction from the model, which is a critical challenge for advancing the explainability and trustworthiness of current TGNNs. To address this challenge, we propose a novel approach, called Temporal Motifs Explainer (TempME), which uncovers the most pivotal temporal motifs guiding the prediction of TGNNs. Derived from the information bottleneck principle, TempME extracts the most interaction-related motifs while minimizing the amount of contained information to preserve the sparsity and succinctness of the explanation. Events in the explanations generated by TempME are verified to be more spatiotemporally correlated than those of existing approaches, providing more understandable insights. Extensive experiments validate the superiority of TempME, with up to 8.21% increase in terms of explanation accuracy across six real-world datasets and up to 22.96% increase in boosting the prediction Average Precision of current TGNNs.",
        "4": "Mutual Information Maximization in Graph Neural Networks. Xinhan Di, Pengqian Yu, Rui Bu, Mingchao Sun. IEEE International Joint Conference on Neural Network, 2019.\nNumber of citations: 21\nAbstract: A variety of graph neural networks (GNNs) frameworks for representation learning on graphs have been recently developed. These frameworks rely on aggregation and ITERATION scheme to learn the representation of nodes. However, information between nodes is inevitably lost in the scheme during learning. In order to reduce the loss, we extend the GNNs frameworks by exploring the aggregation and iteration scheme in the methodology of mutual information. We propose a new approach of enlarging the normal neighborhood in the aggregation of GNNs, which aims at maximizing mutual information. Based on a series of experiments conducted on several benchmark datasets, we show that the proposed approach improves the state-of-the-art performance for four types of graph tasks, including supervised and semi-supervised graph classification, graph link prediction and graph edge generation and classification.",
        "5": "Hi-GNN: hierarchical interactive graph neural networks for auxiliary information-enhanced recommendation. Xiongtao Zhang, Mingxin Gan. Knowledge and Information Systems, 2023.\nNumber of citations: 7\nAbstract: None",
        "6": "Heterogeneous Graph Neural Networks with Post-hoc Explanations for Multi-modal and Explainable Land Use Inference. Xuehao Zhai, Junqi Jiang, Adam Dejl, Antonio Rago, Fangce Guo, Francesca Toni, Aruna Sivakumar. Information Fusion, 2024.\nNumber of citations: 5\nAbstract: Urban land use inference is a critically important task that aids in city planning and policy-making. Recently, the increased use of sensor and location technologies has facilitated the collection of multi-modal mobility data, offering valuable insights into daily activity patterns. Many studies have adopted advanced data-driven techniques to explore the potential of these multi-modal mobility data in land use inference. However, existing studies often process samples independently, ignoring the spatial correlations among neighbouring objects and heterogeneity among different services. Furthermore, the inherently low interpretability of complex deep learning methods poses a significant barrier in urban planning, where transparency and extrapolability are crucial for making long-term policy decisions. To overcome these challenges, we introduce an explainable framework for inferring land use that synergises heterogeneous graph neural networks (HGNs) with Explainable AI techniques, enhancing both accuracy and explainability. The empirical experiments demonstrate that the proposed HGNs significantly outperform baseline graph neural networks for all six land-use indicators, especially in terms of 'office' and 'sustenance'. As explanations, we consider feature attribution and counterfactual explanations. The analysis of feature attribution explanations shows that the symmetrical nature of the `residence' and 'work' categories predicted by the framework aligns well with the commuter's 'work' and 'recreation' activities in London. The analysis of the counterfactual explanations reveals that variations in node features and types are primarily responsible for the differences observed between the predicted land use distribution and the ideal mixed state. These analyses demonstrate that the proposed HGNs can suitably support urban stakeholders in their urban planning and policy-making.",
        "7": "Pointwise Sliced Mutual Information for Neural Network Explainability. Shelvia Wongso, Rohan Ghosh, M. Motani. International Symposium on Information Theory, 2023.\nNumber of citations: 4\nAbstract: When deploying deep learning models such as convolutional neural networks (CNNs) in safety-critical domains, it is important to understand the predictions made by these black-box models. While many different approaches have been taken to improve the interpretability of deep models, most methods lack theoretical properties, making it hard to justify the correctness of the explanations provided. In this paper, we take an information-theoretic approach to understand why CNNs make their predictions. Building upon sliced mutual information, we propose pointwise sliced mutual information (PSI) as a tool for measuring the amount of useful information that a feature has about the label for a single instance. We theoretically justify the use of PSI for explaining predictions made by CNNs through the connection with margin. We show that PSI works as an explainability tool in two ways: (i) Fiber-wise PSI constructs a saliency map that highlights regions in the image which are important in predicting the labels; (ii) Sample-wise PSI provides confidence scores for predictions on the labels.",
        "8": "FACExplainer: Generating Model-faithful Explanations for Graph Neural Networks Guided by Spatial Information. Hua Yang, C. L. P. Chen, Bianna Chen, Tong Zhang. IEEE International Conference on Bioinformatics and Biomedicine, 2023.\nNumber of citations: 3\nAbstract: Graph neural networks (GNNs) have been widely applied in various decision-crucial fields, where accurate predictions with high interpretability are desired. Thus, numerous post-hoc explainers for GNNs have been proposed. However, some prioritize human-intelligible explanations through graph rules, such as the connection rule, which undermines the explanation\u2019s faithfulness to the model. This paper proposes an innovative method, FACExplainer, that re-examines the role of spatial information within GNNs for generating model-faithful explanations. FACExplainer employs activation maps from the last graph convolution to narrow down a compact search space. Our approach further identifies the subgraph that maximizes mutual information as the explanation, eliminating the need for domain-specific knowledge about the downstream task. Empirical analysis of FACExplainer on seven benchmark datasets with three classical GNNs reveals significantly improved explanation quality while consuming less time when compared to leading explainers. The source code of FACExplainer is freely available at https://github.com/HuaYangttt/Facexplainer/.",
        "9": "Mutual information estimation for graph convolutional neural networks. Marius Cervera Landsverk, S. Riemer-S\u00f8rensen. NLDL, 2022.\nNumber of citations: 2\nAbstract: Measuring model performance is a key issue for deep learning practitioners. However, we often lack the ability to explain why a specific architecture attains superior predictive accuracy for a given data set. Often, validation accuracy is used as a performance heuristic quantifying how well a network generalize to unseen data. Mutual information can be used as a measure of the quality of internal representations in deep learning models, and the information plane provide insights into whether the model exploits the available information in data. \nThe information plane has previously been explored for fully connected neural networks and convolutional architectures. We present an architecture-agnostic method for tracking a network's internal representations during training, which are then used to create the mutual information plane. The method is exemplified for a graph convolutional neural network fitted on the Cora citation data. We compare how the inductive bias introduced in the graph convolutional architecture changes the mutual information plane relative to a fully connected neural network.",
        "10": "Explainable Link Prediction based on Mutual Information and Graph Neural Networks. Seolhee Jeon, Kwang Hee Lee, Myoung-Ho Kim. KIISE Transactions on Computing Practices, 2021.\nNumber of citations: 0\nAbstract: None"
    }
}