{
    "query": "Leveraging Large Language Models for High-Performance Computing Problem Solving and Automation",
    "result": {
        "1": "Greening Large Language Models of Code. Jieke Shi, Zhou Yang, Hong Jin Kang, Bowen Xu, Junda He, D. Lo. 2024 IEEE/ACM 46th International Conference on Software Engineering: Software Engineering in Society (ICSE-SEIS), 2023.\nNumber of citations: 29\nAbstract: Large language models of code have shown remarkable effectiveness across various software engineering tasks. Despite the availability of many cloud services built upon these powerful models, there remain several scenarios where developers cannot take full advantage of them, stemming from factors such as restricted or unreliable internet access, institutional privacy policies that prohibit external transmission of code to third-party vendors, and more. Therefore, developing a compact, efficient, and yet energy-saving model for deployment on developers\u2019 devices becomes essential.To this aim, we propose Avatar, a novel approach that crafts a deployable model from a large language model of code by optimizing it in terms of model size, inference latency, energy consumption, and carbon footprint while maintaining a comparable level of effectiveness (e.g., prediction accuracy on downstream tasks). The key idea of Avatar is to formulate the optimization of language models as a multi-objective configuration tuning problem and solve it with the help of a Satisfiability Modulo Theories (SMT) solver and a tailored optimization algorithm. The SMT solver is used to form an appropriate configuration space, while the optimization algorithm identifies the Pareto-optimal set of configurations for training the optimized models using knowledge distillation. We evaluate Avatar with two popular language models of code, i.e., CodeBERT and GraphCodeBERT, on two popular tasks, i.e., vulnerability prediction and clone detection. We use Avatar to produce optimized models with a small size (3 MB), which is 160\u00d7 smaller than the original large models. On the two tasks, the optimized models significantly reduce the energy consumption (up to 184\u00d7 less), carbon footprint (up to 157\u00d7 less), and inference latency (up to 76\u00d7 faster), with only a negligible loss in effectiveness (1.67%).Lay AbstractLarge language models of code have proven to be highly effective for various software engineering tasks, such as spotting program defects and helping developers write code. While many cloud services built on these models (e.g., GitHub Copilot) are now accessible, several factors, such as unreliable internet access (e.g., over 20% of GitHub Copilot\u2019s issues are related to network connectivity [22]) and privacy concerns (e.g., Apple has banned the internal use of external AI tools to protect confidential data [53]), hinder developers from fully utilizing these services. Therefore, deploying language models of code on developers\u2019 devices like laptops appears promising. However, local deployment faces challenges: (1) Consumer-grade personal devices typically lack sufficient memory and the high-performance CPUs/GPUs required for efficient model execution; (2) Even if the hardware requirements are met, deploying the models on many devices can result in considerable energy consumption and carbon emissions, negatively impacting environmental sustainability.To address these challenges, we present Avatar, an innovative approach that optimizes large language models of code and enables their deployment on consumer-grade devices. Avatar can optimize two popular models from a large size of 481 MB to a compact size of 3 MB, resulting in significant reductions in inference time, energy consumption, and carbon emissions by hundreds of times. Our technique effectively lowers the entry barrier for leveraging large language models of code, making them available to ordinary developers without the need for high-performance computing equipment. Furthermore, it also contributes to a more sustainable and user-friendly software development environment.",
        "2": "DREAMS: Density Functional Theory Based Research Engine for Agentic Materials Simulation. Ziqi Wang, Hongshuo Huang, Hancheng Zhao, Changwen Xu, Shang Zhu, Jan Janssen, Venkatasubramanian Viswanathan. arXiv.org, 2025.\nNumber of citations: 4\nAbstract: Materials discovery relies on high-throughput, high-fidelity simulation techniques such as Density Functional Theory (DFT), which require years of training, extensive parameter fine-tuning and systematic error handling. To address these challenges, we introduce the DFT-based Research Engine for Agentic Materials Screening (DREAMS), a hierarchical, multi-agent framework for DFT simulation that combines a central Large Language Model (LLM) planner agent with domain-specific LLM agents for atomistic structure generation, systematic DFT convergence testing, High-Performance Computing (HPC) scheduling, and error handling. In addition, a shared canvas helps the LLM agents to structure their discussions, preserve context and prevent hallucination. We validate DREAMS capabilities on the Sol27LC lattice-constant benchmark, achieving average errors below 1\\% compared to the results of human DFT experts. Furthermore, we apply DREAMS to the long-standing CO/Pt(111) adsorption puzzle, demonstrating its long-term and complex problem-solving capabilities. The framework again reproduces expert-level literature adsorption-energy differences. Finally, DREAMS is employed to quantify functional-driven uncertainties with Bayesian ensemble sampling, confirming the Face Centered Cubic (FCC)-site preference at the Generalized Gradient Approximation (GGA) DFT level. In conclusion, DREAMS approaches L3-level automation - autonomous exploration of a defined design space - and significantly reduces the reliance on human expertise and intervention, offering a scalable path toward democratized, high-throughput, high-fidelity computational materials discovery.",
        "3": "DeepSeek vs. ChatGPT: A Comparative Study for Scientific Computing and Scientific Machine Learning Tasks. Qile Jiang, Zhiwei Gao, G. Karniadakis. arXiv.org, 2025.\nNumber of citations: 3\nAbstract: None",
        "4": "Analysis of High-Performance Parallel Computing using TOPSIS Method. . Computer Science, Engineering and Technology, 2024.\nNumber of citations: 2\nAbstract: High-performance parallel computing involves the simultaneous execution of multiple tasks or processes, orchestrated to achieve improved computational speed and efficiency. This approach leverages the power of parallelism, exploiting both multi-core CPUs and GPUs, distributed computing clusters, and specialized hardware accelerators. The fundamental idea is to divide a task into smaller sub-tasks that can be executed concurrently, thereby reducing processing time and enhancing overall performance. High-performance parallel computing is a transformative approach that enables us to tackle computationally intensive tasks efficiently. This abstract highlight its significance in contemporary computing and sets the stage for further exploration of the intricacies and innovations within this dynamic field. Researchers and practitioners continue to push the boundaries of what is achievable, making high-performance parallel computing a cornerstone of modern computational science and technology. High-performance parallel computing research is of paramount significance due to its transformative impact across diverse fields. It empowers scientists to tackle complex problems that were once computationally intractable, unlocking new frontiers in scientific discovery. It drives innovation in engineering and design, optimizing product development and manufacturing processes across industries. In healthcare, it accelerates genomics research and drug discovery, offering hope for improved medical treatments. Financial institutions rely on it for data analysis and risk assessment, shaping the global economy. Weather forecasting and environmental modelling are enhanced, aiding disaster preparedness and conservation efforts. In the digital age, parallel computing underpins artificial intelligence, enabling advancements in natural language processing and machine learning. Furthermore, it has vital applications in national security, space exploration, and materials science. In essence, high-performance parallel computing research serves as the backbone of technological progress, fostering innovation, efficiency, and problem-solving across a wide spectrum of disciplines, ultimately shaping the future of our world. TOPSIS, this method involves evaluating the geometric distance between each alternative solution and two reference solutions: the positive ideal solution and the negative ideal solution. The underlying principle of TOPSIS assumes that the criteria being assessed are of an ascending nature, where larger values represent better performance. To account for disparate dimensions or scales among the criteria, normalization is often employed within the TOPSIS framework. From the result Scatter- free imaging is got the first rank and object-scatter imaging is having the lowest rank.",
        "5": "Entropy-Gated Branching for Efficient Test-Time Reasoning. Xianzhi Li, Ethan Callanan, Xiaodan Zhu, Mathieu Sibue, Antony Papadimitriou, Mahmoud Mahfouz, Zhiqiang Ma, Xiaomo Liu. , 2025.\nNumber of citations: 2\nAbstract: Test-time compute methods like beam search can significantly improve the reasoning capabilities and problem-solving accuracy of large language models. However, these approaches require substantially increased computational resources, with most computation wasted on exploring low-diversity branches where the model already exhibits high confidence. We observe that a small subset of uncertain reasoning steps has a disproportionately large impact on final prediction accuracy, and branching at these points tends to yield higher-quality and more diverse candidate reasoning steps. Therefore, we introduce Entropy-Gated Branching: a novel inference technique that dynamically allocates computational resources by selectively expanding prediction sequences only at points of high uncertainty. Our method leverages entropy as a gating mechanism to identify when branching is most beneficial, coupled with an external feedback model to rank and prune candidate branches. Empirical results on mathematical and financial reasoning benchmarks show that this strategy improves accuracy by 22.6% over standard inference while operating 37% faster than conventional beam search with similar or higher performance. Our results show that dynamic resource allocation during inference can substantially improve both efficiency and effectiveness, offering a more scalable pathway to enhanced LLM reasoning capabilities.",
        "6": "FALL: Prior Failure Detection in Large Scale System Based on Language Model. Jaeyoon Jeong, Insung Baek, Byungwoo Bang, Junyeon Lee, Uiseok Song, Seoung Bum Kim. IEEE Transactions on Dependable and Secure Computing, 2025.\nNumber of citations: 1\nAbstract: As the scale of high-performance computing systems (HPC) continues to expand, the frequency of failures within the system also increases. The follow-up steps in the event of a failure are necessary because efficient prevention measures are absent. These actions, however, result in decreased operating efficiency as the system normalizes. To solve this problem, recent research has utilized deep learning to predict failures in advance by examining log messages. One common method uses log ID extracted from log messages as input data. However, these methods could lead to information loss in log messages and may not accurately capture log message properties. In this study, we propose the prior failure detection in large-scale systems using language models (FALL) that reflects the properties of log messages. The FALL uses log messages as text data and only uses normal data for training. The FALL makes use of sharpening, which concentrates on the minimal lexical change between normal and abnormal log messages. In addition, by leveraging the characteristics of log messages with limited vocabularies, the FALL utilizes only some tokens for anomaly detection. The experiments\u2019 results from log data from real industry-based HPC systems show that the FALL achieves superior performance in early failure detection.",
        "7": "Computing with a Chemical Reservoir. Connah G. M. Johnson, N. Agostini, William R. Cannon, Antonino Tumeo. International Conference on Rebooting Computing, 2024.\nNumber of citations: 1\nAbstract: Scientific computing, data analytics and artificial intelligence (in particular with the proliferation of large language models) are driving an explosive growth in computing needs. However, leading-edge high-performance computing systems composed of digital CMOS-based processing elements are reaching physical limits that do not allow any more significant gains in energy efficiency. As we progress towards post-exascale computing systems disruptive approaches to break this barrier in energy efficiency are required. Novel analog and hybrid digital-analog systems promise improvements in energy efficiency of orders of magnitude. Among the various solutions under exploration, biochemical computing has the potential to enable a new type of computing devices with immense computational power. These device can harness the efficiency of biological cells in solving optimization problems (chemical reactions naturally converge to optimal steady states) and are scalable by considering increasingly larger reaction systems or vessels, potentially meeting the high-performance requirements of scientific computing. However, several theoretical and practical limitations still exists going from how we formulate and map problems to chemical reaction networks (CRNs) to how we should implement actual chemical reaction computing devices. In this paper, we propose a framework for chemical computation using biochemical systems and present initial components of our approach: an abstract chemical reaction dialect, implemented as a multi-level intermediate representation (MLIR) compiler extension, and a path for representing mathematical problems with CRNs. We demonstrate the potential of this approach by emulating a simplified chemical reservoir device. This work lays the foundation for leveraging chemistry\u2019s computing power to create energy-efficient, high-performance computing systems for contemporary computing needs.",
        "8": "Scalable Artificial Intelligence for Science: Perspectives, Methods and Exemplars. Wesley Brewer, Aditya Kashi, Sajal Dash, A. Tsaris, Junqi Yin, M. Shankar, Feiyi Wang. arXiv.org, 2024.\nNumber of citations: 1\nAbstract: In a post-ChatGPT world, this paper explores the potential of leveraging scalable artificial intelligence for scientific discovery. We propose that scaling up artificial intelligence on high-performance computing platforms is essential to address such complex problems. This perspective focuses on scientific use cases like cognitive simulations, large language models for scientific inquiry, medical image analysis, and physics-informed approaches. The study outlines the methodologies needed to address such challenges at scale on supercomputers or the cloud and provides exemplars of such approaches applied to solve a variety of scientific problems.",
        "9": "Symmetry-Aware Code Generation: Distilling Pseudocode Reasoning for Lightweight Deployment of Large Language Models. Yonglin Li, Shanzhi Gu, Mingyang Geng. Symmetry, 2025.\nNumber of citations: 0\nAbstract: Code generation is a critical task in software engineering, enabling the automation of transforming natural language descriptions into executable code. Recent advancements in large language models (LLMs) have demonstrated their potential to significantly enhance code generation capabilities by leveraging complex reasoning processes. However, the large size of these models poses challenges for deployment in resource-constrained environments, as they require substantial computational resources and memory. The challenge lies in transferring the sophisticated problem-solving strategies of LLMs to smaller, more efficient models without sacrificing performance, while maintaining symmetry between the reasoning steps and final code generation. This task is further complicated by the need to preserve high code generation accuracy while reducing the resource demands of deployment. Although distillation methods have been proposed, efficiently transferring both the reasoning process and final code generation remains an underexplored area. In this work, we propose a novel distillation framework that extracts intermediate reasoning steps, such as pseudocode, from LLMs and transfers them to smaller models. Our approach enables smaller models to replicate the problem-solving strategies of larger models through a multi-task learning framework, which includes both pseudocode and code generation tasks, thus maintaining the symmetry between reasoning and output. We conducted comprehensive experiments on the CodeSearchNet dataset, comparing our distillation framework across four student models (Tranx, CodeT5, NatGen, and SPT-Code) distilled from four large language models (CodeLlama-7B, CodeQwen-7B, DeepSeek, and GPT-4). Results show that our approach consistently improves code generation performance, with the best case (CodeT5 distilled from GPT-4) achieving up to 74% improvement in Top-1 accuracy over the baseline.",
        "10": "COPO: Consistency-Aware Policy Optimization. Jinghang Han, Jiawei Chen, Hang Shao, Hao Ma, Mingcheng Li, Xintian Shen, Lihao Zheng, Wei Chen, Tao Wei, Lihua Zhang. arXiv.org, 2025.\nNumber of citations: 0\nAbstract: Reinforcement learning has significantly enhanced the reasoning capabilities of Large Language Models (LLMs) in complex problem-solving tasks. Recently, the introduction of DeepSeek R1 has inspired a surge of interest in leveraging rule-based rewards as a low-cost alternative for computing advantage functions and guiding policy optimization. However, a common challenge observed across many replication and extension efforts is that when multiple sampled responses under a single prompt converge to identical outcomes, whether correct or incorrect, the group-based advantage degenerates to zero. This leads to vanishing gradients and renders the corresponding samples ineffective for learning, ultimately limiting training efficiency and downstream performance. To address this issue, we propose a consistency-aware policy optimization framework that introduces a structured global reward based on outcome consistency, the global loss based on it ensures that, even when model outputs show high intra-group consistency, the training process still receives meaningful learning signals, which encourages the generation of correct and self-consistent reasoning paths from a global perspective. Furthermore, we incorporate an entropy-based soft blending mechanism that adaptively balances local advantage estimation with global optimization, enabling dynamic transitions between exploration and convergence throughout training. Our method introduces several key innovations in both reward design and optimization strategy. We validate its effectiveness through substantial performance gains on multiple mathematical reasoning benchmarks, highlighting the proposed framework's robustness and general applicability. Code of this work has been released at https://github.com/hijih/copo-code.git."
    }
}