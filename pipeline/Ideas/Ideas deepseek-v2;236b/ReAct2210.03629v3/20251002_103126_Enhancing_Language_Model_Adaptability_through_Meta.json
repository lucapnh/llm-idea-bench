{
    "query": "Enhancing Language Model Adaptability through Meta-Level Prompt Engineering",
    "result": {
        "1": "Meta-Prompting: Enhancing Language Models with Task-Agnostic Scaffolding. Mirac Suzgun, A. Kalai. arXiv.org, 2024.\nNumber of citations: 92\nAbstract: We introduce meta-prompting, an effective scaffolding technique designed to enhance the functionality of language models (LMs). This approach transforms a single LM into a multi-faceted conductor, adept at managing and integrating multiple independent LM queries. By employing high-level instructions, meta-prompting guides the LM to break down complex tasks into smaller, more manageable subtasks. These subtasks are then handled by distinct\"expert\"instances of the same LM, each operating under specific, tailored instructions. Central to this process is the LM itself, in its role as the conductor, which ensures seamless communication and effective integration of the outputs from these expert models. It additionally employs its inherent critical thinking and robust verification processes to refine and authenticate the end result. This collaborative prompting approach empowers a single LM to simultaneously act as a comprehensive orchestrator and a panel of diverse experts, significantly enhancing its performance across a wide array of tasks. The zero-shot, task-agnostic nature of meta-prompting greatly simplifies user interaction by obviating the need for detailed, task-specific instructions. Furthermore, our research demonstrates the seamless integration of external tools, such as a Python interpreter, into the meta-prompting framework, thereby broadening its applicability and utility. Through rigorous experimentation with GPT-4, we establish the superiority of meta-prompting over conventional scaffolding methods: When averaged across all tasks, including the Game of 24, Checkmate-in-One, and Python Programming Puzzles, meta-prompting, augmented with a Python interpreter functionality, surpasses standard prompting by 17.1%, expert (dynamic) prompting by 17.3%, and multipersona prompting by 15.2%.",
        "2": "Inadequacies of Large Language Model Benchmarks in the Era of Generative Artificial Intelligence. Timothy R. Mcintosh, Teo Su\u0161njak, Tong Liu, Paul A. Watters, Malka N. Halgamuge. IEEE Transactions on Artificial Intelligence, 2024.\nNumber of citations: 72\nAbstract: The rapid rise in popularity of Large Language Models (LLMs) with emerging capabilities has spurred public curiosity to evaluate and compare different LLMs, leading many researchers to propose their own LLM benchmarks. Noticing preliminary inadequacies in those benchmarks, we embarked on a study to critically assess 23 state-of-the-art LLM benchmarks, using our novel unified evaluation framework through the lenses of people, process, and technology, under the pillars of benchmark functionality and integrity. Our research uncovered significant limitations, including biases, difficulties in measuring genuine reasoning, adaptability, implementation inconsistencies, prompt engineering complexity, evaluator diversity, and the overlooking of cultural and ideological norms in one comprehensive assessment. Our discussions emphasized the urgent need for standardized methodologies, regulatory certainties, and ethical guidelines in light of Artificial Intelligence (AI) advancements, including advocating for an evolution from static benchmarks to dynamic behavioral profiling to accurately capture LLMs' complex behaviors and potential risks. Our study highlighted the necessity for a paradigm shift in LLM evaluation methodologies, underlining the importance of collaborative efforts for the development of universally accepted benchmarks and the enhancement of AI systems' integration into society.",
        "3": "Privacy Policy Analysis through Prompt Engineering for LLMs. Arda Goknil, Femke B. Gelderblom, Simeon Tverdal, Shukun Tokas, Hui Song. arXiv.org, 2024.\nNumber of citations: 5\nAbstract: Privacy policies are often obfuscated by their complexity, which impedes transparency and informed consent. Conventional machine learning approaches for automatically analyzing these policies demand significant resources and substantial domain-specific training, causing adaptability issues. Moreover, they depend on extensive datasets that may require regular maintenance due to changing privacy concerns. In this paper, we propose, apply, and assess PAPEL (Privacy Policy Analysis through Prompt Engineering for LLMs), a framework harnessing the power of Large Language Models (LLMs) through prompt engineering to automate the analysis of privacy policies. PAPEL aims to streamline the extraction, annotation, and summarization of information from these policies, enhancing their accessibility and comprehensibility without requiring additional model training. By integrating zero-shot, one-shot, and few-shot learning approaches and the chain-of-thought prompting in creating predefined prompts and prompt templates, PAPEL guides LLMs to efficiently dissect, interpret, and synthesize the critical aspects of privacy policies into user-friendly summaries. We demonstrate the effectiveness of PAPEL with two applications: (i) annotation and (ii) contradiction analysis. We assess the ability of several LLaMa and GPT models to identify and articulate data handling practices, offering insights comparable to existing automated analysis approaches while reducing training efforts and increasing the adaptability to new analytical needs. The experiments demonstrate that the LLMs PAPEL utilizes (LLaMA and Chat GPT models) achieve robust performance in privacy policy annotation, with F1 scores reaching 0.8 and above (using the OPP-115 gold standard), underscoring the effectiveness of simpler prompts across various advanced language models.",
        "4": "Visual Prompt Engineering for Vision Language Models in Radiology. Stefan Denner, Markus Bujotzek, Dimitrios Bounias, David Zimmerer, Raphael Stock, Klaus H. Maier-Hein. , 2024.\nNumber of citations: 4\nAbstract: Medical image classification plays a crucial role in clinical decision-making, yet most models are constrained to a fixed set of predefined classes, limiting their adaptability to new conditions. Contrastive Language-Image Pretraining (CLIP) offers a promising solution by enabling zero-shot classification through multimodal large-scale pretraining. However, while CLIP effectively captures global image content, radiology requires a more localized focus on specific pathology regions to enhance both interpretability and diagnostic accuracy. To address this, we explore the potential of incorporating visual cues into zero-shot classification, embedding visual markers, such as arrows, bounding boxes, and circles, directly into radiological images to guide model attention. Evaluating across four public chest X-ray datasets, we demonstrate that visual markers improve AUROC by up to 0.185, highlighting their effectiveness in enhancing classification performance. Furthermore, attention map analysis confirms that visual cues help models focus on clinically relevant areas, leading to more interpretable predictions.To support further research, we use public datasets and provide our codebase and preprocessing pipeline under https://github.com/MIC-DKFZ/VPE-in-Radiology, serving as a reference point for future work on localized classification in medical imaging.",
        "5": "Advanced Techniques in Prompt Engineering for Large Language Models: A Comprehensive Study. Gaurav Beri, Vaishnavi Srivastava. 2024 IEEE 4th International Conference on ICT in Business Industry & Government (ICTBIG), 2024.\nNumber of citations: 3\nAbstract: Prompt engineering is emerging as a pivotal approach to enhance the efficiency and versatility of large language models (LLMs) like GPT-4. This paper provides a comprehensive survey of key techniques in prompt engineering, highlighting foundational methods and advanced strategies. One-shot prompting, a basic yet impactful technique, involves supplying the model with a single example to guide its responses. In contrast, Chain-of-Thought (CoT) prompting encourages the model to generate intermediate reasoning steps, significantly improving its problem-solving abilities. Self-Consistency, an advanced strategy, involves generating multiple potential responses and selecting the most coherent one, thereby increasing the reliability of outputs. The transformative potential of these techniques is showcased through various applications in education, content creation, and programming. In education, prompt engineering can personalize learning experiences and provide tailored feedback to students. Content creation benefits from enhanced creativity and coherence in generated text, aiding writers and marketers. In programming, LLMs assist in code generation and debugging, streamlining the development process. This paper not only outlines the methodologies and their practical applications but also proposes future research directions to further refine and expand the capabilities of prompt engineering. By understanding and leveraging these techniques, practitioners can significantly optimize the performance and adaptability of LLMs across diverse domains. This framework aims to foster innovation and efficiency, laying the groundwork for future advancements in AI-driven technologies.",
        "6": "Selection of Prompt Engineering Techniques for Code Generation through Predicting Code Complexity. Chung-Yu Wang, Alireza Daghighfarsoodeh, Hung Viet Pham. arXiv.org, 2024.\nNumber of citations: 1\nAbstract: Large Language Models (LLMs) have demonstrated impressive performance in software engineering tasks. However, improving their accuracy in generating correct and reliable code remains challenging. Numerous prompt engineering techniques (PETs) have been developed to address this, but no single approach is universally optimal. Selecting the right PET for each query is difficult for two primary reasons: (1) interactive prompting techniques may not consistently deliver the expected benefits, especially for simpler queries, and (2) current automated prompt engineering methods lack adaptability and fail to fully utilize multi-stage responses. To overcome these challenges, we propose PET-Select, a PET-agnostic selection model that uses code complexity as a proxy to classify queries and select the most appropriate PET. By incorporating contrastive learning, PET-Select effectively distinguishes between simple and complex problems, allowing it to choose PETs that are best suited for each query's complexity level. Our evaluations on the MBPP and HumanEval benchmarks using GPT-3.5 Turbo and GPT-4o show up to a 1.9% improvement in pass@1 accuracy, along with a 74.8% reduction in token usage. Additionally, we provide both quantitative and qualitative results to demonstrate how PET-Select effectively selects the most appropriate techniques for each code generation query, further showcasing its efficiency in optimizing PET selection.",
        "7": "Analyzing the Sensitivity of Prompt Engineering Techniques in Natural Language Interfaces for 2.5D Software Visualization. Daniel Atzberger, Adrian Jobst, M. Tytarenko, Willy Scheibel, J\u00fcrgen D\u00f6llner, T. Schreck. The Web Conference, 2025.\nNumber of citations: 1\nAbstract: Natural Language Interfaces (NLIs) backed by Large Language Models (LLMs) are used to interact with visualizations through natural language queries. Using the specific example of 2.5D treemaps, the Delphi tool was recently presented, introducing an interactive 2.5D visualization with an accompanying chat interface, where the LLM can react to user input and adapt the visualization at its own discretion. While Delphi has demonstrated effectiveness, the authors have not included an evaluation of the LLM's performance with respect to its prompt and specific task types. In this study, we systematically evaluate the impact of prompt engineering on Delphi's ability to answer factual questions related to data and visualization. Specifically, we investigate the effect of the Chain-of-Thought prompting technique by employing a questionnaire comprising 40 questions across ten low-level analytic tasks. Our findings aim to refine prompt design methodologies and enhance the usability and effectiveness of NLIs in advanced visualization systems.",
        "8": "Inclusive prompt engineering for large language models: a modular framework for ethical, structured, and adaptive AI. M. Torkestani, Ali Alameer, P. Shivakumara, Taha Manosuri. Artificial Intelligence Review, 2025.\nNumber of citations: 0\nAbstract: None",
        "9": "clickBrick Prompt Engineering: Optimizing Large Language Model Performance in Clinical Psychiatry. F. G. Verhees, F. Huth, V. Meyer, F. Wolf, M. Bauer, A. Pfennig, P. Ritter, J. N. Kather, I. C. Wiest, P. Mikolas. medRxiv, 2025.\nNumber of citations: 0\nAbstract: None",
        "10": "ElderEase AR: Enhancing Elderly Daily Living with the Multimodal Large Language Model and Augmented Reality. Tianyu Song, Zhengyi Liu, Ruibin Zhao, Jie Fu. Proceedings of the 2024 International Conference on Virtual Reality Technology, 2024.\nNumber of citations: 0\nAbstract: Elderly individuals often face challenges in independent living due to age-related cognitive and physical decline. To address these issues, we propose an innovative Augmented Reality (AR) system, \u201cElderEase AR\u201d, designed to assist elderly users in their daily lives by leveraging a Multimodal Large Language Model (MLLM). This system enables elderly users to capture images of their surroundings and ask related questions, providing context-aware feedback. We evaluated the system's perceived ease-of-use and feasibility through a pilot study involving 30 elderly users, aiming to enhance their independence and quality of life. Our system integrates advanced AR technology with an intelligent agent trained on multimodal datasets. Through prompt engineering, the agent is tailored to respond in a manner that aligns with the speaking style of elderly users. Experimental results demonstrate high accuracy in object recognition and question answering, with positive feedback from user trials. Specifically, the system accurately identified objects in various environments and provided relevant answers to user queries. This study highlights the powerful potential of AR and AI technologies in creating support tools for the elderly. It suggests directions for future improvements and applications, such as enhancing the system's adaptability to different user needs and expanding its functionality to cover more aspects of daily living."
    }
}