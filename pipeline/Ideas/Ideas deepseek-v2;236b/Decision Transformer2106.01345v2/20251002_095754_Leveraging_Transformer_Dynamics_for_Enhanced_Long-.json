{
    "query": "Leveraging Transformer Dynamics for Enhanced Long-Horizon Reinforcement Learning",
    "result": {
        "1": "Offline Reinforcement Learning as One Big Sequence Modeling Problem. Michael Janner, Qiyang Li, S. Levine. Neural Information Processing Systems, 2021.\nNumber of citations: 723\nAbstract: Reinforcement learning (RL) is typically concerned with estimating stationary policies or single-step models, leveraging the Markov property to factorize problems in time. However, we can also view RL as a generic sequence modeling problem, with the goal being to produce a sequence of actions that leads to a sequence of high rewards. Viewed in this way, it is tempting to consider whether high-capacity sequence prediction models that work well in other domains, such as natural-language processing, can also provide effective solutions to the RL problem. To this end, we explore how RL can be tackled with the tools of sequence modeling, using a Transformer architecture to model distributions over trajectories and repurposing beam search as a planning algorithm. Framing RL as sequence modeling problem simplifies a range of design decisions, allowing us to dispense with many of the components common in offline RL algorithms. We demonstrate the flexibility of this approach across long-horizon dynamics prediction, imitation learning, goal-conditioned RL, and offline RL. Further, we show that this approach can be combined with existing model-free algorithms to yield a state-of-the-art planner in sparse-reward, long-horizon tasks.",
        "2": "Reinforcement Learning as One Big Sequence Modeling Problem. Michael Janner, Qiyang Li, Sergey Levine. arXiv.org, 2021.\nNumber of citations: 87\nAbstract: None",
        "3": "Q-value Regularized Transformer for Offline Reinforcement Learning. Shengchao Hu, Ziqing Fan, Chaoqin Huang, Li Shen, Ya Zhang, Yanfeng Wang, Dacheng Tao. International Conference on Machine Learning, 2024.\nNumber of citations: 17\nAbstract: Recent advancements in offline reinforcement learning (RL) have underscored the capabilities of Conditional Sequence Modeling (CSM), a paradigm that learns the action distribution based on history trajectory and target returns for each state. However, these methods often struggle with stitching together optimal trajectories from sub-optimal ones due to the inconsistency between the sampled returns within individual trajectories and the optimal returns across multiple trajectories. Fortunately, Dynamic Programming (DP) methods offer a solution by leveraging a value function to approximate optimal future returns for each state, while these techniques are prone to unstable learning behaviors, particularly in long-horizon and sparse-reward scenarios. Building upon these insights, we propose the Q-value regularized Transformer (QT), which combines the trajectory modeling ability of the Transformer with the predictability of optimal future returns from DP methods. QT learns an action-value function and integrates a term maximizing action-values into the training loss of CSM, which aims to seek optimal actions that align closely with the behavior policy. Empirical evaluations on D4RL benchmark datasets demonstrate the superiority of QT over traditional DP and CSM methods, highlighting the potential of QT to enhance the state-of-the-art in offline RL.",
        "4": "Transformer-Enhanced DQN Approach for Energy and Cost-Efficient Large-Scale Dynamic Workflow Scheduling in Heterogeneous Environment. Fan Ding, YaQian Yuan, Lizhi Lv, Rui Zhang, Wenbo Zhou. IEEE Internet of Things Journal, 2024.\nNumber of citations: 3\nAbstract: In a heterogeneous workflow environment, the uncertainty of task execution times, dynamic resource changes, and task dependencies\u2019 evolution pose significant scheduling challenges. This article investigates how to make intelligent and adaptive scheduling decisions in a constantly changing heterogeneous cloud environment. We propose a novel scheduling approach, transformer-enhanced DQN (T-DQN) that combines the strengths of reinforcement learning (RL) and the Transformer model into a hybrid strategy. This method leverages the ability of RL to handle uncertainty and dynamics in the decision-making process while integrating the advantages of the Transformer model in dealing with long sequences and complex relationships. Our experimental evaluation shows that the T-DQN algorithm outperforms existing algorithms consistently in real-world workflow, dynamic scenarios, and high-load environments. T-DQN reduces makespan by up to 13.66%, improves energy by about 16.65%, and improves cost by 44.72% compared to the six other approaches. This performance is particularly significant in high-load environments, where T-DQN\u2019s adaptability and scalability minimize failure rates and optimize resource management, affirming its suitability as a robust solution to complex cloud computing challenges.",
        "5": "Offline Trajectory Optimization for Offline Reinforcement Learning. Ziqi Zhao, Zhaochun Ren, Liu Yang, Yu-Fan Liang, Fajie Yuan, Pengjie Ren, Zhumin Chen, Jun Ma, Xin Xin. Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2, 2024.\nNumber of citations: 1\nAbstract: Offline reinforcement learning (RL) aims to learn policies without online explorations. To enlarge the training data, model-based offline RL learns a dynamics model which is utilized as a virtual environment to generate simulation data and enhance policy learning. However, existing data augmentation methods for offline RL suffer from (i) trivial improvement from short-horizon simulation; and (ii) the lack of evaluation and correction for generated data, leading to low-qualified augmentation. In this paper, we propose offline trajectory optimization for offline reinforcement learning (OTTO). The key motivation is to conduct long-horizon simulation and then utilize model uncertainty to evaluate and correct the augmented data. Specifically, we propose an ensemble of Transformers, a.k.a. World Transformers, to predict environment state dynamics and the reward function. Three strategies are proposed to use World Transformers to generate long-horizon trajectory simulation by perturbing the actions in the offline data. Then, an uncertainty-based World Evaluator is introduced to firstly evaluate the confidence of the generated trajectories and then perform the correction for low-confidence data. Finally, we jointly use the original data with the corrected augmentation data to train an offline RL algorithm. OTTO serves as a plug-in module and can be integrated with existing model-free offline RL methods. Experiments on various benchmarks show that OTTO can effectively improve the performance of representative offline RL algorithms, including in complex environments with sparse rewards like AntMaze. Codes are available at https://github.com/ZiqiZhao1/OTTO.",
        "6": "Data-driven predictive control for power converter with multi-step reinforcement learning. Yihao Wan, Yang Zhang, Qianwen Xu. European Conference on Cognitive Ergonomics, 2024.\nNumber of citations: 0\nAbstract: Finite control set model predictive control (FCS-MPC) is widely researched for converter control, as it incorporates the control objective and system constraints straightforwardly into the control. To address the drawbacks of conventional FCS-MPC, data-driven predictive control schemes, such as reinforcement learning (RL), have been proposed to tackle issues related to parameter sensitivity and unmodeled dynamics. Another challenge with FCS-MPC is the computational burden associated with complex converter systems and longer prediction horizon optimization problems. For FCS-MPC with a longer prediction horizon, the computation cost increases exponentially, rendering it impractical for real-time implementation. Additionally, RL controllers have not been explored for achieving long prediction horizon predictive control in power converters. To bridge this gap, this paper proposes introducing prediction horizons into RL for optimal power converter control, leveraging the nonlinear mapping capability and self-learning characteristics of RL. Thus, a multi-step RL controller is developed to enhance steady-state performance without increasing the computational burden. Both simulation and experimental results confirm the effectiveness of the proposed method.",
        "7": "Transformer-Characterized Approach for Chip Floorplanning: Leveraging HyperGCN and DTQN. Wenbo Guan, Xiaoyan Tang, Hongliang Lu, Jingru Tan, Jinlong Wang, Yuming Zhang. ICCD, 2024.\nNumber of citations: 0\nAbstract: In the realm of very large-scale integration (VLSI) chip design, chip floorplanning is essential, directly impacting key optimization objectives like placement wirelength, which in turn affects signal delay, power efficiency, routability, and overall cost. Traditional reinforcement learning (RL) methods for chip floorplanning often oversimplify the complex, dynamic nature of this task, typically overlooking the cascading effects of module placements and failing to fully grasp the intricate interdependencies crucial for informed decision-making. To address these challenges, we introduce an innovative approach that fuses hypergraph graph convolutional networks (HyperGCN) with deep Transformer Q-networks (DTQN). This integration captures the nuanced dynamics and interconnected aspects of chip design more effectively. We enhance the traditional Markov decision process (MDP) model with a state characterization layer based on Transformer technology. Initially, HyperGCN effectively encodes netlist information, simplifying complex graph structures into lower-dimensional vectors, thus enhancing knowledge processing. Subsequently, viewing the chip as an agent, we apply the DTQN algorithm to optimize the module layout. DTQN's Transformer encoder utilizes a multi-head self-attention (MSA) mechanism to grasp long-range dependencies between modules, coupled with DQN's optimization capabilities, to navigate the complexities of floorplanning. Our approach enhances the state representation by incorporating a broader understanding of the interconnected module characteristics, which allows agents to make decisions that account for the collective impact of module adjustments, rather than viewing each change in isolation. This comprehensive state representation, encompassing diverse features and their evolving relationships, significantly enhances the agent's decision-making capabilities. Our extensive experiments demonstrate that our method outperforms traditional heuristic-based and other learning-based floorplanning techniques. To our knowledge, this is the first application of the Transformer in circuit design, marking a significant advancement in the field.",
        "8": "Transformer with Koopman-Enhanced Graph Convolutional Network for Spatiotemporal Dynamics Forecasting. Zekai Wang, Bing Yao. arXiv.org, 2025.\nNumber of citations: 0\nAbstract: Spatiotemporal dynamics forecasting is inherently challenging, particularly in systems defined over irregular geometric domains, due to the need to jointly capture complex spatial correlations and nonlinear temporal dynamics. To tackle these challenges, we propose TK-GCN, a two-stage framework that integrates geometry-aware spatial encoding with long-range temporal modeling. In the first stage, a Koopman-enhanced Graph Convolutional Network (K-GCN) is developed to embed the high-dimensional dynamics distributed on spatially irregular domains into a latent space where the evolution of system states is approximately linear. By leveraging Koopman operator theory, this stage enhances the temporal consistency during the latent learning. In the second stage, a Transformer module is employed to model the temporal progression within the Koopman-encoded latent space. Through the self-attention mechanism, the Transformer captures long-range temporal dependencies, enabling accurate forecasting over extended horizons. We evaluate TK-GCN in spatiotemporal cardiac dynamics forecasting and benchmark its performance against several state-of-the-art baselines. Experimental results and ablation studies show that TK-GCN consistently delivers superior predictive accuracy across a range of forecast horizons, demonstrating its capability to effectively model complex spatial structures and nonlinear temporal dynamics.",
        "9": "A comparative study of deep learning and ensemble learning to extend the horizon of traffic forecasting. Xiao Zheng, S. A. Bagloee, Majid Sarvi. arXiv.org, 2025.\nNumber of citations: 0\nAbstract: Traffic forecasting is vital for Intelligent Transportation Systems, for which Machine Learning (ML) methods have been extensively explored to develop data-driven Artificial Intelligence (AI) solutions. Recent research focuses on modelling spatial-temporal correlations for short-term traffic prediction, leaving the favourable long-term forecasting a challenging and open issue. This paper presents a comparative study on large-scale real-world signalized arterials and freeway traffic flow datasets, aiming to evaluate promising ML methods in the context of large forecasting horizons up to 30 days. Focusing on modelling capacity for temporal dynamics, we develop one ensemble ML method, eXtreme Gradient Boosting (XGBoost), and a range of Deep Learning (DL) methods, including Recurrent Neural Network (RNN)-based methods and the state-of-the-art Transformer-based method. Time embedding is leveraged to enhance their understanding of seasonality and event factors. Experimental results highlight that while the attention mechanism/Transformer framework is effective for capturing long-range dependencies in sequential data, as the forecasting horizon extends, the key to effective traffic forecasting gradually shifts from temporal dependency capturing to periodicity modelling. Time embedding is particularly effective in this context, helping naive RNN outperform Informer by 31.1% for 30-day-ahead forecasting. Meanwhile, as an efficient and robust model, XGBoost, while learning solely from time features, performs competitively with DL methods. Moreover, we investigate the impacts of various factors like input sequence length, holiday traffic, data granularity, and training data size. The findings offer valuable insights and serve as a reference for future long-term traffic forecasting research and the improvement of AI's corresponding learning capabilities.",
        "10": "A convolutional deep reinforcement learning architecture for an emerging stock market analysis. Anita Hadizadeh, M. Tarokh, Majid Mirzaee Ghazani. Decision Science Letters, 2025.\nNumber of citations: 0\nAbstract: In the complex and dynamic stock market landscape, investors seek to optimize returns while minimizing risks associated with price volatility. Various innovative approaches have been proposed to achieve high profits by considering historical trends and social factors. Despite advancements, accurately predicting market dynamics remains a persistent challenge. This study introduces a novel deep reinforcement learning (DRL) architecture to forecast stock market returns effectively. Unlike traditional approaches requiring manual feature engineering, the proposed model leverages convolutional neural networks (CNNs) to directly process daily stock prices and financial indicators. The model addresses overfitting and data scarcity issues during training by replacing conventional Q-tables with convolutional layers. The optimization process minimizes the sum of squared errors, enhancing prediction accuracy. Experimental evaluations demonstrate the model's robustness, achieving a 67% improvement in directional accuracy over the buy-and-hold strategy across short-term and long-term horizons. These findings underscore the model\u2019s adaptability and effectiveness in navigating complex market environments, offering a significant advancement in financial forecasting."
    }
}