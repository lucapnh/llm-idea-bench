{
    "query": "Synchronized Generative Synthesis for Real-Time Multimodal Interaction",
    "result": {
        "1": "Talking With Hands 16.2M: A Large-Scale Dataset of Synchronized Body-Finger Motion and Audio for Conversational Motion Analysis and Synthesis. Gilwoo Lee, Zhiwei Deng, Shugao Ma, Takaaki Shiratori, S. Srinivasa, Yaser Sheikh. IEEE International Conference on Computer Vision, 2019.\nNumber of citations: 100\nAbstract: We present a 16.2-million frame (50-hour) multimodal dataset of two-person face-to-face spontaneous conversations. Our dataset features synchronized body and finger motion as well as audio data. To the best of our knowledge, it represents the largest motion capture and audio dataset of natural conversations to date. The statistical analysis verifies strong intraperson and interperson covariance of arm, hand, and speech features, potentially enabling new directions on data-driven social behavior analysis, prediction, and synthesis. As an illustration, we propose a novel real-time finger motion synthesis method: a temporal neural network innovatively trained with an inverse kinematics (IK) loss, which adds skeletal structural information to the generative model. Our qualitative user study shows that the finger motion generated by our method is perceived as natural and conversation enhancing, while the quantitative ablation study demonstrates the effectiveness of IK loss.",
        "2": "Real-Time Synthesis of Body Movements Based on Learned Primitives. M. Giese, A. Mukovskiy, Aee-ni Park, Lars Omlor, J. Slotine. Statistical and Geometrical Approaches to Visual Motion Analysis, 2009.\nNumber of citations: 31\nAbstract: None",
        "3": "TalkingAndroid: An interactive, multimodal and real-time talking avatar application on mobile phones. Huijie Lin, Jia Jia, Xiangjin Wu, Lianhong Cai. Asia-Pacific Signal and Information Processing Association Annual Summit and Conference, 2013.\nNumber of citations: 4\nAbstract: None",
        "4": "A Multimodal Interaction System for Speech-Based Autism Intervention in Sinhala-Speaking Sri Lankan Children using the NAO Robot. Aparna Jayawardena, Chan Sri Manukalpa, Heshani Bopage, Poorna Panduwawala, K. Pulasinghe, Samantha Rajapakshe. International Conference on Automation and Computing, 2024.\nNumber of citations: 3\nAbstract: This paper presents a multimodal interaction system designed for speech-based autism intervention in Sinhala-speaking Sri Lankan children, utilizing the NAO robot. The system integrates four components, language content detection, dialog management, voice synthesis, and gesture synthesis, to create an engaging and adaptive therapeutic environment. By leveraging these technologies, the system provides real-time, personalized responses that align with the cultural and linguistic needs of Sinhala-speaking children with autism spectrum disorder (ASD). The NAO robot facilitates natural and supportive interaction through synchronized speech and gestures, aiming to improve communication and social skills in children with ASD. Preliminary evaluations demonstrate the potential of the system to enhance engagement and responsiveness in therapy sessions, marking a significant step forward in developing specialized interventions for autism in non-Western, non-English-speaking contexts.",
        "5": "Osmosis: Generative AI and XR for the real-time transformation of urban architectural environments. Iasonas Paterakis, Nefeli Manoudaki. International Journal of Architectural Computing, 2025.\nNumber of citations: 0\nAbstract: This work contributes to the evolving discourse on biodigital architecture by examining how generative artificial intelligence (AI) and extended reality (XR) systems can be combined to create immersive urban environments. Focusing on the case study of \u201cOsmosis\u201d, a series of large-scale public installations, this work proposes a methodological framework for real-time architectural composition in XR using diffusion models and interaction. The project reframes the architectural fa\u00e7ade as a semi permeable membrane, through which digital content diffuses in response to environmental and user inputs. By integrating natural language prompts, multimodal input, and AI-generated visual synthesis with projection mapping, Osmosis advances a vision for urban architecture that is interactive, data-driven, and sensorially rich. The work explores new design territories where stochastic form-making and real-time responsiveness intersect, and positions AI as an augmentation of architectural creativity rather than its replacement.",
        "6": "A Generative City Digital Twin System for Flooding Emergency Management in Smart City. Chenyu Ge, Shengfeng Qin. International Conference on Automation and Computing, 2024.\nNumber of citations: 0\nAbstract: Emergency Management (EM) is a critical component in maintaining life quality and enhancing public safety in smart cities. City EM is now often triggered by extreme weather conditions, particularly urban flooding resulting from Localized Heavy Rain (LHR), as it requires timely multi-service collaboration, planning and decision-making among all stakeholders, this in turn requires the systematic support of multimodal information model fusion, timely visual situation awareness, edge-cloud collaborative analysis and virtual-real interactive control. The challenge is to provide the above requirements and collaborate multi-domain real world system with its multimodal virtual system in a bidirectional platform. Digital twin (DT), serving as a digital platform architecture, can enable dynamic synchronization, visualization, and virtual-real interaction across the full life cycle of physical space in virtual space. Building upon this concept, we developed an easy-to-use generative system for 'select area of interest - CFDT'. That is, a dynamic flooding information model-driven, auto-generative City Flooding Digital Twin (CFDT) system that supports near-real-time visual situational awareness, XR-based interaction, and smart decision-making. The developed prototype has undergone demonstration and tested at city, regional and street levels across different countries to assess its interpretability, usability, timely ability, and performance quantitatively and qualitatively. The results show that it is efficacy as a valuable tool for various CDT developments, providing a service fusion/integration platform across multiple domains. This work emphasizes the opportunities that CDT brings to smart city and EM.",
        "7": "Eye Image and EOG Signal Conversion via Autoencoders for Human-Machine Interaction. Seokjue Jeong, Sunghan Lee, Jeonghwan Koh, Hyungchan An, In cheol Jeong. IEEE International Conference on Consumer Electronics, 2025.\nNumber of citations: 0\nAbstract: Converting between EOG signals and eye images allows for adaptive gaze-based systems, enabling real-time visualization of eye movements in healthcare and low-cost gaze-tracking in consumer electronics. This study presents a bidirectional mapping framework between eye images and electrooculography (EOG) signals, enabling intuitive and hands-free human-machine interaction (HMI). A generative autoencoder was trained over 7000 epochs to map 2-D eye images to 1-D EOG signals and back, using a 2-step decoder structure for enhanced accuracy. The training utilized a dataset of synchronized EOG and eye image data from 30 participants. Experimental results showed that the model successfully reconstructed key EOG patterns and eye images, achieving an average image correlation of $0.9582 (\\pm 0.0068)$ and EOG correlation values of $0.6825(\\pm 0.3272)$ for horizontal and $0.6526 (\\pm 0.3000)$ for vertical signals, with minor noise noted in fine EOG details. These findings suggest that the proposed model can reliably transform between EOG signals and eye images. It has the potential to provide a promising basis for accessible, gaze-based HMI systems in consumer electronics. Future work will focus on refining detail accuracy and multimodal integration.",
        "8": "Machine Learning for Multimodal Interaction, 5th International Workshop, MLMI 2008, Utrecht, The Netherlands, September 8-10, 2008. Proceedings. Mlmi, Andrei Popescu-Belis, R. Stiefelhagen. Machine Learning for Multimodal Interaction, 2008.\nNumber of citations: 0\nAbstract: None",
        "9": "Research on a Diffusion Model-Driven Framework for New Media Animation Content Generation and Communication Optimization Based on Multimodal User Behavior Data and Attention Mechanism for Artistic Style Transfer Algorithms. Xiaohang Zhang, Qiongfei You. International Journal of High Speed Electronics and Systems, 2025.\nNumber of citations: 0\nAbstract: The rapidly evolving ecosystem of new media platforms calls for animation content generation technologies that are dynamic, personalized, and adaptive to multimodal user contexts. Traditional manual-driven animation pipelines, while effective for ensuring quality, lack the scalability, responsiveness, and flexibility required to meet the demands of modern, user-centric environments. These limitations constrain creative diversity, hinder real-time adaptability, and create significant bottlenecks in content delivery. To address these challenges, we propose a novel framework that leverages diffusion models and multimodal user behavior integration for intelligent animation generation. At the core of the framework is the Dynamic Semantic Animation Engine (DSAE), which utilizes a dual-stream architecture to combine semantic grounding, creative variation, and hierarchical latent modulation. This enables the system to generate animations that are both content-aware and artistically expressive. Complementing DSAE is the Contextual Animation Adaptation Mechanism (CAAM), which introduces real-time context fusion and predictive interaction modeling to adapt generated content to changing user behavior and environmental cues. The proposed system is supported by a set of rigorous mathematical formulations that govern structured synthesis processes, ensuring temporal coherence, semantic fidelity, and stylistic consistency across frames. Experimental evaluations conducted across diverse animation tasks demonstrate that the framework significantly enhances animation quality, interactivity, and context awareness compared to conventional deep generative approaches. The results validate the system\u2019s potential as a scalable, intelligent solution for content generation in computational media arts and interactive digital storytelling, advancing the frontier of AI-driven creative technologies.",
        "10": "Development of an Interactive Digital Human with Context-Sensitive Facial Expressions. Fan Yang, Lei Fang, Rui Suo, Jing Zhang, Mincheol Whang. Italian National Conference on Sensors, 2025.\nNumber of citations: 0\nAbstract: With the increasing complexity of human\u2013computer interaction scenarios, conventional digital human facial expression systems show notable limitations in handling multi-emotion co-occurrence, dynamic expression, and semantic responsiveness. This paper proposes a digital human system framework that integrates multimodal emotion recognition and compound facial expression generation. The system establishes a complete pipeline for real-time interaction and compound emotional expression, following a sequence of \u201cspeech semantic parsing\u2014multimodal emotion recognition\u2014Action Unit (AU)-level 3D facial expression control.\u201d First, a ResNet18-based model is employed for robust emotion classification using the AffectNet dataset. Then, an AU motion curve driving module is constructed on the Unreal Engine platform, where dynamic synthesis of basic emotions is achieved via a state-machine mechanism. Finally, Generative Pre-trained Transformer (GPT) is utilized for semantic analysis, generating structured emotional weight vectors that are mapped to the AU layer to enable language-driven facial responses. Experimental results demonstrate that the proposed system significantly improves facial animation quality, with naturalness increasing from 3.54 to 3.94 and semantic congruence from 3.44 to 3.80. These results validate the system\u2019s capability to generate realistic and emotionally coherent expressions in real time. This research provides a complete technical framework and practical foundation for high-fidelity digital humans with affective interaction capabilities."
    }
}