{
    "query": "Self-Improving Language Models through Meta-Learning for Adaptive Tool Usage",
    "result": {
        "1": "GAugLLM: Improving Graph Contrastive Learning for Text-Attributed Graphs with Large Language Models. Yi Fang, Dongzhe Fan, D. Zha, Qiaoyu Tan. Knowledge Discovery and Data Mining, 2024.\nNumber of citations: 19\nAbstract: This work studies self-supervised graph learning for text-attributed graphs (TAGs) where nodes are represented by textual attributes. Unlike traditional graph contrastive methods that perturb the numerical feature space and alter the graph's topological structure, we aim to improve view generation through language supervision. This is driven by the prevalence of textual attributes in real applications, which complement graph structures with rich semantic information. However, this presents challenges because of two major reasons. First, text attributes often vary in length and quality, making it difficulty to perturb raw text descriptions without altering their original semantic meanings. Second, although text attributes complement graph structures, they are not inherently well-aligned. To bridge the gap, we introduce GAugLLM, a novel framework for augmenting TAGs. It leverages advanced large language models like Mistral to enhance self-supervised graph learning. Specifically, we introduce a mixture-of-prompt-expert technique to generate augmented node features. This approach adaptively maps multiple prompt experts, each of which modifies raw text attributes using prompt engineering, into numerical feature space. Additionally, we devise a collaborative edge modifier to leverage structural and textual commonalities, enhancing edge augmentation by examining or building connections between nodes. Empirical results across five benchmark datasets spanning various domains underscore our framework's ability to enhance the performance of leading contrastive methods (e.g., BGRL, GraphCL, and GBT) as a plug-in tool. Notably, we observe that the augmented features and graph structure can also enhance the performance of standard generative methods (e.g., GraphMAE and S2GAE), as well as popular graph neural networks (e.g., GCN and GAT). The open-sourced implementation of our GAugLLM is available at https://github.com/NYUSHCS/GAugLLM.",
        "2": "To Code or not to Code? Adaptive Tool Integration for Math Language Models via Expectation-Maximization. Haozhe Wang, Long Li, Chao Qu, Weidi Xu, Fengming Zhu, Wei Chu, Fangzhen Lin. Annual Meeting of the Association for Computational Linguistics, 2025.\nNumber of citations: 9\nAbstract: Recent advances in mathematical problem-solving with language models (LMs) integrate chain-of-thought (CoT) reasoning and code execution to harness their complementary strengths. However, existing hybrid frameworks exhibit a critical limitation: they depend on externally dictated instructions or rigid code-integration templates, lacking metacognitive awareness -- the capacity to dynamically evaluate intrinsic capabilities and autonomously determine when and how to integrate tools. This rigidity motivates our study of autonomous code integration, enabling models to adapt tool-usage strategies as their reasoning abilities evolve during training. While reinforcement learning (RL) shows promise for boosting LLM reasoning at scale (e.g., DeepSeek-R1), we demonstrate its inefficiency in learning autonomous code integration due to inadequate exploration of the vast combinatorial space of CoT-code interleaving patterns. To address this challenge, we propose a novel Expectation-Maximization (EM) framework that synergizes structured exploration (E-step) with off-policy RL optimization (M-step), creating a self-reinforcing cycle between metacognitive tool-use decisions and evolving capabilities. Experiments reveal our method achieves superior results through improved exploration. Notably, our 7B model improves over 11% on MATH500 and 9.4% on AIME without o1-like CoT.",
        "3": "Knowledge Prompts: Injecting World Knowledge into Language Models through Soft Prompts. C. D. Santos, Zhe Dong, Daniel Matthew Cer, John Nham, Siamak Shakeri, Jianmo Ni, Yun-Hsuan Sung. arXiv.org, 2022.\nNumber of citations: 8\nAbstract: Soft prompts have been recently proposed as a tool for adapting large frozen language models (LMs) to new tasks. In this work, we repurpose soft prompts to the task of injecting world knowledge into LMs. We introduce a method to train soft prompts via self-supervised learning on data from knowledge bases. The resulting soft knowledge prompts (KPs) are task independent and work as an external memory of the LMs. We perform qualitative and quantitative experiments and demonstrate that: (1) KPs can effectively model the structure of the training data; (2) KPs can be used to improve the performance of LMs in different knowledge intensive tasks.",
        "4": "SMART: Self-learning Meta-strategy Agent for Reasoning Tasks. Rongxing Liu, K. Shridhar, Manish Prajapat, Patrick Xia, Mrinmaya Sachan. arXiv.org, 2024.\nNumber of citations: 3\nAbstract: Tasks requiring deductive reasoning, especially those involving multiple steps, often demand adaptive strategies such as intermediate generation of rationales or programs, as no single approach is universally optimal. While Language Models (LMs) can enhance their outputs through iterative self-refinement and strategy adjustments, they frequently fail to apply the most effective strategy in their first attempt. This inefficiency raises the question: Can LMs learn to select the optimal strategy in the first attempt, without a need for refinement? To address this challenge, we introduce SMART (Self-learning Meta-strategy Agent for Reasoning Tasks), a novel framework that enables LMs to autonomously learn and select the most effective strategies for various reasoning tasks. We model the strategy selection process as a Markov Decision Process and leverage reinforcement learning-driven continuous self-improvement to allow the model to find the suitable strategy to solve a given task. Unlike traditional self-refinement methods that rely on multiple inference passes or external feedback, SMART allows an LM to internalize the outcomes of its own reasoning processes and adjust its strategy accordingly, aiming for correct solutions on the first attempt. Our experiments across various reasoning datasets and with different model architectures demonstrate that SMART significantly enhances the ability of models to choose optimal strategies without external guidance (+15 points on the GSM8K dataset). By achieving higher accuracy with a single inference pass, SMART not only improves performance but also reduces computational costs for refinement-based strategies, paving the way for more efficient and intelligent reasoning in LMs.",
        "5": "The Influence of Meta-Cognitive Listening Strategies on Listening Performance in the MALL: The Mediation Effect of Learning Style and Self-Efficacy. Shaojie Tan, Arshad Abd Samad, Lilliati Ismail. SAGE Open, 2024.\nNumber of citations: 3\nAbstract: The ability to understand listening is an essential skill for non-native speakers of English as a foreign language (EFL). Research has shown that several factors can influence EFL students\u2019 English listening comprehension, including the listening strategy they use during listening activities. This study, therefore, used the meta-cognitive listening strategy to explore the mediation role of learning style and self-efficacy in the relationship with listening performance. In this study, 632 Chinese university students participated in a survey and completed electronic questionnaires on MALQ (Meta-cognitive Awareness of Listening Questionnaire), PLSPQ (Perceptual Learning Style Preference Questionnaire), and GSE (General Self-Efficacy Scale). The data were analyzed using AMOS software and a structural equation modeling (SEM) technique. The results showed that students\u2019 listening strategy directly and positively predicted their English listening performance. Moreover, students\u2019 listening performance was fully mediated by learning style and self-efficacy. After discussing these findings, suggestions as well as limitations for future studies will be given. The findings of this study are expected to contribute to a better understanding of the factors that may affect EFL students\u2019 English listening comprehension and inform instructional practices in the EFL classroom. Specifically, the results of this study may suggest the use of teaching strategies tailored to individual learning styles and the adoption of measures to improve self-efficacy of EFL students. Ultimately, the goal of this research is to improve EFL students\u2019 English listening comprehension skills and enhance their overall language proficiency. Plain Language Summary This study examined the effects of meta-cognitive listening strategies on the listening performance of English as a foreign language (EFL) students in a mobile-assisted language learning (MALL) environment. The study also looked into the role of learning style and self-efficacy as mediators in this relationship. Data was collected from 632 Chinese university students through surveys and questionnaires. Analysis of the data revealed that students who used meta-cognitive listening strategies had better English listening performance. Furthermore, learning style and self-efficacy played a mediating role in the relationship between listening strategy and performance. The findings suggest that tailoring teaching strategies to individual learning styles and boosting self-efficacy can improve EFL students\u2019 English listening comprehension skills. It is hoped that this research contributes to a better understanding of factors influencing EFL students\u2019 listening comprehension and informs instructional practices in the EFL classroom. However, the study has limitations, such as a limited sample size and the use of self-report questionnaires, which may introduce subjectivity and bias. Future research could address these limitations by expanding the sample size and incorporating objective measurement tools.",
        "6": "ToolACE-R: Model-aware Iterative Training and Adaptive Refinement for Tool Learning. Xingshan Zeng, Weiwen Liu, Xu Huang, Zezhong Wang, Lingzhi Wang, Liangyou Li, Yasheng Wang, Lifeng Shang, Xin Jiang, Ruiming Tang, Qun Liu. , 2025.\nNumber of citations: 1\nAbstract: Tool learning, which allows Large Language Models (LLMs) to leverage external tools for solving complex user tasks, has emerged as a promising avenue for extending model capabilities. However, existing approaches primarily focus on data synthesis for fine-tuning LLMs to invoke tools effectively, largely ignoring how to fully stimulate the potential of the model. In this paper, we propose ToolACE-R, a novel framework that includes both model-aware iterative training and adaptive refinement for tool learning. ToolACE-R features a model-aware iterative training procedure that progressively adjust training samples based on the model's evolving capabilities to maximize its potential. Additionally, it incorporates self-refinement training corpus which emphasizes LLM's ability to iteratively refine their tool calls, optimizing performance without requiring external feedback. Furthermore, we introduce adaptive self-refinement mechanism for efficient test-time scaling, where the trained model can autonomously determine when to stop the process based on iterative self-refinement. We conduct extensive experiments across several benchmark datasets, showing that ToolACE-R achieves competitive performance compared to advanced API-based models. The performance of tool invocation can be further improved efficiently through adaptive self-refinement. These results highlight the effectiveness and generalizability of ToolACE-R, offering a promising direction for more efficient and scalable tool learning.",
        "7": "Investigating Smartphone Empowerment for Self-Regulated Learning in English Language Acquisition: A Descriptive Study. E. Rahmani. Voices of English Language Education Society, 2024.\nNumber of citations: 1\nAbstract: This study investigates the perceptions of undergraduate English Language Education students on the utilization of smartphones for self-regulated learning and the acquisition of the English language. The study investigates the ways in which smartphones facilitate the learning of English language through self-directed techniques, using Zimmerman's model of self-regulation and the MALL framework as theoretical basis. An exploratory investigation including 110 students was conducted to analyze their experiences, perspectives, and practices in utilizing language learning applications. The findings indicate a range of levels of knowledge and usage, with students generally acknowledging that smartphones are valuable tools for improving language acquisition through self-regulation. Analysis based on Zimmerman's model yields high average scores (3.42-3.51) for forethought, performance, and reflection & evaluation, indicating students engage in self-regulated learning practices. According to the MALL framework analysis, task design and design characteristics, all received high ratings (ranging from 3.42 to 3.68). This indicates that the app features effectively promote self-regulated learning activities. This study emphasizes the crucial role that smartphones play in promoting self-regulated learning practices and enhancing the acquisition of the English language among university students",
        "8": "MetaAgent: Toward Self-Evolving Agent via Tool Meta-Learning. Hongjin Qian, Zheng Liu. arXiv.org, 2025.\nNumber of citations: 0\nAbstract: In this work, we propose MetaAgent, an agentic paradigm inspired by the principle of learning-by-doing, where expertise is developed through hands-on practice and continual self-improvement. MetaAgent starts with a minimal workflow, equipped only with basic reasoning and adaptive help-seeking abilities. When a knowledge gap is encountered, MetaAgent generates natural language help requests, which are routed to the most suitable external tool by a dedicated tool router. As MetaAgent solves tasks, it continually conducts self-reflection and answer verification, distilling actionable experience into concise texts that are dynamically incorporated into future task contexts. Besides, MetaAgent autonomously builds in-house tools and a persistent knowledge base by organizing its tool-use history, further enhancing its ability to retrieve and integrate relevant information We term this continual, data-driven process as \\textit{meta tool learning}, through which MetaAgent incrementally refines its reasoning and tool-use strategies, without changing model parameters or requiring further post-training. Evaluated on challenging knowledge discovery benchmarks, including GAIA, WebWalkerQA, and BrowseCamp, MetaAgent consistently outperforms workflow-based baselines and matches or exceeds end-to-end trained agents, demonstrating the promise of self-evolving agentic systems for robust, general-purpose knowledge discovery. We provide our source codes in https://github.com/qhjqhj00/MetaAgent.",
        "9": "Adapting Mental Health Prediction Tasks for Cross-lingual Learning via Meta-Training and In-context Learning with Large Language Model. Zita Lifelo, Huansheng Ning, Sahraoui Dhelim. arXiv.org, 2024.\nNumber of citations: 0\nAbstract: Timely identification is essential for the efficient handling of mental health illnesses such as depression. However, the current research fails to adequately address the prediction of mental health conditions from social media data in low-resource African languages like Swahili. This study introduces two distinct approaches utilising model-agnostic meta-learning and leveraging large language models (LLMs) to address this gap. Experiments are conducted on three datasets translated to low-resource language and applied to four mental health tasks, which include stress, depression, depression severity and suicidal ideation prediction. we first apply a meta-learning model with self-supervision, which results in improved model initialisation for rapid adaptation and cross-lingual transfer. The results show that our meta-trained model performs significantly better than standard fine-tuning methods, outperforming the baseline fine-tuning in macro F1 score with 18\\% and 0.8\\% over XLM-R and mBERT. In parallel, we use LLMs' in-context learning capabilities to assess their performance accuracy across the Swahili mental health prediction tasks by analysing different cross-lingual prompting approaches. Our analysis showed that Swahili prompts performed better than cross-lingual prompts but less than English prompts. Our findings show that in-context learning can be achieved through cross-lingual transfer through carefully crafted prompt templates with examples and instructions.",
        "10": "Adaptive Language Translation in Multinational Info Service Platforms. Raami Riadhusin, R. Radhakrishnan, Delecta Jenifer Rajendren, Ibragimov Ulmas Rakhmanovich, N. D. Lal, Utkarsh Anand. Indian Journal of Information Sources and Services, 2025.\nNumber of citations: 0\nAbstract: Modern digital platforms, including worldwide information services, have to serve a multicultural clientele regarding language. Lack of communication because of languages obstructs user interaction and the ability to access the system. As a solution, adaptive language translation systems provide a powerful approach to resolving barriers to communication by offering instantaneous translations that are contextually accurate and precise. This paper addresses constructing and implementing an AI-based adaptive translation system that employs deep learning to tailor the language models to user idiosyncratic preferences, regional inflections, and language usage patterns. The design uses natural language processing (NLP) and neural machine translation algorithms to preserve the accurate translation of languages and the contextual significance of the translated languages. Moreover, the design is self-improving because it learns from user feedback, interactions, and the contextual information available, resulting in improved quality of translations over time. Also, cloud deployment enhances scalability and processing speed, making it ideal for global real-time viewing of multilingual information systems. Enhanced user interaction, communication, and inclusivity in support services, e-commerce, education, and public service will be achievable due to these features. This technology fosters interoperability by reducing language constraints and enabling optimal access to information while improving global connectivity through digital platforms. The final touches of the performance assessments, application scenarios, and further developments are documented, stemming from integrating adaptive translation into wider systems of AI."
    }
}